[{"location":"https://codenow.me/","text":"","title":"ATTS Group"},{"location":"https://codenow.me/articles/","text":"","title":"文章"},{"location":"https://codenow.me/algorithm/","text":"","title":"算法"},{"location":"https://codenow.me/translation/","text":"","title":"翻译"},{"location":"https://codenow.me/tips/","text":"","title":"Tips"},{"location":"https://codenow.me/rules/","text":"","title":"玩法"},{"location":"https://codenow.me/joinus/","text":"","title":"加入我们"},{"location":"https://codenow.me/translation/why_django/","text":"原文地址\n如今，许多的后端开发人员选择使用Python。Python已经称为最受欢迎的Web开发语言之一，它的灵活性和多功能行是它能够取得如此成功的重要原因之一。从开发简单代码到数据分析和机器学习，Python已成为许多开发人员的首选语言。\n有许多框架是可以和Python一起使用的，这些框架基本上允许开发人员选择一个平台，他们可以根据自己的喜好自定义并自由测试。在Python的所有框架中，Django似乎是最受欢迎的选择。实际上，在2018年的Stock Overflow Survey中，Django被列为最受欢迎的框架之一，58%的开发者投票支持。\n很明显，许多开发人员更喜欢使用Django进行Web项目开发，如果您尚未使用Django并且想知道热点是关于什么，这里有一些原因可以解释为什么Django在Web开发人员中很受欢迎。\nDjango的流行 Python是最简单的编码语言之一，Django是一个基于Python的Web开发框架，具有Python的所有最佳功能 - 易于维护，编码简单，干净，调试速度更快。 Django保留了Python的真正本质。 它消除了编码的复杂性，并且不希望编码中有太多冗余，从而更容易将开发成本降至最低，并为简化调试铺平了道路。\nDjango如此受欢迎，以至于世界上一些顶级应用程序都是用它开发的。 世界上一些顶级内容驱动的网站，如华盛顿邮报，纽约时报，卫报和许多其他网站使用Django来处理来自其网站的巨大流量并保持其高水平的性能。 Dropbox是世界顶级云存储平台之一，也主要与Django一起运行，用于存储，同步和共享选项。 Django框架也用于Instagram，Spotify，Disqus，YouTube，Mozilla等，帮助开发人员保持更新的新功能和新的更新，使他们能够快速工作。 其中一些应用程序使用Django作为主要平台运行，而其他一些应用程序部分使用它。 尽管如此，Django已成为目前许多成功网站运营的一部分，难怪它正在快速攀升人气图！\n易于扩展和扩展 Django的组件可以被删除，添加或修改，使得使用冗余代码变得简单易行。 此外，它可以完全自由地向应用程序添加第三方软件包，从而减少了开发人员的大量工作。 除此之外，如果代码运行冗长，可以不必担心找到另一个平台，因为Django可以被缩放以容易地包含任何长度的代码。\n互动而有益的社区 Django是一个免费的开源框架，意味着框架将始终更新为最佳版本。 Django有大量文档证明您拥有使用它所需的所有信息和资源。 当您想讨论使用Python Django开发的新可能性或需要一些特定部分的帮助时，Django社区非常庞大且具有交互性，您可以快速获得答案。 解决您对Django的疑虑或问题很简单 - 只需Google搜索您的问题，您就可以立即获得解决方案！\n有时，当一个新的开发人员加入项目的中间时，感觉很难赶上当前的开发阶段，并在编码阶段快速进入正轨。 但是，随着Django基于MTV架构 - 模型 - 模板 - 视图架构 - 工程中不同任务的代码保持分离。 这使得加入团队的新开发人员更容易达到速度并从第一天开始轻松地开始工作。 自动安全\nDjango为其构建的应用程序提供安全保护。 它在您启动开发产品之前使用自动检查来查找任何错过的安全漏洞。 它还有助于识别和减少Python中编码带来的一些最常见的安全错误，并保护应用程序免受错误。\nPython与Django框架相结合，为创建一个可靠且高度安全的平台来开发Web开发项目奠定了基础。 随着新功能和功能的引入，开发人员使用Django并尽快启动他们的项目变得越来越简单。\n","title":"Why Django Is The Popular Python Framework aMONG wEB dEVELOPERS?"},{"location":"https://codenow.me/articles/the_image_container_repository_in_docker/","text":" Docker镜像 Docker镜像类似于虚拟机镜像，可以将它理解为一个只读模板。 例如，一个镜像包含一个基本的操作系统环境，里面仅安装了Apache应用程序(或用户需要的其他软件)。可以把它称为一个Apache镜像。 镜像是创建Docker容器的清楚。通过版本管理和增量的文件系统，Docker提供了一套十分简单的机制来创建和更新现有的镜像，用户甚至可以从网上下载一个已经做好的应用镜像，并直接使用。\nDocker容器 Docker容器类似于一个轻量级的沙盒，Docker利用容器来运行和隔离应用。 容器时从镜像创建的应用运行实例。他可以启动、停止、删除，而这些容器都是彼此相互隔离、互补可见的。 可以把容器看作一个简易版的Linux系统环境(包括root用户系统，进程空间，用户空间和网络空间等)以及运行在其中的应用程序打包而成的盒子。\nDocker仓库 Docker仓库类似于代码仓库，是Docker集中存放镜像文件的场所。 有时候我们会将Docker仓库和仓库注册服务器(Registry)混为一谈，并不严格区分。实际上，仓库注册服务器是存放仓库的地方，其上往往存放着多个仓库。每个仓库集中存放某一类镜像，往往包括多个镜像文件，通过不同的标签(tag)来进行区分。例如存放Ubuntu操作系统镜像的仓库，被称为Ubuntu仓库，其中可能包括不同版本的镜像。\n根据所存储的镜像公开与否，Docker仓库可以分为公开仓库和私有仓库两种形式。目前，最大的公开仓库是官方提供的Docker Hub，其中存放着数量庞大的镜像供用户下载。国内不少云服务提供商(如腾讯云，阿里云等)也提供了仓库的本地源，可以提供稳定的国内访问。 当然，用户如果不希望公开分享自己的镜像文件，Docker也支持用户在本地网络内创建一个只能自己访问的私有仓库。 当用户创建了自己的镜像之后，就可以使用push命令将它上传到指定的公有或者私有仓库。这样用户下次在另一台机器上使用该镜像时，只需要将其从仓库上pull下来即可。\n镜像和容器的区别： 镜像是一个只读系统，在这个只读系统中存在很多只读层，它们按照层次顺序堆叠在一起，中间使用指针连接起来(指针指向下一层)。统一的文件系统将多层只读层统一起来，所以看起来会是一个整体。 容器在镜像的上层添加了一层可读可写层。通过该层，可以经过系统进行写入操作。初次之外，容器几乎是与镜像一样的。\n","title":"Doker核心概念-镜像、容器和仓库"},{"location":"https://codenow.me/tips/build_go_on_windows/","text":" Windows版Go安装 下载Msi版进行安装。 下载地址\n选择Microsoft Windows版下载。\n安装步骤直接根据步骤提示进行。\n安装文件目录说明： |目录名|备注| |-|-| |api|每个版本的api变更差异| |bin|go源码包编译出的编译器(go)，文档工具(godoc)，格式化工具(gofmt)| |blog|Go博客模板，使用Go的网页模板| |doc|英文版的Go文档| |lib|引用的一些库文件| |misc|杂项用途的文件，例如Android平台的编译、git的提交钩子等| |pkg|Windows平台编译好的中间文件| |src|标准库的源码| |test|测试用例|\n开发环境安装 GoLand下载地址\n具体安装步骤参考： CSDN文章\n","title":"Windows安装Go开发环境"},{"location":"https://codenow.me/algorithm/leetcode_3_longest_substring_without_repeating_characters/","text":" 题号：3 难度：medium 链接：https://leetcode.com/problems/longest-substring-without-repeating-characters/\n 如下为Python3代码\nclass Solution(object): def lengthOfLongestSubstring(self, s): \u0026#34;\u0026#34;\u0026#34; :type s: str :rtype: int \u0026#34;\u0026#34;\u0026#34; b, m, d = 0, 0, {} for i, l in enumerate(s): b, m, d[l] = max(b, d.get(l, -1) + 1), max(m, i - b), i return max(m, len(s) - b)  参考内容\n","title":"Leetcode 3 Longest Substring Without Repeating Characters"},{"location":"https://codenow.me/algorithm/leetcode-14-longest-common-prefix/","text":" 题号：14\n难度：easy\n链接：https://leetcode.com/problems/longest-common-prefix\n描述：多个字符串找公共子串(a-z)\n from typing import List class Solution: def longestCommonPrefix2(self, strs: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;简单粗暴的方式\u0026#34;\u0026#34;\u0026#34; res = \u0026#39;\u0026#39; if not strs: return res for char in zip(*strs): if len(set(char)) == 1: res += char[0] else: return res return res def longestCommonPrefix(self, strs: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;不需要一个一个字符地加，只需要找到分界点，一次性取出即可\u0026#34;\u0026#34;\u0026#34; if not strs: return \u0026#39;\u0026#39; for i, letter_group in enumerate(zip(*strs)): if len(set(letter_group)) \u0026gt; 1: return strs[0][:i] else: return min(strs) if __name__ == \u0026#39;__main__\u0026#39;: a = [\u0026#34;flower\u0026#34;,\u0026#34;flow\u0026#34;,\u0026#34;flight\u0026#34;] res = Solution().longestCommonPrefix(a) print(res)","title":"Leetcode 14 Longest Common Prefix"},{"location":"https://codenow.me/tips/exmail-smtp-tls/","text":"配置腾讯企业邮箱 exmail 时，虽然官方文档上的 SMTP 端口是 465，但那是支持 SSL 校验验证的，如果客户端只支持 TLS 的话，需要把 SMTP 端口配置成 587，才能配置成功\n","title":"Exmail Smtp TLS"},{"location":"https://codenow.me/articles/sentry-raven-vs-sentry_sdk/","text":" 上周写了 Sentry 的 Python SDK raven 包的工作原理，但这周发现 Sentry Team 把 SDK 又给重构了一版，现在叫做 New Unified Version: sentry-sdk。\n但是更新工作没做彻底， 官网(https://docs.sentry.io) 上的文档虽然已经改成 sentry-sdk 了，但 sentry-web 服务上的各种说明还没改，而且新版 SDK 的版本号还没到 1.0(0.7.10)，raven 已经完善到 6.10.0 了\n于是又读了一下这个包的代码，与之前真的大不相同了。\n以下内容分两部分：\n 对 SDK 的用户来说，都有哪些变化 新 SDK 在实现方式上有什么变化  新旧 SDK 变化对比 简单写了个表，功能上的区别基本就是这样了，实际使用上倒是完全感受不到什么\n   对比项 raven sentry_sdk     捕捉错误的方式 flask.got_request_exception + middleware flask.got_request_exception + middleware   获取上下文等信息的方式(flask) 直接访问 flask.request，以及 sys.exc_info 和环境相关信息 使用信号 appcontext_pushed, request_started 和 sys.exc_info，以及一些环境相关信息   同步？异步？ 默认异步，用 queue.Queue 来保证 只支持异步，同样用 queue.Queue 来保证   发送方式 默认 urllib2.urlopen，可更换为 requests 使用 urllib3.PoolManager 做 HTTP 连接池   配置方式 默认读取 SENTRY_开头的所有 flask.app 配置及超多环境变量 默认读取 SENTRY_DSN, SENTRY_RELEASE, SENTRY_ENVIRONMENT 环境变量   发送给服务端的信息 正常 多返回了 当前python环境的所有已安装包    感觉比较明显的改动有四个：\n 新版在兼容 flask 时不需要传入 flask app 新版少了很多配置项 新版使用了 HTTP 连接池 发送事件(event)更方便了  兼容 flask 的时候，只需要这样写：\nimport sentry_sdk from sentry_sdk.integrations.flask import FlaskIntegration sentry_sdk.init(integrations=[FlaskIntegration()]) flask 是作为一个“插件”一样的东西加进去的，里面完成了信号关联，并且在信号回调里也是直接调用的 sentry API，而没有做更定制化的处理\n少的那些配置，我猜是因为新版 SDK 优先实现了核心功能，剩下那些配置要不要加还不知道\nHTTP 连接池加的倒是正好，因为所有发送给服务端的事件都是走的同一个 API: {scheme}://{host}/api/{project_id}/store\n至于发送 event 更方便，则是因为新版把 API 的概念明确做到了 SDK 里，并且只需要\nfrom sentry_sdk import capture_message capture_message(\u0026#39;this message will be sent to sentry server directly\u0026#39;) 就可以直接发送了。而之前由于所有的一切都与 client 有关，都需要依赖 client 对象，因此必须先获取到 client 对象，才能做后面的事。\n那么新版是怎么做到可以不依赖特定 client 对象的呢？\n新 SDK 实现方式详细说明 还是先看看代码目录\nsentry_sdk ├── integrations # 类似与插件，中间件，钩子的东西，用于与其他框架和环境 ├── __init__.py ├── _compat.py # 为各个 python 版本做兼容 ├── api.py # 用户可直接使用的 API 方法 ├── client.py # 包含 Client 类，用于构建时间并发送给服务端 ├── consts.py # 一些常量定义如版本号 ├── debug.py # debug 支持 ├── hub.py # 新版最关键的类，用于并发管理 ├── scope.py # 保存所有待发送给服务端的周边信息 ├── transport.py # 同之前，实际发消息给服务端的东西 ├── utils.py # 一些内部工具 └── worker.py # 执行发送任务的后台 worker  结构很清晰，其中 client, transport, worker, processor 等都与之前概念相同，这里只说一下几个新增概念\n api scope hub  Unified API 关于这个，官方文档里有详细说明，参见：https://docs.sentry.io/development/sdk-dev/unified-api，最后点阅读全文可以直接跳转过去看\npython 新版里给出了 8 个可以全局调用的 API：\n capture_event(event, hit=None): 发送事件给 sentry 服务端 capture_message(message, level=None): 发送一个字符串给 sentry 服务端 capture_exception(error=None): 将一个异常发送给 sentry 服务端 add_breadcrumb(crumb=None, hint=None, **kwargs): 增加 breadcrumb configure_scope(callback=None): 修改 scpoe 配置内容 push_scope(callback=None): 新增一个 scope 层 flush(timeout=None, callback=None): 等待 timeout 秒来发送当前所有事件 last_event_id(): 上一个提交的 event 的 id  对这些 API 来说，任何地方只要 from sentry_sdk import 之后就可以直接用了（前提是已经 init 过了）\nscope 包含要随着事件发送给服务端的各种数据，包括上下文，扩展信息，事件级别等等。另外各个 processor 里获取的信息，也会存在 scope 里，并随着事件发送给服务端\nscope 和 client 是一一对应的，也就是说，一个 client 只会对应一个 scope，因此同一个 client 发送的多条 event，只会获取一次 scope，除非又做了单独配置\nhub 是新版实现中最关键的一个东西，我们先来看一下新版里 Hub 类的描述\nclass Hub(with_metaclass(HubMeta)): # type: ignore \u0026#34;\u0026#34;\u0026#34;The hub wraps the concurrency management of the SDK. Each thread has its own hub but the hub might transfer with the flow of execution if context vars are available. If the hub is used with a with statement it\u0026#39;s temporarily activated. \u0026#34;\u0026#34;\u0026#34; _stack = None # type: List[Tuple[Optional[Client], Scope]] def __init__(self, client_or_hub=None, scope=None): # type: (Union[Hub, Client], Optional[Any]) -\u0026gt; None if isinstance(client_or_hub, Hub): hub = client_or_hub client, other_scope = hub._stack[-1] if scope is None: scope = copy.copy(other_scope) else: client = client_or_hub if scope is None: scope = Scope() self._stack = [(client, scope)] self._last_event_id = None # type: Optional[str] self._old_hubs = [] # type: List[Hub] 怎么理解呢，就是说 Hub 并不参与 sentry 客户端的原有逻辑，捕获异常，发送给服务端，这些东西没有 Hub 也都可以正常完成，Hub 只是做了一个并发管理，让不同线程都可以在任何地方直接获取到当前的 client 和其对应的 scope，从而完成“捕获-发送”流程。\n我们来看一下 Hub 的构造函数，前面的一大段只做了一件事，就是初始化 self._stack，这里面包含有 client 和 scope 两个参数，有了这两个，正常逻辑就可以走通了。剩下的两个参数，self._last_event_id 不用说了，self._old_hubs 也是一个栈，是在用 with 语句使用 hub 的时候，可以在新的上下文里进行操作，而不会影响到原有的上下文。\nclass Hub(with_metaclass(HubMeta)): def __enter__(self): # type: () -\u0026gt; Hub self._old_hubs.append(Hub.current) _local.set(self) return self def __exit__( self, exc_type, # type: Optional[type] exc_value, # type: Optional[BaseException] tb, # type: Optional[Any] ): # type: (...) -\u0026gt; None old = self._old_hubs.pop() _local.set(old) 可以看到 Hub 用在 with 里时十分简单，但是那个 _local 是什么东西？\n_local = ContextVar(\u0026#34;sentry_current_hub\u0026#34;) _local 与 Hub 在同一个文件里定义，这里的 ContextVar 是在 python3.7 引入的新特性，可以理解为线程级别的上下文变量，详细参见 PEP567。他的作用就是保存当前线程的 HUb 对象，使得同一个线程里，任何通过 _local 得到的 Hub 都是同一个。但是我们知道下划线开头的变量属于内部变量，不建议外部使用，这里怎么让其他地方可以获取到他呢？\ndef with_metaclass(meta, *bases): class metaclass(type): def __new__(cls, name, this_bases, d): return meta(name, bases, d) return type.__new__(metaclass, \u0026#34;temporary_class\u0026#34;, (), {}) class HubMeta(type): @property def current(self): # type: () -\u0026gt; Hub \u0026#34;\u0026#34;\u0026#34;Returns the current instance of the hub.\u0026#34;\u0026#34;\u0026#34; rv = _local.get(None) if rv is None: rv = Hub(GLOBAL_HUB) _local.set(rv) return rv @property def main(self): \u0026#34;\u0026#34;\u0026#34;Returns the main instance of the hub.\u0026#34;\u0026#34;\u0026#34; return GLOBAL_HUB class Hub(with_metaclass(HubMeta)): pass ... GLOBAL_HUB = Hub() _local.set(GLOBAL_HUB) 在第一次看到 Hub 类定义的时候，应该就注意到 with_metaclass(HubMeta) 了吧，这里 with_metaclass 我猜可能是为了兼容老版本，实际上就是给他定义了个元类，并且设置了一个只读参数 Hub.current，效果是从 _local 里或者 GLOBAL_HUB 里获取一个 Hub 实例，并且返回。这里有一点要注意，就是 GLOBLE_HUB 是模块级别的变量，新线程找不到 _local 时都会用这个。\n而 HubMeta.main 是直接返回了 GLOBAL_HUB，我看了下这个函数只在 AtexitIntegration 也就是获取到 shutdown signal 时使用，那时会直接调用 hub.client.close 关掉客户端以及 transport，也就是 SDK 完成使命光荣退出了\n看到这里应该要总结一下了，但是稍等，我们再看一下对外暴露的 API 是怎么实现的\ndef capture_message(message, level=None): # type: (str, Optional[Any]) -\u0026gt; Optional[str] hub = Hub.current if hub is not None: return hub.capture_message(message, level) return None 很简单吧，Hub 类持有着 client 和 scope，所有的操作都直接用 Hub 类操作即可\n总结：\n sentry_sdk 明确了 API 的概念，sentry_sdk.init() 完成后，任何地方都可以直接使用相应 API 完成操作 对各种框架的兼容使用了 Integration 的概念，在其内部只做很小的 hook 或关联，实际操作还是通过 API 来完成 调用 API 的时候，通过 Hub.current 获取到当前线程的 Hub 对象，并在 Hub._stack 里拿到最近的 client 和 scope，然后就可以像旧 SDK 一样对 client 进行各种操作了。  最后，关于多线程下各种情况的处理，按说应该是一个比较重要的部分，但是我还没有做测试，就先放一下吧\n","title":"Sentry Raven vs Sentry_sdk"},{"location":"https://codenow.me/translation/example-of-equi-depth-histograms-in-databases/","text":"原文链接：https://stackoverflow.com/a/14284218\n警告：我不是一个数据库底层的专家，所以这只是一个简单泛泛的回答。\n查询编译器会将查询（一般以 sql 的形式）转换成一个查询计划并得到查询结果。查询计划里面包含数据库引擎的低层次指令，比如扫描表 T 在 C 列查询 V 值；在 T 表使用索引 X 来定位 V 等等。\n查询优化是指编译器要在一些列查询计划中判断出哪个代价最小。代价包括始终时间，IO 单宽，存储空间，cpu 等等。从概念上讲，查询编译器是从一组计划空间中衡量每一种计划的代价，选择它能找到的最小代价的。\n上面提到的这些取决于读写的数据条数，数据是否能够被索引定位到，哪些列会被被使用，数据尺寸，以及多少磁盘空间会被占用。\n这些数据很多时候取决于表中到底存了多少数据。比如这样一个查询：select * from data where pay \u0026gt; 100 ，其中 pay 被索引。如果 pay 列没有大于 100 的值，那么这个查询代价会很小。使用索引扫描就行了。相反的，结果可能包含了整张表。\n这块儿，直方图会起作用（等高直方图只是直方图的其中一种）。在前面的查询中，直方图可以在 O(1) 时间复杂度里对查询匹配的数据行数进行一个评估，在不需要了解数据库里到底有哪些数据的情况下。\n实际上，编译器是在数据的摘要之上\u0026rdquo;执行\u0026rdquo;了查询。直方图就是这个摘要。直方图在评估代价和查询算子的结果大小上很有用，比如：表连接结果大小，插入和删除时被影响到的页数等。\n以一个简单的内连接为例，假如我们知道用来连接两张表的列的数据分布：\nBins (25% each) Table A Table B 0-100 151-300 101-150 301-500 151-175 601-700 176-300 1001-1100  可以很容易看到 A 表中 50% 的数据和 B 表中 25% 的数据会参与到查询当中。如果有唯一列的话，我们可以评估连接的结果大小应该是 max(.5 * |A|, .25 * |B|)。这是很简单的一个列子。在很多的场景中，统计分析需要更复杂一些的数学知识。对于连接来说，经常通过算子的直方图技术连接的评估直方图。\n","title":"Example of Equi Depth Histograms in Databases"},{"location":"https://codenow.me/algorithm/leetcode-49-graph-anagrams/","text":"原题链接 ，难度 Medium：\nclass Solution: def groupAnagrams(self, strs): \u0026#34;\u0026#34;\u0026#34; :type strs: List[str] :rtype: List[List[str]] \u0026#34;\u0026#34;\u0026#34; memo = {} for s in strs: s_ = \u0026#39;\u0026#39;.join(sorted(s)) if s_ in memo: memo[s_].append(s) else: memo[s_] = [s] result = [] for key in memo: result.append(memo[key]) return result","title":"Leetcode 49 Graph Anagrams"},{"location":"https://codenow.me/tips/golang-output-table-in-console/","text":"使用 tablewriter 即可。\n","title":"golang 中输出表格到 console"},{"location":"https://codenow.me/translation/what-every-python-project-should-have/","text":"  原文地址\n Python 语言在过去的几年有着突飞猛进的发展，社区也在快速发展。在发展过程中，社区中出现了许多工具保持着资源的结构性和可获取性。在这篇文章中，我将提供一个简短列表，让每个 Python 项目中都具有可访问性和可维护性。\nrequirements.txt 首先， requirements.txt 在安装项目时候是十分重要的，通常是一个纯文本文件，通过 pip 安装，每行一个项目的依赖。\n真是简单又实用。\n你也可以有多个用于不同目的 requirements.txt。例如，requirements.txt 是让项目正常启动的依赖，requirements_dev.txt 是用于开发模式的依赖，requirements_docs.txt 是生成文档的依赖（像 Sphinx 需要的主题）\nsetup.py setup.py 文件在通过 pip 安装时候时候是十分重要的。编写容易，很好的可配置性并且可以处理很多事情，例如导入，项目元数据，更新源，安装依赖项等等。\n可以查看 setuptools 文档获取更多的信息。\n正确的项目结构 项目结构至关重要。有了一个组织良好的结构，它会更容易组织的东西，找到某些源文件，并鼓励其他人贡献。\n一个项目目录应具有类似的结构\nroot/ docs/ tests/ mymodule/ scripts/ requirements.txt setup.py README LICENSE  当然，这不是组织项目的唯一方法，但这肯定是最常用的模板。\n测试 单元测试对项目十分重要，可以保证代码的稳定性。我推荐 unittest 模块，因为它是内置的，并且足够灵活，完成正确工作。\n还有其他可用于测试项目的库，例如 test.py 或 nose。\n文档 如果你开发一个项目，我确信你不只是为你自己写。其他人也要必须知道如何使用你的项目。即使你只是为自己编写的项目（虽然是开源的目的），但是一段时间后不开发后，你一定不会记得你的代码中发生的任何事情（或API）。\n因此，为了实现可重用的代码，你应该：\n 设计一个简单的API，易于使用和记忆 API应该足够灵活，容易配置 记录相关使用例子 例子不要追求 100% ，最合适的是覆盖 80％ 。  为了充分的记录你的代码，你应该使用特殊的工具开完成文档工作，例如 Sphinx 或者 mkdocs ，所以你可以使用一个流行的标记语言（rst或markdown）来生成具有适当引用链接的漂亮的文档。\n结论 在熟悉上述话题之后，一定能够生成符合社区标准的漂亮的结构化项目和库。不要忘记总是使用PEP-8！\n","title":"「译」Python 项目应该都有什么？"},{"location":"https://codenow.me/algorithm/word-break/","text":" 题号：139\n难度：中等\n链接：https://leetcode-cn.com/problems/word-break/submissions/\n class Solution: def wordBreak(self, s: str, wordDict: List[str]) -\u0026gt; bool: if not s: return True word_idx = [0] for i in range(len(s) + 1): for j in word_idx: if s[j:i] in wordDict: word_idx.append(i) break return word_idx[-1] == len(s)","title":"Leetcode: 139 Word Break"},{"location":"https://codenow.me/tips/goland-refactor/","text":"Goland 的 Refactor 功能，在修改、移动等操作时候可以对整个文件或者工程生效。\n","title":"Goland Refactor"},{"location":"https://codenow.me/articles/linux-cgroups/","text":" cgroups 是 Linux 内核中的一个功能，用来限制、控制分离一个进程的资源，比如 CPU、内存、IO 等。\ncgroups 是由一组子系统构成，每种子系统即时一种资源，目前可使用的资源如下：\n cpu：限制 cpu 的使用率 cpuacct：cpu 的统计报告 cpuset：分配 cpu memory：分配 mem 的使用量 blkio：限制块设备的 io devices：能够访问的设备 net_cls：控制网络数据的访问 net_prio：网络流量包的优先级 freezer：pause 或者 resume 进程 ns：控制 namespace 的访问  cgroups 中有个 hierarchy 的概念，意思一组 cgroup 是一棵树，cgroup2 可以挂在 cgroup 1 上，这样可以从 cgroup1 中继承设置。\n所以 process、subsystem、hierarchy 存在一些关系。\n 一个 subsystem 只能附加到一个 hierarchy 一个 hierarchy 可以附加到多个 subsystem 中 一个 process 可以作为多个 cgroups 成员，但是要在不同的 hierarchy 中 fork 出的子进程默认和父进程使用一个 cgroups，但是可以移动到其他的 cgroups 中  在 linux 中 /sys/fs/cgroup 中是 cgroups 默认的 hierarchy，可以看到目前的 subsystem\ndr-xr-xr-x 6 root root 0 Dec 17 17:31 blkio lrwxrwxrwx 1 root root 11 Dec 17 17:31 cpu -\u0026gt; cpu,cpuacct dr-xr-xr-x 6 root root 0 Dec 17 17:31 cpu,cpuacct lrwxrwxrwx 1 root root 11 Dec 17 17:31 cpuacct -\u0026gt; cpu,cpuacct dr-xr-xr-x 4 root root 0 Dec 17 17:31 cpuset dr-xr-xr-x 6 root root 0 Dec 17 17:31 devices dr-xr-xr-x 4 root root 0 Dec 17 17:31 freezer dr-xr-xr-x 7 root root 0 Dec 17 17:31 memory lrwxrwxrwx 1 root root 16 Dec 17 17:31 net_cls -\u0026gt; net_cls,net_prio dr-xr-xr-x 3 root root 0 Dec 17 17:31 net_cls,net_prio lrwxrwxrwx 1 root root 16 Dec 17 17:31 net_prio -\u0026gt; net_cls,net_prio dr-xr-xr-x 3 root root 0 Dec 17 17:31 perf_event dr-xr-xr-x 3 root root 0 Dec 17 17:31 pids dr-xr-xr-x 5 root root 0 Dec 17 17:31 systemd 假如我们想给一个进程添加内存限制，第一步需要创建一个 hierarchy 在 /sys/fs/cgroup/memory 中\nsudo mkdir /sys/fs/cgroup/memory/mytestcgroup 系统会帮助我们创建一系列文件，这是因为我们挂载的类型是 cgroup，cgroup 的 hierarchy 目录会被映射成文件目录，方便操作：\n-rw-r--r-- 1 root root 0 Apr 14 21:36 cgroup.clone_children --w--w--w- 1 root root 0 Apr 14 21:36 cgroup.event_control -rw-r--r-- 1 root root 0 Apr 14 21:36 cgroup.procs -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.failcnt --w------- 1 root root 0 Apr 14 21:36 memory.force_empty -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.limit_in_bytes -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.max_usage_in_bytes -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.memsw.failcnt -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.memsw.limit_in_bytes -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.memsw.max_usage_in_bytes -r--r--r-- 1 root root 0 Apr 14 21:36 memory.memsw.usage_in_bytes -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.move_charge_at_immigrate -r--r--r-- 1 root root 0 Apr 14 21:36 memory.numa_stat -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.oom_control ---------- 1 root root 0 Apr 14 21:36 memory.pressure_level -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.soft_limit_in_bytes -r--r--r-- 1 root root 0 Apr 14 21:36 memory.stat -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.swappiness -r--r--r-- 1 root root 0 Apr 14 21:36 memory.usage_in_bytes -rw-r--r-- 1 root root 0 Apr 14 21:36 memory.use_hierarchy -rw-r--r-- 1 root root 0 Apr 14 21:36 notify_on_release -rw-r--r-- 1 root root 0 Apr 14 21:36 tasks 在上面的文件中我们可以看到 tasks，这里面放着就是被限制的进程 pid，我们把当前 session 的 pid 放入 task 中，以后从这个 session 启动的进程将会被限制，比如限制一下内存只能使用 100m。\nsudo bash -c \u0026#34;echo \u0026#34;100m\u0026#34; \u0026gt; memory.limit_in_bytes\u0026#34; sudo bash -c \u0026#34;echo $$\u0026gt; tasks\u0026#34; 然后使用 stress 工具启动一个测压\nstress --vm-bytes 200m --vm-keep -m 1 最后通过 top 等工具可以发现内存被限制到了 100m。\nGo 语言控制 cgroup Go 语言中并没有特殊的 API 接口来处理 cgroup，依然是通过和命令行一样的模式（读写文件）来控制 cgroup。\n所以，在 go 语言中就是创建文件夹，删除文件加，写入文件这三个操作来使用 cgroup 功能。\n","title":"CGroups 控制进程资源"},{"location":"https://codenow.me/articles/tidb-join-performance-optimization-1/","text":" 最近在尝试自己写数据库查询模块，满足 http://sigmod18contest.db.in.tum.de/task.shtml 的功能要求。一边看着 TiDB 的代码，一边写… 这个过程中发现了一些 TiDB 优化点。\njoin reorder 每个数据库系统基本都要实现 join reorder，修改表连接的顺序，从而提高 join 的性能，比如下面这个查询：\nselect a.id, b.id, c.id from a, b, c where a.id = b.a_id and a.id = c.a_id; 查询要连接 a/b/c 三张表，可以先连接 a 和 b，也可以先连接 a 和 c，当然如果你想不开的话，也可以先连接 a 和 c。如果 a join b 产生的数据比 a join c 产生的数据多，那么先计算 a join c 一般性能会更好。\n很多数据库在表少的时候会使用动态规划来解决这个问题，比如 这篇文章 中介绍的算法。大致思路是根据表和用来连接的条件看做是一个无环图，表是节点，筛选条件是边。要计算最优的连接顺序，就是根据这张图计算出一个 sJoin Tree，Join Tree 除叶子节点以外其他的节点都是 Join。动规的过程是将图拆分成各种子图的组合并从中找出最优组合。\n下面是 TiDB 中 join reorder 的主体代码：\nfunc (s *joinReorderDPSolver) solve(joinGroup []LogicalPlan, conds []expression.Expression) (LogicalPlan, error) { // joinGroup 可以简单认为是要连接的 table 列表，代码中先计算出图的邻接表的结构和“边”列表 \tadjacents := make([][]int, len(joinGroup)) totalEdges := make([]joinGroupEdge, 0, len(conds)) addEdge := func(node1, node2 int, edgeContent *expression.ScalarFunction) { totalEdges = append(totalEdges, joinGroupEdge{ nodeIDs: []int{node1, node2}, edge: edgeContent, }) adjacents[node1] = append(adjacents[node1], node2) adjacents[node2] = append(adjacents[node2], node1) } // Build Graph for join group \tfor _, cond := range conds { // 根据筛选条件的列，找到每个条件中连接的表，记录表之间的连接关系 \tsf := cond.(*expression.ScalarFunction) lCol := sf.GetArgs()[0].(*expression.Column) rCol := sf.GetArgs()[1].(*expression.Column) lIdx, err := findNodeIndexInGroup(joinGroup, lCol) //... \trIdx, err := findNodeIndexInGroup(joinGroup, rCol) //... \taddEdge(lIdx, rIdx, sf) } visited := make([]bool, len(joinGroup)) nodeID2VisitID := make([]int, len(joinGroup)) var joins []LogicalPlan // BFS the tree. \t// 使用 BFS 计算出联通子图，如果存在多个子图，子图之间没有连接关系，子图之间 join 结果是他们的笛卡尔乘积 \tfor i := 0; i \u0026lt; len(joinGroup); i++ { if visited[i] { continue } visitID2NodeID := s.bfsGraph(i, visited, adjacents, nodeID2VisitID) // Do DP on each sub graph. \t// 使用 DP 算法找到每个子图的最优 join 顺序 \tjoin, err := s.dpGraph(visitID2NodeID, nodeID2VisitID, joinGroup, totalEdges) if err != nil { return nil, err } joins = append(joins, join) } // Build bushy tree for cartesian joins. \treturn s.makeBushyJoin(joins), nil } 下面是 bp 部分的代码，算法使用位图来表示不同的子图，使用了自下而上的方式，从小到大的计算每个子图的最优 join 顺序，从而最终计算出整个图的最优解。算法中使用了位图，拆分子图和判断子图之间是否连接的代码感觉很棒，非常的简洁高效。\nfunc (s *joinReorderDPSolver) dpGraph(newPos2OldPos, oldPos2NewPos []int, joinGroup []LogicalPlan, totalEdges []joinGroupEdge) (LogicalPlan, error) { // 使用位图来表示不同子图，使用自下而上的方式计算每个子图的最优 join 顺序 \tnodeCnt := uint(len(newPos2OldPos)) bestPlan := make([]LogicalPlan, 1\u0026lt;\u0026lt;nodeCnt) bestCost := make([]int64, 1\u0026lt;\u0026lt;nodeCnt) // bestPlan[s] is nil can be treated as bestCost[s] = +inf. \tfor i := uint(0); i \u0026lt; nodeCnt; i++ { bestPlan[1\u0026lt;\u0026lt;i] = joinGroup[newPos2OldPos[i]] } // 从小到大罗列所有子图 \tfor nodeBitmap := uint(1); nodeBitmap \u0026lt; s \u0026lt;\u0026lt; nodeCnt); nodeBitmap++ { if bits.OnesCount(nodeBitmap) == 1 { continue } // This loop can iterate all its subset. \tfor sub := (nodeBitmap - 1) \u0026amp; nodeBitmap; sub \u0026gt; 0; sub = (sub - 1) \u0026amp; nodeBitmap { remain := nodeBitmap ^ sub if sub \u0026gt; remain { // 由于是无向图，所有相同两个子图的组合，只计算一遍 \tcontinue } // 如果 sub/remain 这两个子图中某一个不是强连通的，不继续计算 \tif bestPlan[sub] == nil || bestPlan[remain] == nil { continue } // Get the edge connecting the two parts. \tusedEdges := s.nodesAreConnected(sub, remain, oldPos2NewPos, totalEdges) if len(usedEdges) == 0 { // 如果 sub 和 remain 是不连通的，也不再继续计算 \tcontinue } join, err := s.newJoinWithEdge(bestPlan[sub], bestPlan[remain], usedEdges) if err != nil { return nil, err } // 更新 nodeBitmap 所代表的子图中最优的 join 顺序 \tif bestPlan[nodeBitmap] == nil || bestCost[nodeBitmap] \u0026gt; join.statsInfo().Count()+bestCost[remain]+bestCost[sub] { bestPlan[nodeBitmap] = join bestCost[nodeBitmap] = join.statsInfo().Count() + bestCost[remain] + bestCost[sub] } } } return bestPlan[(1\u0026lt;\u0026lt;nodeCnt)-1], nil } 需要注意的是，bp 算法中需要估算每个 join 的代价，评估代价的过程当中需要使用统计信息，统计信息有的时候会不准确，这会影响 bp 算法的结果。\n补充知识\n从上面的代码可以看到，评估 join 代价的时候主要还是看 join.StatsInfo().Count() 的数值大小，这个数值表示 join 会产生的数据条数。评估 join 的数据条数和评估单表的数据条数的计算方法不同，这块的知识可以看一下 数据库概念 13.3.3 的讲解和 TiDB 的代码实现。\n复用 Chunk 为了提高查询执行器的执行速度，特别是在数据量比较大的情况下，TiDB 使用了 chunk。在执行查询的过程中，执行器每次不再只返回一条数据，而是返回一组数据。\n除了使用 Chunk 以外，TiDB 的执行器还增加了 Chunk 复用的逻辑，有效的降低了内存的开销。在做一个数据量很大的 HashJoin 时（比如外表有几百万条数据），TiDB 会启动多个 worker 来计算 join 结果，worker 之间通过 Chunk 分发任务、接收计算结果。如果没有复用 Chunk 的话，查询过程会差生大量的 Chunk，GC 势必会影响性能。\nTiDB 中当 worker 使用完了 chunk 以后，会通过特定的 channel 将 chunk 还回从而实现 Chunk 的复用。这块的代码不易拆分出来，暂略。\n","title":"Tidb 源码学习：关于 join 性能优化"},{"location":"https://codenow.me/translation/did_you_know_you_can_easily_extend_and_expand_your_datacenter_footprint/","text":" 您是否知道可以轻松地延伸和扩展您的数据中心占用空间? 原文链接：http://app.learn.vmware.com/e/es?s=279193683\u0026amp;e=2902981\u0026amp;elqTrackId=80b432a7915c49a0a70c7d2846b7ff14\u0026amp;elq=52464596ac594d2d93bd227a6575bdc5\u0026amp;elqaid=25584\u0026amp;elqat=1\n翻译如下：\n现代IT团队承受着难以置信的压力，要以越来越快的速度创新和推出产品和服务，但在快速扩展产品方面存在许多障碍。其中最主要的挑战是如何快速轻松地利用公共云提供的无限可伸缩性、随需应变能力和灵活的消费。\nAWS上的VMware Cloud是一种随需应变服务，它允许您跨基于websphere的云环境运行应用程序，并访问广泛的本机AWS服务。\n该服务由VMware Cloud Foundation™提供支持，集成了VMware的旗舰计算、存储和网络虚拟化产品(VMware vSphere、VMware vSAN和VMware NSX)以及VMware vCenter management，并优化为在弹性的、裸机AWS基础设施上运行。\n有了这项服务，客户可以使用熟悉的VMware工具管理基于云的资源。\n","title":"Did_you_know_you_can_easily_extend_and_expand_your_datacenter_footprint"},{"location":"https://codenow.me/algorithm/leetcode_216_combination_sum_iii/","text":"题号：216 难度：Medium 链接：https://leetcode.com/problems/combination-sum-iii/\n#!/usr/bin/python # -*- coding: utf-8 -*- class Solution: def combinationSum3(self, k, n): result_list = [] def f(k, n, cur, next): if len(cur) == k: if sum(cur) == n: result_list.append(cur) return for i in range(next, n+1): f(k, n, cur+[i], i+1) f(k, n, [], 1) return result_list if __name__ == \u0026#39;__main__\u0026#39;: k = 3 n = 7 print(Solution().combinationSum3(k, n)) k = 3 n = 9 print(Solution().combinationSum3(k, n)) C:\\Python37\\python.exe C:/python_workspace/leecode/array/leecode_216.py [[1, 2, 4]] [[1, 2, 6], [1, 3, 5], [2, 3, 4]]","title":"Leetcode_216_Combination_Sum_III"},{"location":"https://codenow.me/articles/docker_process_about_run_shell_and_exec/","text":" 1. 前言 Docker容器内运行的进程对于宿主机而言，是独立进程，还是Docker容器进程？\nDocker容器内启动的进程全部都是宿主机上的独立进程\nDocker容器内启动的进程是不是Docker进程本身要看Dockerfile的写法\n比如Docker内启动redis，如果用CMD \u0026ldquo;/usr/bin/redis-server\u0026rdquo;，这是用shell启动，会先启动shell，然后再启动redis，所以不是Docker进程本身；\n如果用CMD [\u0026ldquo;/usr/bin/redis-server\u0026rdquo;]，这是用exec启动，是直接启动redis，进程号为1，所以是Docker进程本身\n2. shell方式 1) shell方式的Dockerfile\nroot@ubuntu:~# cat Dockerfile FROM ubuntu:18.04 RUN apt-get update \u0026amp;\u0026amp; apt-get -y install redis-server \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* EXPOSE 6379 CMD \u0026#34;/usr/bin/redis-server\u0026#34; 2)shell方式创建容器\nroot@ubuntu:~# docker build -t redisshell -f Dockerfile . 3) shell方式创建并运行镜像\nroot@ubuntu:~# docker run --name redisshell redisshell 4) redisshell容器内进程\nroot@ubuntu:~# docker exec -it redisshell ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 17:36 ? 00:00:00 /bin/sh -c \u0026#34;/usr/bin/redis-ser root 6 1 0 17:36 ? 00:00:00 /usr/bin/redis-server *:6379 root 10 0 0 17:37 pts/0 00:00:00 ps -ef 5) 宿主机进程\nroot@ubuntu:~# ps -ef|grep redis root 4151 2874 0 10:36 pts/0 00:00:00 docker run --name redisshell redisshell root 4202 4176 0 10:36 ? 00:00:00 /bin/sh -c \u0026#34;/usr/bin/redis-server\u0026#34; root 4252 4202 0 10:36 ? 00:00:00 /usr/bin/redis-server *:6379 root 4392 4102 0 10:39 pts/1 00:00:00 grep --color=auto redis 3. exec方式 1) exec方式的Dockerfile\nroot@ubuntu:~# cat Dockerfile FROM ubuntu:18.04 RUN apt-get update \u0026amp;\u0026amp; apt-get -y install redis-server \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* EXPOSE 6379 CMD [\u0026#34;/usr/bin/redis-server\u0026#34;] 2)shell方式创建容器\nroot@ubuntu:~# docker build -t redisexec -f Dockerfile . 3) shell方式创建并运行镜像\nroot@ubuntu:~# docker run --name redisexec redisshell 4）redisexec器内进程\nroot@ubuntu:~# docker exec -it redisexec ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 17:44 ? 00:00:00 /usr/bin/redis-server *:6379 root 9 0 0 17:45 pts/0 00:00:00 ps -ef 5) 宿主机进程\nroot@ubuntu:~# ps -ef|grep redis root 4151 2874 0 10:36 pts/0 00:00:00 docker run --name redisshell redisshell root 4202 4176 0 10:36 ? 00:00:00 /bin/sh -c \u0026#34;/usr/bin/redis-server\u0026#34; root 4252 4202 0 10:36 ? 00:00:01 /usr/bin/redis-server *:6379 root 4442 4102 0 10:44 pts/1 00:00:00 docker run --name redisexec redisexec root 4496 4466 0 10:44 ? 00:00:00 /usr/bin/redis-server *:6379 root 4646 4561 0 10:46 pts/2 00:00:00 grep --color=auto redis 4. 两种方式的区别 除了进程是否独立有一定的区别外，两种启动模式导致进程的退出机制也完全不同，从而形成了僵尸进程和孤儿进程\n4.1 具体说来。Docker提供了docker stop和docker kill两个命令向容器中的1号进程发送信号 1） 当执行docker stop命令时，Docker会首先向容器的1号进程发送一个SIGTERM信号，用于容器内程序的退出。\n如果容器在收到SIGTERM信号后没有结束进程，那么Docker Daemon会在等待一段时间(默认是10秒)后再向容器发送SIGKILL信号，将容器杀死并变为退出状态\n这种方式给Docker应用提供了一个优雅的退出机制，允许应用在收到stop命令时清理和释放使用中的资源\n2）docker kill命令可以向容器内的1号进程发送任何信号，默认是发送SIGKILL信号来强制退出\n3) 从Docker1.9版本开始，Docker支持停止容器时向其发送自定义信号量，并指明容器退出机制，该参数的缺省值是SIGTERM\n4.2 两种方式运行docker stop结果不一样： 1)exec方式\nroot@ubuntu:~# docker stop redisexec redisexec root@ubuntu:~# docker logs -f redisexec 1:C 08 Apr 17:44:48.982 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1:C 08 Apr 17:44:48.982 # Redis version=4.0.9, bits=64, commit=00000000, modified=0, pid=1, just started 1:C 08 Apr 17:44:48.982 # Warning: no config file specified, using the default config. In order to specify a config file use /usr/bin/redis-server /path/to/redis.conf 1:M 08 Apr 17:44:48.985 * Running mode=standalone, port=6379. 1:M 08 Apr 17:44:48.985 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1:M 08 Apr 17:44:48.985 # Server initialized 1:M 08 Apr 17:44:48.985 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add \u0026#39;vm.overcommit_memory = 1\u0026#39; to /etc/sysctl.conf and then reboot or run the command \u0026#39;sysctl vm.overcommit_memory=1\u0026#39; for this to take effect. 1:M 08 Apr 17:44:48.986 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command \u0026#39;echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled\u0026#39; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 1:M 08 Apr 17:44:48.986 * Ready to accept connections 1:signal-handler (1554746551) Received SIGTERM scheduling shutdown... 1:M 08 Apr 18:02:31.880 # User requested shutdown... 1:M 08 Apr 18:02:31.880 * Saving the final RDB snapshot before exiting. 1:M 08 Apr 18:02:31.892 * DB saved on disk 1:M 08 Apr 18:02:31.892 # Redis is now ready to exit, bye bye... 在容器日志中看到了\u0026rdquo;Received SIGTERM scheduling shutdown\u0026hellip;\u0026ldquo;的内容，说明redis-server进程已经接收到了SIGTERM消息，并优雅地关闭了资源\n2）shell方式\nroot@ubuntu:~# docker stop redisshell redisshell root@ubuntu:~# docker logs -f redisshell 6:C 08 Apr 17:36:54.062 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 6:C 08 Apr 17:36:54.063 # Redis version=4.0.9, bits=64, commit=00000000, modified=0, pid=6, just started 6:C 08 Apr 17:36:54.063 # Warning: no config file specified, using the default config. In order to specify a config file use /usr/bin/redis-server /path/to/redis.conf 6:M 08 Apr 17:36:54.067 * Running mode=standalone, port=6379. 6:M 08 Apr 17:36:54.067 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 6:M 08 Apr 17:36:54.068 # Server initialized 6:M 08 Apr 17:36:54.068 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add \u0026#39;vm.overcommit_memory = 1\u0026#39; to /etc/sysctl.conf and then reboot or run the command \u0026#39;sysctl vm.overcommit_memory=1\u0026#39; for this to take effect. 6:M 08 Apr 17:36:54.068 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command \u0026#39;echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled\u0026#39; as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. 6:M 08 Apr 17:36:54.068 * Ready to accept connections docker stop redisshell 容器停止缓慢，而且容器没有优雅关机的内容\n原因在于，用shell脚本启动的容器，其1呈进程是shell进程，shell进程中没有对SIGTERM信号的处理逻辑，所以它忽略了接收到的SIGTERM信号\n当Docker等待stop命令执行10秒超时之后，Docker Daemon将发送SIGKILL信号强制杀死1号进程，并销毁它的PID命名空间\n其子进程redis-servere也在收到SIGKILL信号后被强制终止并退出。\n如果此时应用中还有正在执行的事务或未持久化的数据，强制退出可能导致数据丢失或状态不一致\n5. 总结 所以，容器的1号进程必须能够正确的处理SIGTERM信号来支持优雅退出，如果容器中包含多个进程，则需要1号进程能够正确地传播SIGTERM信号来结束所有的进程，之后再退出\n当然，更正确的做法是，令每一个容器中只包含一个进程，同时采用exec模式启动进程。\n这也是Docker官方推荐的做法。\n","title":"运行shell和exec命令时，Docker进程的区别"},{"location":"https://codenow.me/tips/change_nginx_group_user/","text":" 1. Nginx默认用户  为了让Web服务更安全没需要尽可能地改掉软件默认的所有配置，包括端口、用户等。\n首先查看Nginx服务的默认用户，一般情况下，Nginx服务启动的用户是Nobody,查看默认的配置文件，代码如下：\n[root@localhost conf]# grep \u0026#39;#user\u0026#39; /etc/nginx/nginx.conf.default #user bobody;  为了防止黑客猜到这个Web服务的用户，我们需要将其更改成特殊的用户名；下面以nginx用户为例进行说明。\n 2. 为nginx服务建立新用户 [root@localhost conf]# useradd nginx -s /sbin/nologin -M #\u0026lt;== 不需要有系统登录权限，应当禁止其登录能力，相当于Apache里的用户 [root@localhost conf]# id nginx #\u0026lt;==检查用户  3. 修改Nginx默认用户的两种方法  3.1 在编译Nginx软件时直接指定编译的用户和组，命令如下\n[root@lnmp nginx-1.14.0]# ./configure --user=nginx --group=nginx ...  3.2 配置Nginx 服务，让其使用刚建立的Nginx用户\n[root@lnmp nginx-1.14.0]# egrep \u0026#34;user\u0026#34; /etc/nginx/conf/nginx.conf user nginx; [root@lnmp nginx-1.14.0]# ps -ef|grep nginx|grep -v grep nginx 56998 56721 0 21:18 ? 00:00:00 nginx: worker process  ","title":"更改Nginx默认用户与组"},{"location":"https://codenow.me/algorithm/travelsallistwithoutloops/","text":"之前一次面试上遇到的题，不允许用 for 循环和 while 循环，要求遍历出列表每个元素。 当时没有想法，回去后就查了一下，原来可以用递归解决。\nclass Solution(): def travelsal_list_with_recursion(self, Ls, index=0): if len(Ls) == index: return print(Ls[index], end=\u0026#39; \u0026#39;) self.travelsal_list_with_recursion(Ls, index+1) if __name__ == \u0026#39;__main__\u0026#39;: s = Solution() ls = [\u0026#39;a\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;3\u0026#39;] s.travelsal_list_with_recursion(ls) ","title":"使用递归思想进行列表遍历"},{"location":"https://codenow.me/tips/mysql_where11/","text":"这周工作时看了同事的代码，发现他写的很多 sql 语句都在后面加上了 where1=1，研究了一下，才发现了他的作用和好处。\n例子：\nstring MySqlStr=”select * from table where”； if(Age.Text.Lenght\u0026gt;0) { MySqlStr=MySqlStr+“Age=“+“\u0026#39;Age.Text\u0026#39;“； } if(Address.Text.Lenght\u0026gt;0) { MySqlStr=MySqlStr+“and Address=“+“\u0026#39;Address.Text\u0026#39;“； } 如果两个条件都符合，即 sql 语句 \u0026ldquo;select * from table where Age=x and Address=xx\u0026rdquo; 成立。 但是如果两个条件都不符合，则语句变成了 \u0026ldquo;select * from table where\u0026rdquo;，这个时候就会报错。 使用 where 1=1 可以避免上面发生的问题。\n参考链接： mysql中使用 where 1=1和 0=1 的作用及好处\n","title":"在 Mysql 里使用 where1=1 的作用"},{"location":"https://codenow.me/articles/about_mysql_locks/","text":" Mysql 锁：灵魂七拷问 作者：柳树 on 美业 from 有赞coder\n原链接： Mysql 锁：灵魂七拷问\n一、缘起 假设你想给别人说明，Mysql 里面是有锁的，你会怎么做？\n大多数人，都会开两个窗口，分别起两个事务，然后 update 同一条记录，在发起第二次 update 请求时，block，这样就说明这行记录被锁住了： 二、禁锢 问题来了，貌似只有显式的开启一个事务，才会有锁，如果直接执行一条 update 语句，会不会加锁呢？\n比如直接执行：\nupdate t set c = c + 1 where id = 1; 这条语句，前面不加 begin，不显式开启事务，那么 Mysql 会不会加锁呢？\n直觉告诉你，会。\n但是为什么要加锁？\n给你五秒钟，说出答案。\n学过多线程和并发的同学，都知道下面这段代码，如果不加锁，就会有灵异事件：\ni++; 开启十个线程，执行 1000 次这段代码，最后 i 有极大可能性，会小于 1000。\n这时候，用 Java 的套路，加锁：\nsynchornize { i++; } 问题解决。\n同理，对于数据库，你可以理解为 i，就是数据库里的一行记录，i++ 这段代码，就是一条 update 语句，而多线程，对应的就是数据库里的多个事务。\n既然对内存中 i 的操作需要加锁，保证并发安全，那么对数据库的记录进行修改，也必须加锁。\n这道理很简单，但是很多人，未曾想过。\n三、释然 为什么大家都喜欢用第一部分里的例子来演示 Mysql 锁？\n因为开两个事务，会 block，够直观。\n那么问题又来了，为什么会 block，或者说，为什么 Mysql 一定要等到 commit 了，才去释放锁？\n执行完一条 update 语句，就把锁释放了，不行吗？\n举个例子就知道 Mysql 为什么要这么干了： 一开始数据是：{id:1,c:1}；\n接着事务A通过 select .. for update，进行当前读，查到了 c=1；\n接着它继续去更新，把 c 更新成 3，假设这时候，事务 A 执行完 update 语句后，就把锁释放了；\n那么就有了第 4 行，事务 B 过来更新，把 c 更新成 4；\n结果到了第 5 行，事务 A 又来执行一次当前读，读到的 c，竟然是 4，明明我上一步才把 c 改成了 3\u0026hellip;\n事务 A 不由的发出怒吼：我为什么会看到了我不该看，我也不想看的东西？！\n事务 B 的修改，居然让事务 A 看到了，这明目张胆的违反了事务 ACID 中的 I，Isolation，隔离性（事务提交之前，对其他事务不可见）。\n所以，结论：Mysql 为了满足事务的隔离性，必须在 commit 才释放锁。\n四、自私的基因 有人说，如果我是读未提交（ Read Uncommited ）的隔离级别，可以读到对方未提交的东西，是不是就不需要满足隔离性，是不是就可以不用等到 commit 才释放锁了？\n非也。\n还是举例子： 事务A是 Read Committed，事务B是 Read Uncommitted；\n事务B执行了一条 update 语句，把 c 更新成了3\n假设事务 B 觉得自己是读未提交，就把锁释放了\n那这时候事务 A 过来执行当前读，读到了 c 就是3\n事务 A 读到了别的事务没有提交的东西，而事务 A，还说自己是读已提交，真是讽刺\n根因在于，事务 B 非常自私，他觉得自己是读未提交，就把锁释放了，结果让别人也被“读未提交”\n显然，Mysql 不允许这么自私的行为存在。\n结论：就算你是读未提交，你也要等到 commit 了再释放锁。\n五、海纳百川 都知道 Mysql 的行锁，分为X锁和S锁，为什么 Mysql 要这么做呢？\n这个简单吧，同样可以类比 Java 的读写锁：\nIt allows multiple threads to read a certain resource, but only one to write it, at a time.  允许多个线程同时读，但只允许一个线程写，既支持并发提高性能，又保证了并发安全。\n六、凤凰涅磐 最后来个难点的。\n假设事务 A 锁住了表T里的一行记录，这时候，你执行了一个 DDL 语句，想给这张表加个字段，这时候需要锁表吧？但是由于表里有一行记录被锁住了，所以这时候锁表时会 block。\n那 Mysql 在锁表时，怎么判断表里有没有记录被锁住呢？\n最简单暴力的，遍历整张表，遍历每行记录，遇到一个锁，就说明表里加锁了。\n这样做可以，但是很傻，性能很差，高性能的 Mysql，不允许这样的做法存在。\nMysql 会怎么做呢？\n行锁是行级别的，粒度比较小，好，那我要你在拿行锁之前，必须先拿一个假的表锁，表示你想去锁住表里的某一行或者多行记录。\n这样，Mysql 在判断表里有没有记录被锁定，就不需要遍历整张表了，它只需要看看，有没有人拿了这个假的表锁。\n这个假的表锁，就是我们常说的，意向锁。\nIntention locks are table-level locks that indicate which type of lock (shared or exclusive) a transaction requires later for a row in a table  很多人知道意向锁是什么，但是却不知道为什么需要一个粒度比较大的锁，不知道它为何而来，不知道 Mysql 为何要设计个意向锁出来。\n知其然，知其所以然。\n七、参考文献 InnoDB Locking\nReadWriteLock\n","title":"Mysql 锁：灵魂七拷问"},{"location":"https://codenow.me/translation/4waysdeveloperimprovetools/","text":" 作者： bmusings 原链接： 4 Ways Every Software Developer Should Improve Their Tools\n工具可以让开发人员的想法转换成为计算机能够理解的确切指令。 从文本编辑器到源代码控制，为了技术和效率，它们是我们每天使用上百次的伙伴。和木匠不同，当我们想要更好的工具时，不能直接去商店里买一把更好的锤子。 这引发了一个问题： 我们如何去改善我们的日常工具。 下面我介绍 4 个能够升级工具的方法。\n学习快捷键 设置快捷键和别名是我们使用现有工具最方便最有效的方法。 它通过使用键盘输入代替鼠标点击操作，从而节省时间。 一个人机交互领域出名的法则 Fitt\u0026rsquo;s Law 指出，点击目标所需的时间和指针的距离成正相关，和目标的大小成反比。也就是说，更小更远的目标比更大更近的目标需要花更多的时间。我见过很多次有些专业的软件工程师使用键盘在三个越来越小的目录花费了很多时间只为了操作一个指令，这真是荒谬。学习快捷键进行指令操控会让你把时间控制到常量级别（0.1 - 0.4 秒之间）\n大多数文字编辑器、浏览器还有其它界面工具都有一套可以让你马上使用的快捷键，不要想着一下子就把它们都学会。从一两个开始，把它们拿下，然后再用多几个再熟练，重复这个过程即可。\n有时候你需要使用一系列键盘操作才能完成一个指令，这时候别名和宏就派上用场了。你只需要把你常用的一系列操作用一个简单明了的命令代替就可以了。做好得了的话，一个月可以节省了几个小时的时间呢。\n自定义 有许多你重要的工具（比如你的文本编辑器）会允许你自定义它们的界面，这样不仅仅可以让界面符合你个人的使用习惯，甚至可以加强与别的工具的协同效应。比如说，你是一个 vim 使用者，你经常使用 j 和 k 进行上下移动，那为什么不尝试安装 chrome 插件让你可以在 chrome 里使用一样的快捷键呢？ 你甚至可以不止步于简单的键盘绑定。许多工具都有脚本语言或扩展框架让你可以自己写代码去自定义工具。当然这比简单的键盘绑定花费更多的时间，但是这是一个能够让你明白工具内部运行的好方法，也是一个启动开源项目的好机会，这些都可以让你的简历锦上添花。\n可移植的配置 你安装了所有工具，学了所有的键盘绑定，把它们都自定义化了。 但是突然，你被迫要在一个远程机器或者新电脑上工作，那前面所有努力都白费了，你要从 0 开始，对吗？ 那如果你不仅仅自定义了你的工具，还把相应的配置都变得轻便可以移植了呢？许多工具拥有包含工具配置参数的 .rc 文件或者配置文件。这个可以允许你把它们放到源代码控制里面，或分享到其它机器其它仓库里面。你只需要用脚本把它们关联到正确的位置，让你大多数的配置在两步命令中启动-克隆你的仓库，运行脚本，然后，你就可以开始工作了。\n这是一个很好的类似的例子\n减少文本切换 最后一个方法是最小化你切换工具的时间。 在你其实只需要看几行代码的时候，你真的需要把终端放大到占满整个屏幕吗？\n尝试一下使用不同的窗口界面、不同的屏幕数量和不同的工作区设置，来帮你你找到一套最适用于你的解决方案。你可以在不同的工作场景使用几套不同的预设置界面。像我，我写代码和调试的时候会使用一套窗口设置，在读代码和审核代码时会使用另一套。切换它们也是关联了一个热键。\n结论 你的工具就是你的扩展，就像你的其它部位，需要投资和实践去改善。 如果你有时会因为你过时的工作流而感到沮丧，或者只是想挤出更多的时间，我希望我这篇文章可以在如何改善你的日常工作工具使用上给到帮助。\n","title":"只需 4 步，改善你的编程工具"},{"location":"https://codenow.me/tips/run-scheduled-py-on-windows/","text":"Windows下定时执行任务，本质即在任务计划程序中添加自定义的Py文件执行任务。\n操作方法：\n 在计算机管理中打开任务计划程序。  点击创建基本任务，开始创建任务程序  设置启动py文件任务   最后点击完成，即可在活动任务中找到添加的任务。\n","title":"Windows下定时执行Py文件"},{"location":"https://codenow.me/algorithm/leetcode_2_add_two_numbers/","text":" 题号：2\n难度：Medium\n链接：https://leetcode.com/problems/add-two-numbers/\n如下是 python3 代码\n # Definition for singly-linked list. # class ListNode(object): # def __init__(self, x): # self.val = x # self.next = None class Solution(object): def addTwoNumbers(self, l1, l2): \u0026#34;\u0026#34;\u0026#34; :type l1: ListNode :type l2: ListNode :rtype: ListNode \u0026#34;\u0026#34;\u0026#34; if l1.next is None and l1.val == 0: return l2 if l2.next is None and l2.val == 0: return l1 str1 = \u0026#39;\u0026#39; str2 = \u0026#39;\u0026#39; while l1: str1 = str(l1.val) + str1 l1 = l1.next while l2: str2 = str(l2.val) + str2 l2 = l2.next add_num = list(str(int(str1)+int(str2)))[::-1] Nodes = [ListNode(num) for num in add_num] for i in range(len(Nodes)-1): Nodes[i].next = Nodes[i+1] return Nodes[0] ","title":"Leetcode: 2 Add Two Numbers"},{"location":"https://codenow.me/articles/build_api_by_flask/","text":" 前言 最近在学习微信小程序，前后端数据交互时，需要API提供数据操作。便学习通过python建立API。\n示例 创建获取数据API @app.route(\u0026#39;/api/v1.0\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_data(): data = function # function为从数据库中获取内容的操作函数。 return jsonify({\u0026#39;data\u0026#39;:data}) # 返回json格式的数据。 操作结果 创建数据提交API @app.route(\u0026#39;/post/\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def post_data(): args = request.args.get(\u0026#39;arg_name\u0026#39;) # request.args.get提供了从url获取参数的功能，通过参数将数据传递后后端 status = post_function(args) # post数据的函数，成功返回200，失败返回错误信息。 return jsonify({\u0026#39;status\u0026#39;:status}) # 将信息返回给前端 上述情况中参数传递形式为：\nhttp://127.0.0.1:5000/post/?name=小李\u0026amp;age=20 # name和age为参数名, 后面的值为具体参数值。  动态url规则下可以直接获取参数： app.route(\u0026#39;/post/\u0026lt;id\u0026gt;\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def post_data(id): # 直接将\u0026lt;id\u0026gt;的值作为函数参数 status = post_function(id) # psot id return jsonify({\u0026#39;status\u0026#39;:status}) 上述情况参数传递形式为：\nhttp://127.0.0.1:5000/post/1 # 1为参数  ","title":"Python下使用Flask建立API"},{"location":"https://codenow.me/tips/awk-de-duplication/","text":"使用 AWK 对数据进行去重：awk '!a[$0]++{print}'\n","title":"Awk De Duplication"},{"location":"https://codenow.me/articles/sentry-python-sdk/","text":"  Sentry 是一个开源的实时错误报告工具，支持 web 前后端、移动应用以及游戏，支持 Python、OC、Java、Go、Node、Django、RoR 等主流编程语言和框架 ，还提供了 GitHub、Slack、Trello 等常见开发工具的集成。\n 这周重新用 docker 部署了一下 Sentry server，比 python 部署确实方便多了，docker-compose 官方都给写好了，改改配置就可以直接上\nserver 端部署好之后，又看了一下 client 端是怎么做的。就感觉这个 SDK 做的真好，接入成本极低，以 Flask 为例，只要加上两行即可：\nfrom raven.contrib.flask import Sentry sentry = Sentry(app, dsn='http://00d5b7d6d7f1498687430d160fd48ea8:ae6eaaf3c8d24d3f98537453da1f6a4f@localhost:9000/2')  于是仔细读了读 SDK 的实现，主要研究三个问题： 1. sentry 是怎么把 SDK 写得如此简洁的？ 2. sentry 是如何捕捉到要发送的错误信息的？ 3. sentry 是如何把错误信息发送到 server 端的？\n注：以下使用的语言及 package 版本为： * python 3.6 * raven 6.10.0 * Flask 1.0.2\n先看一下目录结构：\nraven ├── conf # 配置相关 ├── contrib # 适配各框架的代码 ├── data # 证书等 ├── handlers\t# 日志记录等 ├── scripts # 测试脚本 ├── transport # 实际发消息给服务端的东西 ├── utils # 一些内部工具 ├── __init__.py ├── base.py # 最主要的 Client 类 ├── breadcrumbs.py # 一个特殊概念，还没搞很懂 ├── context.py # 上下文相关 ├── events.py # 事件，是与服务端交互的基本单位 ├── exceptions.py # 异常类 ├── middleware.py # wsgi中间件 ├── processors.py # 数据处理相关 └── versioning.py # 获取 git 版本或 pkg 版本  最核心的，是 base.py 里的 Client 类，所有的操作都是围绕它来展开的。但是由于 SDK 的封装，我们在使用默认配置时并未直接接触到它，而是使用的 raven.contrib.flask.Sentry。下面我们写一段最简单的代码，然后顺着看下去\nfrom flask import Flask from raven.contrib.flask import Sentry app = Flask(__name__) sentry = Sentry(app, dsn='http://00d5b7d6d7f1498687430d160fd48ea8:ae6eaaf3c8d24d3f98537453da1f6a4f@localhost:9000/2') @app.route('/') def index(): return 'index\\n' @app.route('/bad') def bad(): return 1/0 if __name__ == '__main__': app.run('0.0.0.0', 8888, debug=True)  这段代码创建了一个最简单的 Flask app，然后将其传入 raven.contrib.flask.Sentry 中，并传入 dsn。dsn 就是一个字符串，但包含了很多信息，它会在 conf.remote.RemoteConfig 中被处理，处理结果如下：\ndsn  schema: 'http'，标记给服务端发送请求的方式，同时也可以指示使用哪种 raven.transport，比如写 \u0026lsquo;requests+http\u0026rsquo; 就是用 requests 包直接发送请求，写 \u0026lsquo;gevent+http\u0026rsquo; 就是用 gevent 包异步发送请求 public_key: '00d5b7d6d7f1498687430d160fd48ea8'，用户名，没有这个的话服务端不会接收请求 secret_key: 'ae6eaaf3c8d24d3f98537453da1f6a4f'，密码，新版已经废弃，不建议继续使用 base_url: 'http://localhost:9000'，是服务端地址 project: '2'，是在 sentry 的 web 页面里创建的项目的编号 options: 是 url.query，我这里内容为空 transport: 新版建议 Transport 应在初始化 Client 时明确传入，而不是用 schema 的方式配置。但仍然支持着这个功能。  这里详细说一下 transport，顺便就可以解决掉我们刚才提出的第三个问题：sentry 是如何把错误信息发送到 server 端的？\ntransport sentry 中的 transport 有两类，同步的和异步的，异步的需要继承 AsyncTransport 和 HTTPTransport，同步的只需要继承 HTTPTransport，如果想增加新的 transport，只需要实现其规定的方法，然后传入 Client 即可：\n同步的有： * HTTPTransport * EventletHTTPTransport * RequestsHTTPTransport\n异步的有： * ThreadedHTTPTransport * GeventedHTTPTransport * TwistedHTTPTransport * ThreadedRequestsHTTPTransport * TornadoHTTPTransport\n默认 transport 是在 conf.remote.DEFAULT_TRANSPORT 中定义的，当运行环境是 Google App Engine 或 AWS Lambda 时，使用 HTTPTransport(同步)，否则使用 ThreadedHTTPTransport(异步)\nThreadedHTTPTransport 内部用 FIFO Queue + 新开线程的方式来实现异步，这里我们就不用太担心其性能问题了。而发送请求用的是 HTTPTransport 的 urllib2，超时时间5秒，没有重试\nbase.Client 接下来，我们来看一看初始化 Client 的时候，都干了些什么，顺便在这里解决掉第二个问题: sentry 是如何捕捉到要发送的错误信息的？\nclass Client(object): def __init__(self, dsn=None, raise_send_errors=False, transport=None, install_sys_hook=True, install_logging_hook=True, hook_libraries=None, enable_breadcrumbs=True, _random_seed=None, **options):  先看看可传入的参数： * raise_send_errors: 没找到合理的用法，无视 * transport: 上面有说，不建议用 dsn.schema 来控制 transport，最好应该在这里传入 * install_sys_hook: 默认是True，作用是修改 sys.excepthook，把自己的异常处理函数放进去 * hook_libraries: 对 httplib 和 requests 库做了些操作，没搞懂用处 * enable_breadcrumbs: 是否开启此功能 * _random_seed: sentry 可以设置只有部分异常会被发送到服务端，这个值是生成随机数的种子 * 其他参数\n其中，install_sys_hook 是重点，Client.__init__ 里除了进行各种初始化外，最重要的一件事就是这个了\ndef install_sys_hook(self): global __excepthook__ if __excepthook__ is None: __excepthook__ = sys.excepthook def handle_exception(*exc_info): self.captureException(exc_info=exc_info, level='fatal') __excepthook__(*exc_info) handle_exception.raven_client = self sys.excepthook = handle_exception  可以看到，这里在原有内置函数的基础上，加了一句 self.captureException，当一个异常未被 catch 住时，就会调用 sys.excepthook，同时也就发出了发出了请求\n至于 self.captureException 内部，简单来说就做三件事： 1. 获取上下文及各种信息，用到了 sys.exc_info(), flask._request_ctx_stack, flask._app_ctx_stack 和 breadcrumbs 等 2. 到处记日志 3. 构建消息体，选择 transport，发送\n于是，第二个问题，我们已经知道 sentry 的思路了：用 hook 来获取所有未处理的异常\n然而，对 flask 而言，事情并没有这么简单，因为在 flask 里，推荐的异常处理是 @app.errorhandler，同时 sys.excepthook 永远不会被调用\n那怎么办？我们看一下 raven.contrib.flask 吧\nraven.contrib.flask.Sentry class Sentry(object): def __init__(self, app=None, client=None, client_cls=Client, dsn=None, logging=False, logging_exclusions=None, level=logging.NOTSET, wrap_wsgi=None, register_signal=True):  这个参数就容易懂多了 * app: flask app * client: 即上文的 Client，可以自己定制后传入，不定制的话就默认生成一个 * client_cls: Client 类，用来默认生成 Client 对象，不过这个参数没什么意思 * dsn: 开头就有说 * logging, logging_exclusions，level: 日志定制 * wrap_wsgi: 是否将 sentry 加入 flask 中间件 * register_signal: 是否将 sentry 异常处理注册至 flask 的 got_request_exception 信号\n这里的关键是最后一个参数\n默认情况下 register_signal 被设置为 True，于是它会把一个 capture_exception 函数注册到 flask 的 got_request_exception 上\n而 flask 的 got_request_exception 会在 flask.app.handle_exception 的开头，把当前异常 send 出去，然后再执行原有逻辑\n到这里，是不是终于可以说，第二个问题也搞明白了？当发生了无法处理的异常时，flask 先用信号把异常发给 sentry.client，然后 sentry 用 sys.exc_info() 获取上下文，再把拼装好的信息传给相应 passport，最后发送给服务端\n但是，在做测试的时候，我们可能会发现，发生一个异常的同时，服务端收到了两个一毛一样的 event，这是怎么回事？\n答案就在 raven.contrib.flask.Sentry 的一个初始化参数里：wrap_wsgi\nif wrap_wsgi is not None: self.wrap_wsgi = wrap_wsgi elif self.wrap_wsgi is None: # Fix https://github.com/getsentry/raven-python/issues/412 # the gist is that we get errors twice in debug mode if we don't do this if app and app.debug: self.wrap_wsgi = False else: self.wrap_wsgi = True if self.wrap_wsgi: app.wsgi_app = SentryMiddleware(app.wsgi_app, self.client)  已知： 1. 在中间件 raven.middleware.Sentry.client 里，对所有未处理的异常，会向 sentry 服务端发送一次请求 2. production 模式下，发送完 got_request_exception 信号后，会 return InternalServerError()。而 debug 模式下则是把异常重新抛出\n因此当 debug 模式 + 有中间件时，raven.contrib.flask.Sentry.client(信号发送) 和 raven.middleware.Sentry.client(中间件异常捕捉发送) 都会捕捉到这个异常，并发送给服务端。而 production 模式下，只有信号的那一次，中间件不会发\n上面的代码中可以看到，这个问题被修复过一次了，但在我最开头的代码里，flask.app.debug = True 是在最后才运行的，传给 Sentry 的时候，还是 app.debug = False，因此 https://github.com/getsentry/raven-python/issues/412 仍然会发生，临时解决方式就是把 app.debug = True 放到初始化 Sentry 前就好\n好像写的有点长了，最后终于说到了第一个问题，sentry 是怎么把 SDK 写得如此简洁的？\n我认为答案是其丰富的配置项和默认设置，以及对环境变量和全局变量的灵活使用，我真是一边看代码，一边感慨这思路我真得赶紧抄过来\u0026hellip;\nSDK 里还有一些内容文章里没有涉及到(主要我也还没看)，但比较重要的部分都在这里了，总结一下吧：\n 初始化 Client 类时，会从环境变量里获取各种配置，从 dsn 里确定 remote 服务端，选定 transport 并初始化各种 logger, 上下文等等 设置 hook，或者其他手段，保证在发生异常时，能够获取得到异常及上下文 同步/异步 发送消息给服务端，同时不让 sentry 占用太多资源  TODO: 1. breadcrumbs 到底是干啥的 2. 其他框架都是怎么和 sentry 结合的\n","title":"Sentry Python Sdk"},{"location":"https://codenow.me/algorithm/leetcode-13-roman-to-integer/","text":" 题号：13\n难度：easy\n链接：https://leetcode.com/problems/roman-to-integer\n描述：罗马数字转阿拉伯数字(1-3999)\n class Solution(object): def romanToInt1(self, s: str) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot;仿照上一题的无脑解法，先直接怼一个出来\u0026quot;\u0026quot;\u0026quot; ten = ['', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX'] res = 0 if s.startswith('MMM'): res += 3000 s = s[3:] elif s.startswith('MM'): res += 2000 s = s[2:] elif s.startswith('M'): res += 1000 s = s[1:] if s.startswith('CM'): res += 900 s = s[2:] elif s.startswith('D'): res += 500 s = s[1:] elif s.startswith('CD'): res += 400 s = s[2:] if s.startswith('CCC'): res += 300 s = s[3:] elif s.startswith('CC'): res += 200 s = s[2:] elif s.startswith('C'): res += 100 s = s[1:] if s.startswith('XC'): res += 90 s = s[2:] elif s.startswith('L'): res += 50 s = s[1:] elif s.startswith('XL'): res += 40 s = s[2:] if s.startswith('XXX'): res += 30 s = s[3:] elif s.startswith('XX'): res += 20 s = s[2:] elif s.startswith('X'): res += 10 s = s[1:] res += ten.index(s) return res def romanToInt2(self, s: str) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot;有一个微妙的规律，当 左\u0026lt;右 时，减掉左，其余为加左\u0026quot;\u0026quot;\u0026quot; res = 0 roman = {'M': 1000, 'D': 500, 'C': 100, 'L': 50, 'X': 10, 'V': 5, 'I': 1} for i in range(0, len(s) - 1): if roman[s[i]] \u0026lt; roman[s[i + 1]]: res -= roman[s[i]] else: res += roman[s[i]] res += roman[s[-1]] return res def romanToInt(self, s: str) -\u0026gt; int: \u0026quot;\u0026quot;\u0026quot;看了看 discuss，有人给了个更简单的写法 这里反向迭代的原因是，可以确定 I 是最小的\u0026quot;\u0026quot;\u0026quot; res, p = 0, 'I' roman = {'M': 1000, 'D': 500, 'C': 100, 'L': 50, 'X': 10, 'V': 5, 'I': 1} for c in s[::-1]: res, p = res - roman[c] if roman[c] \u0026lt; roman[p] else res + roman[c], c return res if __name__ == '__main__': print(Solution().romanToInt('MCMXCIV'))  ","title":"Leetcode 13 Roman to Integer"},{"location":"https://codenow.me/translation/control_startup_and_shutdown_order_in_compose/","text":"原文链接：https://docs.docker.com/compose/startup-order/，翻译如下：\n您可以使用“depends_on”选项控制服务启动和关闭的顺序。compose总是按依赖顺序启动和停止容器，依赖性由depends_on、links、volumes_form和网络模式“service:…”确定。\n但是，对于启动，compose不会等到容器“就绪”（对于特定的应用程序来说，这意味着什么）之后才运行。这是有充分理由的。\n等待数据库（例如）准备就绪的问题实际上只是分布式系统中一个更大问题的子集。在生产环境中，数据库可能随时不可用或移动主机。您的应用程序需要能够适应这些类型的故障。\n要处理此问题，请设计应用程序以尝试在失败后重新建立与数据库的连接。如果应用程序重试连接，它最终可以连接到数据库。\n最好的解决方案是在应用程序代码中执行这种签入，无论是在启动时还是在任何时候由于任何原因而丢失连接。但是，如果您不需要这种级别的恢复能力，您可以使用包装脚本来解决这个问题：\n 使用诸如wait for it、dockerize或sh-compatible wait for等工具。这些是小包装脚本，您可以将其包含在应用程序的映像中，以轮询给定的主机和端口，直到它接受TCP连接。  例如，要使用wait-for-it.sh或wait-for-wrap服务的命令：\nversion: \u0026quot;2\u0026quot; services: web: build: . ports: - \u0026quot;80:8000\u0026quot; depends_on: - \u0026quot;db\u0026quot; command: [\u0026quot;./wait-for-it.sh\u0026quot;, \u0026quot;db:5432\u0026quot;, \u0026quot;--\u0026quot;, \u0026quot;python\u0026quot;, \u0026quot;app.py\u0026quot;] db: image: postgres   提示：第一个解决方案有局限性。例如，它不验证特定服务何时真正准备好。如果向命令添加更多参数，请使用带有循环的bash shift命令，如下一个示例所示。\n  或者，编写自己的包装器脚本来执行更特定于应用程序的健康检查。例如，您可能希望等待Postgres完全准备好接受命令：  #!/bin/sh # wait-for-postgres.sh set -e host=\u0026quot;$1\u0026quot; shift cmd=\u0026quot;$@\u0026quot; until PGPASSWORD=$POSTGRES_PASSWORD psql -h \u0026quot;$host\u0026quot; -U \u0026quot;postgres\u0026quot; -c '\\q'; do \u0026gt;\u0026amp;2 echo \u0026quot;Postgres is unavailable - sleeping\u0026quot; sleep 1 done \u0026gt;\u0026amp;2 echo \u0026quot;Postgres is up - executing command\u0026quot; exec $cmd  您可以将其用作包装脚本，如前一个示例中所示，方法是设置：\n command: [\u0026ldquo;./wait-for-postgres.sh\u0026rdquo;, \u0026ldquo;db\u0026rdquo;, \u0026ldquo;python\u0026rdquo;, \u0026ldquo;app.py\u0026rdquo;]\n ","title":"Control_startup_and_shutdown_order_in_Compose"},{"location":"https://codenow.me/translation/five-simple-strategies-for-securing-apis/","text":" 保护API的五个简单策略 验证参数 任何弹性API实现的第一步是清理所有传入数据以进行确认它是有效的，不会造成伤害。对参数唯一最有效的防御操作和注入攻击时针对严格的模式验证所有传入的数据有效地描述了被认为是系统允许的输入。模式验证应尽可能具有限制性，尽可能使用输入、范围、集合甚至显性列表。还要考虑从许多开发工具生产的自动生成的模式通常会将所有参数减少到过于宽泛而无法有效识别潜在威胁的模型。手工构建的白名单时更优选的，因为开发人员可以根据他们对应用程序所期望的数据模型的理解来约束输入。基于XML的内容类型的一个选项是使用XML模式语言，该语言在创建受限制的内容模型和高度受约束的结构方面非常有效。对于日益普遍的JSON数据类型，有几种JSON模式描述语言。虽然没有XML那么丰富，但JSON的编写和理解要简单的多，提供透明度使其安全度提高。\n应用显示威胁检测 良好的模式验证可以防止许多注入攻击，但也要考虑显示扫描常见的攻击签名。SQL注入或脚本注入攻击经常通过扫描原始输入容易发现的常见模式来进行攻击。 同时考虑可能采取其他形式，例如拒绝服务（DoS）。利用网咯基础设施俩发现和缓解网络级DoS攻击，还可以检查利用参数的DoS攻击。庞大的信息、严重嵌套的数据结构或过于复杂的数据结构都可能导致有效的拒绝服务攻击，从而不必要地消耗受影响的API服务器上的资源。将病毒检测应用于所有潜在风险的编码内容。文件传输中涉及的API硬解码base64附件并将其提交到服务器级病毒扫描，然后再保存到文件系统，在这些文件系统中可能会无意中激活它们。\n始终开启SSL 使SSL / TLS成为所有API的规则。 在21世纪，SSL并不奢侈; 这是一个基本要求。 添加SSL / TLS并正确应用它可以有效抵御中间人攻击的风险。 SSL / TLS为客户端和服务器之间交换的所有数据提供完整性，包括重要的访问令牌，例如OAuth中使用的令牌。 它可选地使用证书提供客户端身份验证，这在许多环境中很重要。\n应用严格的身份验证和授权 用户和应用程序标识是必须单独实现和管理的概念。 考虑基于广泛身份上下文的授权，包括实际因素，例如传入IP地址（如果已知是固定的或在特定范围内），访问时间窗口，设备标识（对移动应用程序有用），地理位置等。 OAuth正在迅速成为以用户为中心的API授权的首选资源，但它仍然是一个复杂，快速变化和困难的技术。 开发人员应该遵循基本的，易于理解的OAuth用例，并始终使用现有的库而不是尝试构建自己的库。\n使用经过验证的Solutions 安全的第一条规则是：不要发明自己的。 没有理由创建自己的API安全框架，因为API已经存在优秀的安全解决方案。 挑战在于正确应用它们。\n","title":"Five Simple Strategies for Securing APIs"},{"location":"https://codenow.me/algorithm/leetcode_283_move_zeroes/","text":" 题号：283 难度：Easy 链接：https://leetcode.com/problems/move-zeroes/\n #!/usr/bin/python # -*- coding:utf-8 -*- class Solution: def moveZeroes(self, nums): \u0026#34;\u0026#34;\u0026#34; Do not return anything, modify nums in-place instead. \u0026#34;\u0026#34;\u0026#34; empty_index_list = [] for i in range(len(nums)): if i == 0: empty_index_list.append(i) elif empty_index_list: nums[empty_index_list.pop(0)] = nums[i] if empty_index_list: for i in range(len(empty_index_list)): nums[len(nums)-i] = 0","title":"Leetcode_283_Move_Zeroes"},{"location":"https://codenow.me/tips/how_to_set_ip_address_in_ubuntu18/","text":"修改yaml文件\nUbuntu 18.04使用netplan配置网络，其配置文件是yaml格式的。\n安装好Ubuntu 18.04之后，在/etc/netplan/目录下默认的配置文件名是50-cloud-init.yaml或者是01-network-manager-all.yaml\n我们通过VIM修改它\nsudo vim /etc/netplan/50-cloud-init.yaml #Let NetworkManager manage all devices on this system network: #version: 2 #renderer: NetworkManager ethernets: ens33: addresses: [172.20.12.74/24] gateway4: 172.20.12.254 dhcp4: no nameservers: addresses: [103.16.125.251, 103.16.125.252]  重启网络服务使配置生效:\n sudo netplan apply\n 注意事项\n1） 无论是ifupdown还是netplan，配置的思路都是一致的，在配置文件里面按照规则填入IP、掩码、网关、DNS等信息。\n注意yaml是层次结构，需要缩进，冒号(:)表示字典，连字符(-)表示列表。\n比如：\nnetwork: ethernets: ens160: addresses: - 210.72.92.28/24 # IP及掩码 gateway4: 210.72.92.254 # 网关 nameservers: addresses: - 8.8.8.8 # DNS version：2  2）只是针对ubuntu18.04 Server版，对于18.04 desktop它缺省是使用NetworkManger来进行管理，可使用图形界面进行配置，其网络配置文件是保存在：/etc/NetworkManager/system-connections目录下的，跟Server版区别还是比较大的。\n3) 同时，在 Ubuntu 18.04 中，我们定义子网掩码的时候不是像旧版本的那样把 IP 和子网掩码分成两项配置。\n在旧版本的 Ubuntu 里，我们一般配置的 IP 和子网掩码是这样的：\n address = 192.168.225.50 netmask = 255.255.255.0\n 而在 netplan 中，我们把这两项合并成一项，就像这样：\n addresses : [192.168.225.50\u0026frasl;24]\n 4) 配置完成之后保存并关闭配置文件。然后用下面这行命令来应用刚才的配置：\n $ sudo netplan apply\n 如果在应用配置的时候有出现问题的话，可以通过如下的命令来查看刚才配置的内容出了什么问题。\n $ sudo netplan \u0026ndash;debug apply\n 这行命令会输出这些 debug 信息：\nroot@ubuntu:/etc/netplan# vim 01-network-manager-all.yaml root@ubuntu:/etc/netplan# netplan --debug apply ** (generate:27915): DEBUG: 00:58:04.024: Processing input file /etc/netplan/01-network-manager-all.yaml.. ** (generate:27915): DEBUG: 00:58:04.025: starting new processing pass ** (generate:27915): DEBUG: 00:58:04.025: ens33: setting default backend to 1 ** (generate:27915): DEBUG: 00:58:04.025: Generating output files.. ** (generate:27915): DEBUG: 00:58:04.025: NetworkManager: definition ens33 is not for us (backend 1) DEBUG:netplan generated networkd configuration exists, restarting networkd DEBUG:no netplan generated NM configuration exists DEBUG:ens33 not found in {} DEBUG:Merged config: network: bonds: {} bridges: {} ethernets: ens33: addresses: - 192.168.23.129/24 dhcp4: false gateway4: 172.20.12.254 nameservers: addresses: - 103.16.125.251 - 103.16.125.252 vlans: {} wifis: {} DEBUG:Skipping non-physical interface: lo DEBUG:device ens33 operstate is up, not changing DEBUG:Skipping non-physical interface: docker0 DEBUG:Skipping non-physical interface: veth6c57bfa DEBUG:Skipping non-physical interface: vethf518b7b DEBUG:Skipping non-physical interface: veth9faaf09 DEBUG:{} DEBUG:netplan triggering .link rules for lo DEBUG:netplan triggering .link rules for ens33 DEBUG:netplan triggering .link rules for docker0 DEBUG:netplan triggering .link rules for veth6c57bfa DEBUG:netplan triggering .link rules for vethf518b7b DEBUG:netplan triggering .link rules for veth9faaf09 root@ubuntu:/etc/netplan#  5) 在 Ubuntu 18.04 LTS 中配置动态 IP 地址\n其实配置文件中的初始配置就是动态 IP 的配置，所以你想要使用动态 IP 的话不需要再去做任何的配置操作。\n如果你已经配置了静态 IP 地址，想要恢复之前动态 IP 的配置，就把在上面静态 IP 配置中所添加的相关配置项删除，把整个配置文件恢复成之前的样子\n# Let NetworkManager manage all devices on this system network: version: 2 renderer: NetworkManager  然后，运行: \u0026gt; $ sudo netplan apply\n","title":"如何在Ubuntu18中设置静态IP"},{"location":"https://codenow.me/articles/python_usefull_log_method/","text":" 1. 使用logger root@ubuntu:/home/hank# cat test_logger.py #! /usr/bin/python import logging import os class TestLogger: def __init__(self, log_name, log_dir=None, default_level=logging.DEBUG): self.logger = logging.getLogger(log_name) if not self.logger.handlers: log_dir = \u0026#34;/var/log/\u0026#34; if not log_dir else log_dir os.mkdir(log_dir) if not os.path.exists(log_dir) else None absolute_log = os.path.join(log_dir, log_name + \u0026#39;.log\u0026#39;) handler = logging.FileHandler(absolute_log) formatter = logging.Formatter(\u0026#39;%(asctime)-25s%(levelname)-8s%(message)s\u0026#39;) handler.setFormatter(formatter) self.logger.addHandler(handler) self.logger.setLevel(default_level) def debug(self, msg): self.logger.debug(msg) def info(self, msg): self.logger.info(msg) def error(self, msg): self.logger.error(msg) def critical(self, msg): self.logger.critical(msg) if __name__ == \u0026#39;__main__\u0026#39;: log_file, __ = os.path.splitext(os.path.basename(os.path.realpath(__file__))) logger = TestLogger(log_file) logger.info(\u0026#39;information test\u0026#39;) logger.error(\u0026#39;error test\u0026#39;)root@ubuntu:/home/hank# cat test.py #!/usr/bin/python from test_logger import TestLogger logger = TestLogger(\u0026#34;test\u0026#34;) logger.info(\u0026#34;information test\u0026#34;) logger.error(\u0026#39;error test\u0026#39;) formatter = logging.Formatter(\u0026lsquo;%(asctime)-25s %(levelname)-8s %(message)s\u0026rsquo;) 这里定义-25s的原因是asctime为22。所以25正好够和后面的levelname相隔3\nroot@ubuntu:/home/hank# cat /var/log/test.log 2019-04-04 19:18:53,432 INFO information test 2019-04-04 19:18:53,432 ERROR error test  2. 简单的自定义log模块 #! /usr/bin/python class SimpleLogger: def __init__(self, file_name): self.file_name = file_name def _write_log(self, level, msg): with open(self.file_name, \u0026#34;a\u0026#34;) as log_file: log_file.write(\u0026#34;[{0}] {1}\\n\u0026#34;.format(level, msg)) def debug(self, msg): self._write_log(\u0026#34;DEBUG\u0026#34;, msg) def info(self, msg): self._write_log(\u0026#34;INFO\u0026#34;, msg) def warn(self, msg): self._write_log(\u0026#34;WARN\u0026#34;, msg) def error(self, msg): self._write_log(\u0026#34;ERROR\u0026#34;, msg) def critical(self, msg): self._write_log(\u0026#34;CRITICAL\u0026#34;, msg) root@ubuntu:/home/hank# cat test3.py  #! /usr/bin/python from simple_logger import SimpleLogger logger = SimpleLogger(\u0026#34;simple_logger.log\u0026#34;) logger.warn(\u0026#34;this is a warm\u0026#34;) logger.info(\u0026#34;this is a info\u0026#34;) root@ubuntu:/home/hank# cat simple_logger.log  [WARN] this is a warm [INFO] this is a info 3. log中关于读写的问题 写日志时，如是日志文件不存在，则创建；且可以写日志，且方式是追加，所以用a\n","title":"工作中用到的python写log方式"},{"location":"https://codenow.me/articles/spark_installation_onwindows/","text":" 下载安装Java，安装版本为8 Java8下载地址 安装教程详见：菜鸟教程—Java安装\n下载spark安装包 spark2.3.3下载地址\n建议安装2.3.3版本，高版本的2.4.0在运行时会报错Py4j error。 下载后解压文件夹，并将路径配置到系统变量中。\n系统环境变量中配置路径如下：\n下载Hadoop支持包 百度网盘下载地址 提取码：ezs5\n下载后解压，并添加系统变量：\n下载并安装pycharm和anaconda 具体安装教程可自行百度。\n安装后，将spark下的python中的pyspark拷贝到安装的python路径下的：Lib\\site-packages 然后运行pip install py4j\n配置pycharm运行spark环境 根据上图进行配置后即可运行spark程序。\n配置日志显示级别 在spark\\conf目录下创建log4j.properties配置文件，该目录下有template模板，可以直接复制。\n然后将其中的：log4j.rootCategory=INFO, console 修改为 log4j.rootCategory=WARN, console\n配置cmd下pyspark在jupyter下运行 编辑spark目录下：bin\\pyspark2.cmd 修改其中对应部分为以下格式：\nrem Figure out which Python to use. if \u0026quot;x%PYSPARK_DRIVER_PYTHON%\u0026quot;==\u0026quot;x\u0026quot; ( set PYSPARK_DRIVER_PYTHON=jupyter set PYSPARK_DRIVER_PYTHON_OPTS=notebook if not [%PYSPARK_PYTHON%] == [] set PYSPARK_DRIVER_PYTHON=%PYSPARK_PYTHON% )  ","title":"Windows安装spark"},{"location":"https://codenow.me/algorithm/knn/","text":" K-近邻算法（KNN）  算法思想： 存在一个训练样本集，并且样本数据集中每个数据都存在标签。将新数据的每个特征与样本数据集中数据对应的特征进行比较，然后算法提取前K个相似的数据。\n 优缺点及适用范围： 优点：精度高、对异常值不敏感、无数据输入假定；\n缺点：计算复杂度高、空间复杂度高；\n使用数据范围：数值型和标称型\n 计算距离公式  示例分析：约会人员属性 使用的库 Numpy operator  读取数据：将文本记录转化为Numpy矩阵 def file2matrix(filename): fr = open(filename) arrayOLines = fr.readlines() numberOfLines = len(arrayOLines) returnMat = zeros((numberOfLines, 3)) # 创建numpy矩阵 1000行,3列 classLabelVector = [] # 类标签向量 index = 0 for line in arrayOLines: # 逐行填充矩阵和标签向量 line = line.strip() listFromLine = line.split('\\t') returnMat[index, :] = listFromLine[0:3] classLabelVector.append(int(listFromLine[-1])) index += 1 return returnMat, classLabelVector  处理数据：归一化数值(min-max标准化) def autoNorm(dataSet): minValue = dataSet.min(0) maxValue = dataSet.max(0) ranges = maxValue - minValue normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minValue, (m, 1)) # titl函数将minValue复制成dataSet矩阵大小 normDataSet = normDataSet/tile(ranges, (m, 1)) return normDataSet, ranges, minValue  K近邻算法代码 def classify0(inX, DataSet, labels, k: int): # inX:用于分类的输入向量 DataSet:训练样本 labels：标签向量 k:用于选择的最近邻居的数目 # 距离计算：向量之间的距离公式 DataSetSize = DataSet.shape[0] DiffMat = tile(inX, (DataSetSize, 1)) - DataSet # 将inX与DataSet做向量减法 sqDiffMat = DiffMat ** 2 # 将相减后的结果进行平方 sqDistances = sqDiffMat.sum(axis=1) # 将矩阵进行行相加 distances = sqDistances ** 0.5 # 将矩阵结果开平方 # 选择距离最小的k个点 sortedDistIndicies = distances.argsort() # 得到distances矩阵从小到大排序后的对应index classCount = {} for i in range(k): # 统计前k个距离中各个类别的个数 voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # 排序 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) # 将字典按照value值排序 return sortedClassCount[0][0] # 将排序后的第一个key值  测试算法:测试算法正确率 def datingClassTest(): hoRatio = 0.10 datingDataMat, datingLabels = file2matrix(\u0026quot;your dataset path\u0026quot;) normat, ranges, minvalue = autoNorm(datingDataMat) m = normat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0.0 for i in range(numTestVecs): classifierResult = classify0(normat[i, :], normat[numTestVecs:m, :], datingLabels[numTestVecs: m], 3) print(\u0026quot;the classifier came back with %s, the real answer is: %s\u0026quot; % (classifierResult, datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print(errorCount, numTestVecs) print(\u0026quot;the total error rate is %f\u0026quot; % (errorCount/float(numTestVecs)))  测试结果 percentage of time spent playing video games?10\n  frequent flier miles earned per year?10\nliters of ice cream consumed per year?20\nYou will probably like this person: In small does\n","title":"K近邻算法"},{"location":"https://codenow.me/tips/rsshub/","text":"Rsshub 是一个轻量、易于扩展的 RSS 生成器, 可以给任何奇奇怪怪的内容生成 RSS 订阅源\n","title":"万物皆可 RSS"},{"location":"https://codenow.me/articles/golang-namespace/","text":" 总所周知 Docker 最早诞生于 Linux 平台，利用的是 Linux LXC 技术作为基础。Docker 作为一种 “轻量级虚拟机” 跑在通用操作系统中，那么势必就要对容器进行隔离，保证在宿主机内的独立性。\nNamespace Overview 在 Linux Kernel 中有一组名为 Namespace 的系统调用 API。主要作用是封装了全局的系统资源的调用分配，在一个进程中隔离了其他进程的可见性，让自己 “拥有” 整个计算机的资源的能力。一个典型的用途就是容器的实现。\nnamespace 一种只有 4 个 API：\n clone：创建一个隔离的进程，可以通过参数控制所拥有的资源 setns：允许一个进程到现有的 namespace unshare：从现有 namespace 中移除一个进程 ioctl：用法发现 namespace 信息  接下来主要讨论如何创建一个具有隔离性的进程，也就是 clone 这个系统调用的用法。\nclone 创建一个新的 namespace（进程），可以对其控制几个方面的资源（通过 CLONE_NEW* 这系列参数）。\n IPC：CLONE_NEWIPC，System V IPC 和 POSIX message queue Network：CLONE_NEWNET，网络设备等 Mount：CLONE_NEWNS，挂载点 PID：CLONE_NEWPID，进程的 ID User：CLONE_NEWUSER：用户或组的 ID UTS：CLONE_NEWUTS：Hostname 和 NIS domain  这里 CLONE_NEWNS 比较奇特，这是最早的一个参数，后面也想不到还有更多粒度的资源控制，所以这是一个历史遗留问题。\nNamespace Usage 由于 Namespace 是 Linux 的系统调用，所以在其他操作系统是无法编译通过的。可以在 build 时候通过设置 GOOS = linux 解决，但是运行还是要放在 Linux 上运行。\n在 Golang 中创建一个新的进程，通过 CLONE_NEW* flag 设置资源隔离。\n// +build linux  package main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;syscall\u0026#34; ) func main() { cmd := exec.Command(\u0026#34;sh\u0026#34;) cmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER | syscall.CLONE_NEWNET, } cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil { log.Fatal(err) } } 使用 env GOOS=linux go build -o nsprocess 编译后，copy nsprocess 到 linux 机器上执行。\n先看一下 CLONE_NEWUSER 的功能：\n$ id uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup) 我们可以看到，这时候 UID 和我们宿主机上的不同，表明 user 资源被隔离了。\n$ ifconfig $ 网络设备信息也是空的，CLONE_NEWNET 的隔离也生效了。\n# hostname -b zxytest # hostname zxytest 修改 hostname 后到宿主机发现 hostname 并没有被修改，这就是 CLONE_NEWUTS 的隔离性。\n# mount -t proc proc /proc # ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 12:16 pts/0 00:00:00 sh root 3 1 0 12:17 pts/0 00:00:00 ps -ef mount proc 之后发现进程信息都没有了，只有当前的进程信息。\n ps 命名是通过读取 /proc 文件输出的，所以要先 mount proc\n 以上就 Linux Namespace 的基本用法，也是 docker 的基础技术。\n","title":"Golang Linux Namespace Usage"},{"location":"https://codenow.me/translation/beachmark-details/","text":" 我一直在优化我的 go 代码并且一直优化我的性能测试方案。\n让我们先看一个简单的例子：\nfunc BenchmarkReport(b *testing.B) { runtime.GC() for i := 0; i \u0026lt; b.N; i++ { r := fmt.Sprintf(\u0026#34;hello, world %d\u0026#34;, 123) runtime.KeepAlive(r) } } 执行 go test -beach . 会看到这样子的结果：\nBenchmarkReport-32 20000000 107 ns/op  这可能可以初略的估计性能表现，但是彻底的优化需要更详细的结果。\n将所有的内容压缩成一个数字必然是简单的。\n让我向你们介绍我写的 hrtime 包，以便于获取更详细的性能测试结果。\n直方图 第一个推荐使用的是 hrtime.NewBeachmark，重写上面的简单例子：\nfunc main() { bench := hrtime.NewBenchmark(20000000) for bench.Next() { r := fmt.Sprintf(\u0026#34;hello, world %d\u0026#34;, 123) runtime.KeepAlive(r) } fmt.Println(bench.Histogram(10)) } 它会输出：\navg 372ns; min 300ns; p50 400ns; max 295µs; p90 400ns; p99 500ns; p999 1.8µs; p9999 4.3µs; 300ns [ 7332554] ███████████████████████ 400ns [12535735] ████████████████████████████████████████ 600ns [ 18955] 800ns [ 2322] 1µs [ 20413] 1.2µs [ 34854] 1.4µs [ 25096] 1.6µs [ 10009] 1.8µs [ 4688] 2µs+[ 15374] 我们可以看出 P99 是 500ns，表示的是 1% 的测试超过 500ns，我们可以分配更小的字符串来优化：\nfunc main() { bench := hrtime.NewBenchmark(20000000) var back [1024]byte for bench.Next() { buffer := back[:0] buffer = append(buffer, []byte(\u0026#34;hello, world \u0026#34;)...) buffer = strconv.AppendInt(buffer, 123, 10) runtime.KeepAlive(buffer) } fmt.Println(bench.Histogram(10)) } 结果如下：\navg 267ns; min 200ns; p50 300ns; max 216µs; p90 300ns; p99 300ns; p999 1.1µs; p9999 3.6µs; 200ns [ 7211285] ██████████████████████▌ 300ns [12658260] ████████████████████████████████████████ 400ns [ 81076] 500ns [ 3226] 600ns [ 343] 700ns [ 136] 800ns [ 729] 900ns [ 8108] 1µs [ 15436] 1.1µs+[ 21401] 现在可以看到 99% 的测试已经从 500ns 降到了 300ns。\n如果你眼神犀利，可能已经注意到 go beachmark 给出了 107ns/op 但是 hrtime 给了 372ns/op。 这是获取更多测试信息的副作用，他们总是会有开销的。最终结果包括这种开销。\nStopwatch 有时候我们还行测试并发操作，这时候可能需要 Stopwatch。\n假如你想在测试一个多竞争 channel 的持续时间。当然这是一个认为的例子，大致描述了如何从一个 goroutine 开始在另一个 goroutine 结束并且打印结果。\nfunc main() { const numberOfExperiments = 1000 bench := hrtime.NewStopwatch(numberOfExperiments) ch := make(chan int32, 10) wait := make(chan struct{}) // start senders  for i := 0; i \u0026lt; numberOfExperiments; i++ { go func() { \u0026lt;-wait ch \u0026lt;- bench.Start() }() } // start one receiver  go func() { for lap := range ch { bench.Stop(lap) } }() // wait for all goroutines to be created  time.Sleep(time.Second) // release all goroutines at the same time  close(wait) // wait for all measurements to be completed  bench.Wait() fmt.Println(bench.Histogram(10)) } hrtesting 当然重写所有的测试用例是不现实的。为此有 github.com/loov/hrtime/hrtesting 为测试提供 testing.B。\nfunc BenchmarkReport(b *testing.B) { bench := hrtesting.NewBenchmark(b) defer bench.Report() for bench.Next() { r := fmt.Sprintf(\u0026#34;hello, world %d\u0026#34;, 123) runtime.KeepAlive(r) } } 会打印出 P50、P90、P99：\nBenchmarkReport-32 3000000 427 ns/op --- BENCH: BenchmarkReport-32 benchmark_old.go:11: 24.5µs₅₀ 24.5µs₉₀ 24.5µs₉₉ N=1 benchmark_old.go:11: 400ns₅₀ 500ns₉₀ 12.8µs₉₉ N=100 benchmark_old.go:11: 400ns₅₀ 500ns₉₀ 500ns₉₉ N=10000 benchmark_old.go:11: 400ns₅₀ 500ns₉₀ 600ns₉₉ N=1000000 benchmark_old.go:11: 400ns₅₀ 500ns₉₀ 500ns₉₉ N=3000000  在 Go 1.12 中将会打印出所有的 Beachmark 而不是最后一个，但是在 Go 1.13 中可以输出的更好：\nBenchmarkReport-32 3174566 379 ns/op 400 ns/p50 400 ns/p90 ...  获得的结果也可以和 beachstat 进行比较。\nhrpolt 最后载介绍一下 github.com/loov/hrtime/hrplot，使用我实验性质的绘图包，我决定添加一种方便的方法来绘制测试结果。\nfunc BenchmarkReport(b *testing.B) { bench := hrtesting.NewBenchmark(b) defer bench.Report() defer hrplot.All(\u0026#34;all.svg\u0026#34;, bench) runtime.GC() for bench.Next() { r := fmt.Sprintf(\u0026#34;hello, world %d\u0026#34;, 123) runtime.KeepAlive(r) } } 将会创建一个 SVG 文件 all.svg。其中包括线性图，显示了每次迭代所花费的时间；第二个就是密度图，显示了测量时间的分布图，以及最后一个百分位的详情。\nConclusion 性能优化很有趣，但是有更好的根据可以变得更加有趣。\n去尝试 github.com/loov/hrtime 让我知道你更多的想法。\n","title":"更详细的 Go 性能测试"},{"location":"https://codenow.me/algorithm/leetcode_148_sort_list/","text":" 题号：148\n难度：中等\n链接：https://leetcode-cn.com/problems/sort-list/submissions/\n /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func sortList(head *ListNode) *ListNode { quickSort(head, nil) return head } func getPartion(start *ListNode, end *ListNode) *ListNode { key := start.Val i := start j := start.Next for j != end { if (j.Val \u0026lt; key) { i = i.Next i.Val, j.Val = j.Val, i.Val } j = j.Next } start.Val, i.Val = i.Val, start.Val return i } func quickSort(head *ListNode, tail *ListNode) { if head != tail { partion := getPartion(head, tail) quickSort(head, partion) quickSort(partion.Next, tail) } }","title":"Leetcode: 148 Sort List"},{"location":"https://codenow.me/translation/how-does-facial-recognition-work/","text":" How does facia recognition work? (原文地址)[https://us.norton.com/internetsecurity-iot-how-facial-recognition-software-works.html]\nBy Steve Symanovich\n面部识别是一种通过技术识别人脸的方式。面部识别系统使用生物识别技术来映射来自照片或视频的面部特征。它将信息与已知面部数据库进行比较以找到匹配项。面部识别可以帮助验证个人身份，但它也会引发隐私问题。\n面部识别市场预计将从2017年的40亿美元增长到2022年的77亿美元。这是因为面部识别具有各种商业应用。它可用于从监控到营销的各个方面。\n但这就是它变得复杂的地方。如果隐私对您很重要，您可能希望控制您的个人信息（您的数据）的使用方式。事情就是这样：你的“面子”是数据。\n面部识别的工作原理 你可能善于识别面孔。您可能会发现很难找到家人，朋友或熟人的面孔。你熟悉他们的面部特征 - 他们的眼睛，鼻子，嘴巴 - 以及他们如何走到一起。\n这就是面部识别系统的工作方式，但是在一个宏大的算法规模上。在您看到面部的地方，识别技术会看到数据。可以存储和访问该数据。例如，根据乔治敦大学的一项研究，一半的美国成年人将他们的图像存储在执法机构可以搜索的一个或多个面部识别数据库中。\n那么面部识别是如何工作的呢？技术各不相同，但以下是基本步骤：\n第1步。从照片或视频中捕获您的脸部照片。你的脸可能会单独或在​​人群中出现。您的图像可能会显示您正向前看或几乎在剖面图中。\n第2步。面部识别软件可读取您脸部的几何形状。关键因素包括眼睛之间的距离以及从额头到下巴的距离。该软件识别面部标志 - 一个系统识别其中的68个 - 这是区分您的面部的关键。结果：你的面部特征。\n第3步。您的面部特征 - 数学公式 - 与已知面部的数据库进行比较。并考虑到这一点：至少有1.17亿美国人在一个或多个警察数据库中有他们的面孔图像。根据2018年5月的一份报告，联邦调查局已经获得了4.12亿张用于搜索的面部图像。\n第4步。做出决定。您的面部印记可能与面部识别系统数据库中的图像相匹配。\n一般来说，面部识别是如何运作的，但谁使用它？\n谁使用面部识别？ 许多人和组织使用面部识别 - 并且在许多不同的地方。这是一个抽样：\n美国政府机场。面部识别系统可以监控机场来来往往的人员。美国国土安全部使用该技术识别逾期签证或可能受到刑事调查的人。华盛顿杜勒斯国际机场的海关官员于2018年8月首次使用面部识别器进行了逮捕，抓住了一名试图进入该国的冒名顶替者。 手机制造商的产品。Apple首先使用面部识别来解锁其iPhone X，并继续使用iPhone XS。面部身份验证 - 确保您在访问手机时成为现实。苹果表示，随机面部解锁手机的可能性大约为百万分之一。 大学在课堂上。面部识别软件本质上可以采取滚动。如果你决定削减课程，你的教授可以知道。甚至不要想让你聪明的室友接受考试。 网站上的社交媒体公司。当您将照片上传到其平台时，Facebook会使用算法来识别面部。社交媒体公司询问您是否要在照片中标记人物。如果您同意，则会创建指向其个人资料的链接。Facebook可以识别98％准确率的面孔。 入口和禁区内的企业。一些公司已经交易面部识别系统的安全徽章。除了安全性，它可能是与老板面对面交流的一种方式。 宗教场所的宗教团体。教会使用面部识别来扫描他们的会众，看看谁在场。这是跟踪常客和不那么常客的好方法，也是帮助定制捐赠请求的好方法。 商店中的零售商。零售商可以结合监控摄像头和面部识别来扫描购物者的面部。一个目标：识别可疑角色和潜在的扒手。 航空公司在登机口。您可能习惯于让代理人在登机口扫描登机牌以登机。至少有一家航空公司扫描你的脸。 营销活动中的营销人员和广告客户。营销人员在针对产品或创意的群组进行定位时，通常会考虑性别，年龄和种族等因素。面部识别可以用来定义那些观众甚至在音乐会之类的东西。\n","title":"人脸识别如何工作"},{"location":"https://codenow.me/translation/tidb-proposal-a-new-aggregation-function-execution-framework/","text":" 原文链接 Proposal: A new aggregate function execution framework\n摘要 这篇 proposal 提出了一种的聚合计算执行框架，用来提高聚合函数的执行性能。\n背景 在 release-2.0 版本中，聚合计算框架在 expression/aggregation 模块中。在这个框架中，所有的聚合函数都实现了 Aggregation 接口。所有的聚合函数使用 AggEvaluateContext 来保存聚合计算的中间结果(partial result）。AggEvaluateContext 中的 DistinctChecker 字段使用 byte 数组作为 key，用来对相同分组中的数据进行去重。在执行过程中， Update 接口会被调，为每行数据计算和更新中间结果。在执行过程中，它为每个聚合函数枚举每种可能的聚合状态，这回带来大量的 CPU 分支检测。\n在这个框架下，可以很简单的实现一个新的聚合函数。但是它也有很多缺点：\n Update 方法会为每条数据被调用。每次调用中都可能会带来大量开销，特别是执行过程中包含了上万条数据的时候。 Update 方法会为每种计算状态调用，这也会带来大量的 CPU 分支检测。比如， AVG 函数在 Partial1 和 Final 状态下行为是不一样的，Update 方法不得不使用 switch 语句来处理所有可能的状态。 GetResult 方法返回 types.Datum 类型作为每个分组的最终结果。在执行阶段，TiDB 目前使用 Chunk 来保存数据。使用了 aggregation 框架，不得不将返回的 Datum 类型转换成 Chunk ，这会带来大量的数据转换和内存分配工作。 AggEvaluateContext 用来保存每组分组数据的最终结果，相比实际所需，这会消耗更多的内存。比如 COUNT 函数原本只需要一个 int64 字段来保存行数。 distinctChecker 用来为数据去重，它使用的是 byte 数组作为 key。针对输入数据的 encoding 和 decoding 操作会带来大量的 CPU 开销，其实这个问题可以通过直接使用输入数据作为 key 来避免掉。  方案 在这个 PR 中 https://github.com/pingcap/tidb/pull/6852 ，提出了一个新的框架。新框架在 executor/aggfuncs 模块中。\n在新的执行框架中，每个聚合函数实现了 AggFunc 接口。使用 PartialResult 作为每个聚合函数的中间结果，PartialResult 实际是 unsafe.Pointer 类型。unsafe.Pointer 允许中间结果可以使用任何数据类型。\nAggFunc 接口包含以下函数：\n AllocPartialResult 分配和初始化某种特定数据结构来保存中间结果，将它转换成 PartialResult 类型并返回。聚合操作的实现，比如流式聚合 (Stream Aggregation) 要保存分配的 PartialResult ，用在后续和中间结果有关的操作上，比如 ResetPartialResult ，UpdatePartialResult 等等。 ResetPartialResult 为聚合函数重置中间结果。将输入的 PartialResult 转换成某种数据结构，用来存中间结果，并将每个字段重置成初始状态。 UpdatePartialResult 根据属于相同分组的输入数据计算并更新中间结果。 AppendFinalResult2Chunk 完成最终的计算并将最终结果直接添加到输入的 chunk 当中。像其他操作一样，它把 PartialResult 先转换成某种数据结构，计算最终的结果，然后将最终结果添加到提供的 chunk 当中。 MergePartialResult 使用输入的 PartialResults 计算最终结果。假设输入的 PartialResults 名称分别是dst 和 srt，它先把 dst 和 src 转换相同的数据结构，合并中间结果，将结果保存在 dst 中。  新的框架使用 Build() 函数来构建可执行的聚合函数。输入参数是：\n aggFuncDesc ：在查询优化器层表示聚合函数的数据结构 ordinal：聚合函数的序号。这也是相应的聚合操作输出的 chunk 中输出列的顺序  Build() 方法为具体某种输入参数类型和聚合状态 (aggregate state) 构建可执行的聚合函数，输入数据类型、聚合状态等等信息越具体越好。\n原理 优点：\n 在新框架下，中间结果可以是任何类型。聚合函数可以根据实际需要来分配内存，不会造成浪费。当用在 hash aggregation 时，OOM 的风险也会被降低。 中间结果可以是任何类型，这意味着，聚合函数可以使用 map，并以具体某种输入类型作为 key。比如，使用 map[types.MyDecimal 来对输入的数值进行去重。通过这种方式，旧框架中 decoding 和 encoding 带来的开销被降低了。 UpdatePartialResult 被用来调用批量处理一组输入数据。为每条记录上而调用函数所带来的开销被节省掉了。由于所有的计算都使用 Chunk 来保存输入数据，在 Chunk 中相同列的数据在内存当中被连续存储，聚合函数会一个挨一个的执行，充分利用 CPU 缓存，减少缓存未命中（cache miss），从而提高执行性能。 对于每一种聚合状态和任何输入类型，都要实现对应的一个聚合函数来支持。这意味着在聚合状态和输入类型上面的 CPU 分支检测运算可以在 UpdatePartialResult 执行过程中被减少，更好的利用 CPU pipeline，提供执行速度。 AppendFinalResult2Chunk 直接将最终结果加入到 chunk 当中，不需要将数据转成 Datum 再将 Datum 转换回 Chunk。这减少了大量的对象分配，降低了 golang gc worker 的开销，避免了 Datum 和 Chunk 之间不必要的数据转换。  缺点：\n 每种聚合函数要分别为每一种可能的聚合状态和输入类型，实现对应的计算函数。这可能会带来大量的开发工作。需要做更多的编码工作来支持新的聚合函数。  兼容性 目前，新的框架只支持了流式聚合。如果 Build() 方法返回 nil ，那么系统会使用旧的框架。\n所以，这个新框架可以在开发过程中测试，所有的结果应该和旧框架一样。\n","title":"Tidb Proposal: A new aggregate function execution framework"},{"location":"https://codenow.me/algorithm/leetcode-find-k-pairs-with-smallest-sums/","text":"原题链接: 373. Find K Pairs with Smallest Sums\n典型的 Kth elements 问题，使用堆就行：\nimport heapq class Solution: def kSmallestPairs(self, nums1: \u0026#39;List[int]\u0026#39;, nums2: \u0026#39;List[int]\u0026#39;, k: \u0026#39;int\u0026#39;) -\u0026gt; \u0026#39;List[List[int]]\u0026#39;: if len(nums1) == 0 or len(nums2) == 0: return [] q = [] for n1 in nums1: for n2 in nums2: heapq.heappush(q, (n1 + n2, n1, n2)) i = k ret = [] while i \u0026gt; 0 and len(q) \u0026gt; 0: item = heapq.heappop(q) ret.append([item[1], item[2]]) i = i - 1 return ret","title":"Leetcode Find K Pairs With Smallest Sums"},{"location":"https://codenow.me/tips/golang-type-detecting/","text":"类型直接转换：\nv := value.(*TypeA) 类型检测+转换：\nv, isTypeB := value.(*TypeB) 类型检测 + switch:\nswitch v.(*Type) { case TypeA: //...  case TypeB: //... \tdefault: //... }","title":"golang 类型检测"},{"location":"https://codenow.me/articles/tidb-aggregation/","text":"没了解过 Aggregation 的执行细节之前，感觉 Aggregation 比较神奇，它和普通的 SPJ 查询不太一样，Aggregation 会对数据分组并聚合计算，经过 Aggregation，整个数据的 schema 都会发生改变。\n但其实，常见的 Aggregation 也并不复杂，从代码里看，和 Aggregation 相关的数据结构是这样的：\n// LogicalAggregation represents an aggregate plan. type LogicalAggregation struct { logicalSchemaProducer AggFuncs []*aggregation.AggFuncDesc GroupByItems []expression.Expression // groupByCols stores the columns that are group-by items.  groupByCols []*expression.Column possibleProperties [][]*expression.Column inputCount float64 // inputCount is the input count of this plan. } type basePhysicalAgg struct { physicalSchemaProducer AggFuncs []*aggregation.AggFuncDesc GroupByItems []expression.Expression } // PhysicalHashAgg is hash operator of aggregate. type PhysicalHashAgg struct { basePhysicalAgg } // PhysicalStreamAgg is stream operator of aggregate. type PhysicalStreamAgg struct { basePhysicalAgg } 无论是逻辑查询计划还是物理查询计划，聚合计算所需的关键信息都主要是聚合函数 (AggFuncs) 和分组规则 (GroupByItems) 。执行查询时，遍历子节点数据，根据 GroupByItems 将数据划分到不同的组中，然后调用聚合函数更新计算结果，直到子节点的数据全部消费完。\n下面是 v2.0.9 版本 HashAggExec 的计算代码：\n// execute fetches Chunks from src and update each aggregate function for each row in Chunk. func (e *HashAggExec) execute(ctx context.Context) (err error) { inputIter := chunk.NewIterator4Chunk(e.childrenResults[0]) for { err := e.children[0].Next(ctx, e.childrenResults[0]) // 获取子节点数据  // ...  // no more data.  if e.childrenResults[0].NumRows() == 0 { return nil } for row := inputIter.Begin(); row != inputIter.End(); row = inputIter.Next() { groupKey, err := e.getGroupKey(row) // 为每行数据计算 groupKey  //...  aggCtxs := e.getContexts(groupKey) // 根据 groupKey 获取对应的计算结果  for i, af := range e.AggFuncs { // 遍历聚合函数，针对每行数据调用每一个聚合函数，更新聚合计算的结果  err = af.Update(aggCtxs[i], e.sc, row) //...  } } } } HashAggExec 是使用 HashTable 保存不同分组的中间计算结果 (PartialResult) ，等数据全部消费完以后，HashTable 中的结果则是最终结果了。上面代码的主要计算过程：\n 遍历子节点数据（子节点可能是 Join、TableScan、Selection 等等） 为每行数据计算 groupKey ，根据 groupKey 获取中间计算结果 调用聚合函数，使用新遍历到的数据更新计算结果  下面以 avg 函数代码为例，分析一下聚合函数的代码逻辑：\nfunc (af *aggFunction) updateSum(sc *stmtctx.StatementContext, evalCtx *AggEvaluateContext, row types.Row) error { a := af.Args[0] value, err := a.Eval(row) // 计算每行数据对应的 value  //...  evalCtx.Value, err = calculateSum(sc, evalCtx.Value, value) // 更新总和  //...  evalCtx.Count++ // 更新数据行数  return nil } 备注：avg 函数相对特殊一点，和 sum/count 相比，计算过程中要记录 Sum 和 Count 作为中间数据。\n备注：新版本为 HashAggExec 加入了并行计算功能，代码逻辑更加复杂，不过聚合计算逻辑没有太大变化。\n原文链接\n","title":"TiDB 源码学习：聚合查询"},{"location":"https://codenow.me/translation/howtoencryptyourentirelifelessthanonehour/","text":" 不用一小时，加密你的整个人生 作者：Quincy Larson How to encrypt your entire life in less than an hour \n前言略 全文有精简\n好，让我们开始吧！ 首先，解释几个术语。 Attacker：所有未经本人同意却尝试获取本人数据的人或组织，甚至政府。 Private/secure：理想很丰满，现实是只要人类参与，没有任何系统可以保证百分百隐私或安全。\n只要你的手机、电脑 、账户受到充分保护，它们的内容会保持在一个加密的状态，那无论其他人多么强大，也无法可施。\nTip1: 对你的收件箱进行两步验证 你的收件箱是你生活的主钥。如果入侵者解开了它，那他不仅可以读取你的邮件，还可以通过重置你的密码去做更多的事，包括社交账户甚至银行账户。 你仅仅需要通过一个很简单的操作就可以大大改善你的个人隐私，就是去开启你收件箱的两步验证。 基本上两步验证就是你登录后的第二层保护。通常当你登录你的账户时，你会收到一条验证码。 两步验证基本上可以减少你收件箱被黑的可能性。 如果你用的是 gmail，你应该开启两步验证\n说真的，现在就去开启吧，我在这等你回来。\nTip2: 加密你的硬盘 Windows 和 Macos 都有内置全盘加密。你只需要开启它。\nTip3: 开启你手机的密码保护 指纹验证比其他方法都好，但是还不足够。 第五修订法允许你可以对你的密码进行保密，但是法院可以强迫你用指纹解锁手机（美国法律）。 而且，你的指纹是唯一的，不能在入侵者掌控你的指纹信息后进行更改。 一般入侵者会在手机完全锁住前有十次尝试机会。所以如果你的 4 个数字的密码属于以下常见的这些之一的话，请修改你的密码。\n1234 9999 1111 3333 0000 5555 1212 6666 7777 1122 1004 1313 2000 8888 4444 4321 2222 2001 6969 1010  如果为了方便还是坚持使用指纹验证的话，万一被逮捕，请马上关机。当他们重启手机，因为没有你的密码，他们也没办法解锁你的手机。（接前面美国法律的事）\nTip4: 不同的设备用不同的密码 密码本质上就是不安全的 Mark Zuckerberg 曾经用 ‘dadada’ 作为 他的 Linkedin 账户密码。 早些年，有黑客对外开放 117 百万个邮箱密码关联，他就是受害者之一。 黑客可以使用他的邮箱和密码去获取 Twitter 和 Pinterest 的登录。 所以，一个设备用一个密码。 当然，你没可能记得住那么多的密码，你可以使用工具 password manager\nTip5: 用 Signal 发私人信息 Signal 是一个获得 Electronic Frontier Foundation 组织高分认证的流行社交媒介，你可以像在其他平台一样收发信息、群聊、发照片或发视频。不同的是，所有都是加密保护的。 Signal 是免费的、开源的，有 Android 和 ios 版本。5 分钟都不需要，下载 Signal 后，就可以和亲友们安全对话了。 下载后，恭喜你，你就可以畅所欲言了，基本上没有人可以监视你的对话。 你甚至可以用 Signal 打私密电话。\nTip6: 你的浏览器匿名模式也不足够安全 就算你使用 Chrome 或 Firefox 的匿名模式，下面各方还是可以偷窥你的网络活动：网络服务提供方、掌管所在地网络的系统管理者、google 或其它浏览器制造商等等， 如果你想要更好的安全浏览，你最好用 Tor。\nTip7: 用 Tor 进行私密浏览 Tor 来自 ‘The Onion Router’， 表示使用了像洋葱那么多层的方式来隐藏网络活动。 Tor 是免费的、开源的，使用也很简单。\nTip8： 用安全的搜索引擎 如果你觉得 Tor 不是很方便，那么至少尝试一下用私密的搜索引擎， 像 DuckDuckGo ，这些搜索引擎不会追踪的的网络行为。 DuckDuckGo 没有 Google 那样通过多年和成千上万工程师帮助强大的搜索能力，但是只需要一小步，你就可以兼容私密和 Google 强大的资源，你只需要在搜索关键字前加 !google 即可。\n我建议你看看电脑安全专家 Bruce Schneier 的书 《Data and Goliath: The Hidden Battles to Collect Your Data and Control Your World 》。从这本书里我学到了很多，现在准备听多一次。\n","title":"不用一小时，加密你的整个人生"},{"location":"https://codenow.me/articles/aaron_huoju/","text":" 互联网之子 Aaron Swarts 想要看到的世界 作者: Jade \u0026amp; 霍炬 链接：互联网之子 Aaron Swarts 想要看到的世界 \nJade 和我偶尔会聊起一些宏大的话题，最近聊到了 Aaron 和互联网创建者们的一些历史。她觉得应该正经的来一次对话，记录下来分享给其他人。我们约了个时间，原计划聊 2 个小时，实际上聊了 5 个小时。最后形成了一篇交谈形式的文字，她称之为文字版的 Podcast。我很喜欢这种形式，我也更认同文字的价值，更好分享，更好检索，也更好修改或者摘录使用。以后我们应该还会继续这样的对话，这次聊天里面很多东西都可以继续讲下去。希望你也喜欢这个形式。\n网络和 BBS 在我们现在知道的互联网诞生之前就存在了，普通人有机会接触网络的历史，至今也有 30 多年了。虚拟世界的时间进度远远比现实世界快，在中国，也有“互联网是属狗的，一年当作七年用这个说法”。按照这个比例推测，换算到现实世界，互联网实际上走过了相当于 200 年左右的历史了。对它的研究已经可以产生一个“互联网考古学”之类的新学科了，然而没有多少人意识到这件事，也没多少人对这些历史和人物有兴趣。尽管我认为这些非常重要，其中有太多的教训和经验今天仍然可以学习。而且，这些历史也不应该被忘掉。\nUsenet 今天已经变成了下载者的乐园，但是它的废墟里埋葬了太多的历史和欢笑血泪。一些人已经消失，一些人还在积极工作，一些人已经走上了另外一条道路，还有一些人已经离开了我们。如果你从 80 年代就对整个网络世界有所了解，你会发现到今天一切都是相连的，从拨号 BBS 到区块链，有一条暗线始终存在。差不多也到了挖掘这些故事的时候了。\n01 自由的代价 Jade：首先问个题外话，你为什么会选择在寒冷的加拿大生活和开发产品？\n霍炬：几年前我觉得世界似乎变得越来越混乱，我和太太就想找一个“安全的地方”躲起来。于是我去读了所有可能去的国家的历史和政治制度，最后认为最安全的两个地方是新西兰和加拿大。但新西兰太偏远，科技和互联网不够发达，加拿大科技水平很高，创新能力也好，于是，就加拿大了。\nJade：没想到一个八卦问题引发了如此深刻的答案，吓了我一跳。之所以咱们决定有这次对谈，我记得是有一次我们聊起了 Aaron Swarts，你给我推荐了关于他的纪录片（互联网之子 The Internet\u0026rsquo;s Own Boy: The Story of Aaron Swartz (2014)），然后就一发不可收拾地聊起了一大堆宏大的主题。能不能从你的角度再介绍一下 Aaron 这个人，我们都知道他是一个互联网天才，14 岁参与制订 RSS 标准，26 岁在 MIT 事件的压力下自杀。为什么你觉得这个人很重要？他带来了什么？\n霍炬：Aaron 最常见的介绍是“reddit 联合创始人，RSS 参与者，Markdown 标准参与者”。但是这些不是我想说的重点，重点是他是一个承接上一代和下一代的人物。应该是承接互联网创建者们的理念，并且用来改造世界的人。\nAaron 深受 John Perry Barlow（电子前线基金会 EFF 的创始人）的影响。在 Aaron 中学时代，John 到他们学校演讲，Aaron 听了这个演讲之后，深受影响。后来 Aaron 的爸爸说那天他回家就像变了一个人一样。以及后来 Aaron 和 Tim Berners-Lee 在一起工作，等等。按照他的年龄，很难想象和这些互联网的创建者们一起工作和活动。但是他和他们相处很好，这些人也都喜欢他。\n他的死除了是一个巨大的悲剧之外，彻底改变了很多人对美国的看法，很多人因此对奥巴马政府，以及民主党由正面转向负面。一直到现在，2016 年民主党的溃败和川普的上台，和这件事仍然有一些关联，这种关联是通过 wikileaks 体现出来的。\nJade：是的，Aaron 除了是个天才，最迷人的还是他坚定地想要改变世界的气质。他曾经说，他的所有时间都需要花在“创造”上，而且一定对人类社会有价值。到他主导了 MIT 事件的时候，他已经成了一个 activist。Aaron 内心的主张是什么？为什么他会如此奋不顾身？\n霍炬：说 Aaron 的主张之前，我们得引入一个新词，叫做 Technolibertarianism 或者 Cyber-Libertarianism。这个流派的代表人物 Julian Assange。Libertarianism 就是自由意志主义，代表人物安兰德。它看上去是个左派词，实际上是极右翼。在中国，很多人误以为安兰德是白左，这是望文生义。实际上这是保守主义再往右移动的结果。\nCyber-Libertarianism 就是互联网自由意志主义，这是硅谷 90 年代之前就开始的运动。主要是追求版权自由，代码自由，加密算法自由之类的。总之就是尽量自由，少受管控的互联网。这种视角塑造了那个时代的互联网和科技行业，并且影响到了 Aaron。所以他的最基本主张，就是互联网自由和知识平等。\nJade：看来Aaron的主张是一代人的延续。在他死后，也有 Alexandra Elbakyan 和她创立的 Sci-Hub，可以说是在继续延续 Aaron 的理念。为什么那个时候会出现这样一群人？是否与互联网发展的阶段有关？那时的互联网世界是什么样子的？\n霍炬：这个故事要从互联网出现之前讲起，那时候还是拨号 BBS 的时代。80 年代吧。网络被当作通信的延伸，所以也受很多通信法规的制约。最早玩这些东西的人，本来是觉得找到了一块没有那么多监管的世界。大家玩的很开心，并且创造了这个词：cyberspace（赛博空间）。\n但是很快，“你不关心政治，政治就来关心你”。很多人开始撞上了执法部门。一些是因为盗版，一些是因为黑客行动，还有一些是因为有人在 BBS 上贴了违法的东西（比如贴了个盗版软件的下载），那个时候所谓的“避风港原则”还没出现，所以很多人因为软件开始被 FBI 调查。\n这种调查没能阻止他们，反而把他们联合起来了。一些早期在软件上赚到钱的人，从 BBS 上知道这些事，就开始提供援助，比如 Mitch Kapor，就开始花钱帮大家找律师打官司。最后这些行为成立的组织就是前面说的 EFF。\n之后大家就开始逐渐清晰了一些原则，比如如何看待虚拟世界，隐私，加密，专有软件，论坛的言论自由…这条路走下去，就成了这种意识形态，即：互联网上的监管越少越好。\nJade：所以当时的互联网原住民有过一段时间是真心想要破除所有可能的监管的，并且非常努力地在促成这件事。你自己怎么看这种意识形态，你内心认同吗？\n霍炬：对，到今天也是一样。wikileaks 至今仍然在运转，虽然现在我们知道它或多或少是受俄罗斯控制的。以及，加密货币的发明者们，最早也是为了让资产流转不受控制。尽管现在我们可以用区块链做去一些另外的“受控制但仍然有意义的事情”。\n我在很多年前确实是认同的。但到后来，大家都意识到了完全没有监管，商业垄断力量就会长大。这样会让事情变的更糟糕，还不如接受政府监管（这里说的是基于代议制民主的政府监管），所以现在我是支持欧盟 GDPR 的。这个在 90 年代看来是不可思议的，90 年代的我也不可能支持它，但是现在社会情况变了，我现在支持它。\n02 屠龙者与龙 Jade：看来“去中心化”也是与“去监管”密不可分的主张了，但它是从技术设计上体现的。比如 RSS，最开始是完全去中心化的订阅管理的，后来衰落了。看看我们现在用的社交媒体，会觉得 RSS 被替代是必然。但是当时互联网人可不是这么认为的，我记得 RSS 还经历过非常激烈的内部辩论。\n霍炬：最早的互联网全是去中心的，人们先确定了协议，大家各自实现。RSS 走的是真正的互联网延续的这条路，它和去中心化的问题一样，就是对”用户存在一定要求“。商业互联网是另外一条路，最好标准和产品都在我这家公司手里，才能避免竞争对手，同时获得最大利润。这样当然同时也有更高的效率，更好的用户体验。用户可以什么都不懂也能用。\n产品上说这样肯定会更好。但是人们到底在乎什么，这是个问题。早期互联网用户极度在意隐私保护和加密（因为被 FBI 骚扰的太多了），但是我们现在说的互联网人们不太在意这些，到今天变得更不在意。虽然 GDPR 的出现似乎预示着开始有一定数量的人“认为这个问题很重要”了。所以下面如何发展，主要还是看用户如何看待这个问题。\nJade：与商业互联网并行的，是开源软件。开源软件在过去 20 年可以说是进展显著的，也孕育了很多优秀的商业和非商业产品。开源软件的今天和最早的互联网原住民想象的未来一样吗？\n霍炬：开源软件算是并行发展的一条线索，它比互联网商用早的多。最早程序员就是共享代码的，之后才有二进制发行，才有软件公司。可以说，我们前面说的互联网发生的事情，在软件上也发生过一次。开始大家都是交换代码的，后来软件公司开始变大，占领更大的市场份额，使得大部分人不得不去用这些软件（当时的代表就是微软），然后开源运动开始进入新的高度，得到了更多支持。\nESR 发表《大教堂与市集》是 1997 年，微软当时几乎是完全垄断的。\nSlashdot 上，当年微软的新闻，旁边的图标都是这个。那时候邪恶帝国就是微软。\n如果从软件角度看，开源软件算是部分实现了当年的想法，大部分基础设施都开源了，操作系统，编译器，编辑器，常见程序库，都是开源的，在此之上可以创建任何你想要的东西，从个人爱好，到巨型企业，都可以。在开源运动开始的时代，这个几乎是不可能的，没可能离开微软使用计算机。\n今天离开任何软件公司，你都可以在计算机上实现一样的功能。微软也变成了开源项目最多，对开源社区贡献最大的企业之一。但是互联网巨型企业创造了新问题，就是：大部分人没办法脱离这些公司了，Facebook 也好微信也好，现在很多用户离开这些的话，互联网对他们就没意义了。\nJade：我觉得这不完全是一个技术问题。不管是开源软件，还是知识平权，都面临一个无法回避的问题，那就是开发者的激励。比如很多人觉得没有专利，没有知识产权，创作者就没有动力去生产。说到底，这是一个资本主义的基础理论。很想知道你对这个反面的观点怎么看？开源世界的商业模式是一个值得探索的问题？没有专利的世界会像今天这样好吗？\n霍炬：商业公司当然存在，即使是 Technolibertarianism 也承认商业公司存在。他们的标准表述是“去除一切监管，然后让大家赚自己的钱”。商业公司从始至终存在，但是开源要解决的是没有商业产品替代品的问题，即，没有开源软件或自由软件的替代品，只有商业软件，就叫不自由。\n所谓的自由包括很多意思，比如没钱的人能不能通过软件获得同样的帮助，希望修改某些功能的用户有没有这种可能性去自行修改。知识产权需要存在，但是知识产权如果阻挡了这些自由，那就出问题了。\n当然在中国谈这个的时间点还不太对，中国知识产权保护是太弱，但美国知识产权保护是过强。这两个都有问题。所以总体来说，我支持开源，也支持 Technolibertarianism，但是同样在微信上会因为别人洗我稿而搏斗几年。\nJade：那我们怎么在“让更多人拥有更好的体验”和“维护有替代品的自有”之间找到平衡呢？这个问题的解决者应该是政府吗？\n霍炬：导致 Aaron 自杀的案件，就是他认为论文的商业利润导致知识不自由，一个人如果没钱，又没上大学，即使他有学习的能力和意愿，也很难看到需要的论文。这在 Technolibertarianism 看来是不可接受，必须改变的。Sci-Hub 的出现同样继承了这个观点，即：支持公司赚钱，但是赚钱到成为阻碍另外一些人的自由的时候，那它就成为障碍了。\n这个平衡很难找。你可以看到前面我们聊了这么多，从 70 年代开始，到 90 年代，到互联网时代，到区块链时代…大家都是在寻找这个问题的答案。而且社会发展，资本主义的逐利性，都会影响这个平衡的变化。一不小心，小公司突然变成了巨型企业，之前大家拥护的屠龙者突然变成了龙。\n上面这个问题的解决者是不是应该是政府，人们的看法也一直是在变化和摇摆的。Technolibertarianism 和 Libertarianism 在这里也有体现。\nTechnolibertarianism 支持的是政府在数字世界放松管制，但是在现实世界并不反对管制。这也是为什么中国人一说起来加州公司，都认为是“白左”，他们支持各种现实世界的平权和平等，支持控枪，但是同时支持数字世界的完全自由，以及政府不得管制加密算法。\n在 Technolibertarianism 看来，枪是无所谓的东西，但是加密算法和隐私才是最重要的。他们对加密算法自由的追求就和美国右派挺第二修正案一样。围绕加密自由，EFF 打了好几个影响巨大的官司。不然今天区块链也不会出现了，因为用的算法几乎都会被美国出口法律管制。\nJade：在互联网出现之前，信息和知识的获取本来就是不平等的。我们刚才触及到了互联网公司专利垄断，数据垄断的问题，以及政府监管的问题。但是换个例子，为什么 Google 这样的公司让信息获取很大程度地平等了？它没有对信息收费，它可以有其他的方式。那我是不是可以理解为，新的生产力，新的商业模式，很可能可以打破我们今天一些思考中的“僵局”，去到达更好的平衡？\n霍炬：Google 是一个特殊的例子，它现在有了很多变化，又因为竞争和利润压力不得不改变很多逻辑。但是它仍然是一家特殊的公司。如果你看 Facebook，那么就不这么乐观了。\n互联网公司促进了一部分信息平等，这没错。但是同时它们带来了一堆新的问题，比如隐私问题，以及获取了过量的个人数据。加上 AI 之后，这些东西产生什么后果难以预测。\n从乐观的角度说，是的，新的商业模式可以提供一些更好的平衡。从悲观的角度说，新的商业模式部分的解决了以往的问题，同时制造了更多更大更难以解决的新问题。\n03 什么才是真正的问题 Jade：“解决了以往的问题，同时制造了更多更大更难以解决的新问题。”这可就是一个哲学问题了。比如从哲学或神学来看，人类社会的发展，尤其是技术发展看起来是不断进步，其实只是螺旋打转。We are not going anywhere. 对此不同的哲学流派还提出了不同的解决方案。比如老庄就建议人类回到最原始朴素的生活状态，拒绝发展。而佛学认为人应该去关注自己的内心，把这些所谓的“进步”都看成不好也不坏的变化。有些西方学者，为了证明人还是在进步的，还举出了大量的数据来证明。\n这个说远了。。其实我想说的是，对事物本质的探索没有尽头。我之前写过一篇文章，本来是探索区块链行业的经济模型，结果最后变成了探讨资本主义。对于我来说，开源xx是难以用经济学解释的谜题，也和资本主义格格不入。你怎么从经济学角度，或人性角度去看开源这件事？\n霍炬：Google 是一个特殊的例子，它现在有了很多变化，又因为竞争和利润压力不得不改变很多逻辑。但是它仍然是一家特殊的公司。如果你看 Facebook，那么就不这么乐观了。\n互联网公司促进了一部分信息平等，这没错。但是同时它们带来了一堆新的问题，比如隐私问题，以及获取了过量的个人数据。加上 AI 之后，这些东西产生什么后果难以预测。\n从乐观的角度说，是的，新的商业模式可以提供一些更好的平衡。从悲观的角度说，新的商业模式部分的解决了以往的问题，同时制造了更多更大更难以解决的新问题。\n开源这件事本身不奇怪，因为程序员希望别人参与改进代码。\n比如高德纳发布 TEX 是开源的，从经济学角度很难解释一套如此强大的软件，已经超过当时商业排版软件水平的软件，为什么会开源。\n但是对于高德纳来说，这是他写 TAOCP 的副产品。他是因为没有工具能帮助他排版自己的书，所以写了个软件。所以，他不靠这个软件生存，又希望更多人使用它，希望别人参与改进它，就开源了。\n大型软件的开源是另外一回事，在有互联网之前，很难想象有 Linux 这个规模的软件是通过开源完成的。分布在世界各地的人们，竟然可以协调一致开发出一个这样的操作系统，GNU 这种组织梦想多年拥有自己的操作系统，但是始终没有成功。\n开源本身是有商业模式的，早期可以提供咨询，后期可以包装商业产品赚钱。所以开源软件是一个商业和个人兴趣，以及对自由的崇尚混合的产物。它也不算不符合经济学规律，只是对程序员的回报链条不一样。\nJade：说是这样说，但我也认识不少苦逼的开源创始人和程序员，经济回报难以衡量。说到底，资本主义和市场经济那一套没法概括所有的人类行为和动机，比如我和你在这聊这些再传播出去，仅仅是因为我们认为有意义，也没有什么回报。去年一些有名的开源公司，虽然不是开源产品本身，被收购了。这算不算开源世界的失败？能不能解释下最近热议的 amazon 损害开源社区的话题？开源有没有可能是个已经过时的“模式”？\n霍炬：说的没错，开源，创作共用，知识共享，这几件事在源头上都是一个。就是一些有余力，或者以其他人认为的“工作”为有趣的人，创造出来的东西分享给别人。这里面不完全包含经济回报。比如我们讨论这些问题，对于很多人这就是工作。我们在这不为报酬的工作，这也不符合经济规律。但是咱们是觉得这些话题有趣，愿意讨论，并没在乎回报这事。\n开源产品被收购不一定算失败，甚至可能算是成功，这说明了开源产品达到了比较好的质量或者潜力。但是收购确实导致了几次大型危机，比如 Oracle 收购了 Sun，这简直是开源社区的历史最大危机，这个危机一直延续到今天仍然有影响。比如，ZFS 成了 Oracle 的独家产品，开源的版本只是旧版，后继开发变得很困难，因为开发人员一旦参与这个开发，就要避免去很多家公司工作。不然的话同时进行这两项工作，就可能导致 Oracle 对他所在的公司发起的知识产权诉讼。\nAmazon 损害开源社区是最新的一次危机。Amazon 从开源社区拿走软件，重新包装之后当作云服务卖掉了。之后大批用户是通过 AWS 使用这些软件，自己不会亲自去用了。Amazon 在这些软件之上的修改也未必开源。这使得 AWS 竞争力会高于开源社区提供的版本，成了恶性循环。\n这些会造成开源社区的萎缩或者部分项目受损，但是开源运动生命力是很顽强的。即使萎缩成了更小的社区，它也仍然会存在和发展。在这个问题上比互联网的问题小一些，因为开源软件的参与者是职业程序员，比起来需要包装成良好用户体验的普通用户，职业程序员的容忍度更好一些。\n04 我们正在 1992 年 Jade：这么看来，如今的互联网世界越来越“中庸”。而在过去 10 年里，最极客，最让人又开始尝试挑战权威的可能就是比特币和区块链了。很多人说现在的区块链特别像过去的互联网，哪儿像了？要是像，那到底是 80 年代，90 年代的，还是 2000 年的？\n霍炬：非常像。这个像可以从很多不同角度来看。\n比如，前面说了，开源社区当年视微软为邪恶帝国，一心希望提供一个替代操作系统和办公软件，彻底打败微软。但最终打败微软的不是他们提供的 Linux 桌面，而是基于开源软件的互联网。人们使用计算机的主要活动变成了通过浏览器完成，软件和操作系统都不重要了。\n区块链和互联网也一样。人们希望解决互联网公司的数据封闭和隐私问题，最终也需要通过一种完全不一样的模式来解决，区块链在目前看来最有可能成为这种模式。\n像哪一年这个问题，我仔细对比过，我个人觉得像互联网的 1992 年。\n1992 年的互联网还只是一个展示简单信息的地方，几乎没有任何交互。1995 年 Javascript 才发明出来，页面上才有了一点交互。那时候 www 还很弱，我们今天意义上这个互联网还没成型（实际上后来对于大多数用户来说，浏览器里的 www 就是互联网）用户基本上都是专业用户和爱好者。差不多就是这时候。\nJade：那么在 90 年代初，互联网有没有经历过现在区块链在“协议层”上的竞争和讨论呢？现在，区块链行业 90% 以上的资源和注意力都还是在公链上，还在 debate 不同的共识机制，隐私强度等，在不可能三角上来回取舍。你觉得最后是会各有各的用户和目的，还是从百花齐放到统一标准？\n霍炬：经历了，之前说到 RSS 协议上的斗争，其实那就是互联网发展一贯的状况，只是到 RSS 这个时代，商业力量太强了，也太快了。所以 SNS 瞬间就拿到了更多用户，RSS 就被抛弃了。\n除了在一个协议标准上的竞争，不同协议之间也在竞争。比如，现在难以想象，Tim Berners-Lee 发明 www 的时候，是有很多竞争者的，其中一个强力对手叫做 Gopher。在 90 年代，Gopher 一度发展的比 www 快的多，大量信息都是在 Gopher 服务器上，在 www 上只有非常简单的内容。而且 Gopher 速度快的多，信息检索能力也强得多。\n但是 Gopher 的发明者，明尼苏达大学，在那个时候把这个协议变成了收费软件，后来就比不上 www 的发展了。今天我们说“Tim Berners-Lee 没有为 www 的发明申请专利，完全免费提供了”，实际上是，如果 www 当时也收费，今天的互联网可能根本就不一样了。搞不好我们用的是增强功能之后发展出来的下一代 gopher。\n这也是发生了 95 年前后的事情，所以今天这些竞争也都正常，大家都在寻找更好的解决方案。最终可能会统一到几个标准上，但是希望人们接受教训，能统一到一个 www 这样自由，开放，扩展性好的标准上，而不是一个专有_垄断_高度受控于某个商业公司的标准。\nJade：现在区块链重新开始提去中心化，自由，隐私等”老概念”，在你看来有任何新颖之处吗？我问这个问题的原因是，我觉得新的技术通常解决的是新的问题，就好像 2000 年前的一批互联网公司，总是尝试把传统行业的一些问题搬到互联网上解决。我不是说这样不对，我只是觉得缺乏创意。\n霍炬：对我来说，没有新颖之处，因为 Cypherpunk 这帮人从 90 年代就在谈这些事情，一直谈到今天。如果从他们的视角看，今天的互联网是一个岔路，他们研究的路线才是正路。从 Cypherpunk 中冒出来一个比特币，就像从开源世界冒出来 Linux。\n就是说，Linux 刚出现的时候，弱小，粗糙，和商业化 Unix 差十万八千里。但是它代表了一种可以与之抗衡的力量登上舞台了。\n区块链这个行业现在混杂了各种人，跟那个时代的互联网也差不多。骗子，野心家，真正的高手，大家都混在一起了。普通观众看着热闹，但是分不清角色。\n你看 David Chaum 最近还经常冒出来对区块链世界发表一些看法和展望。但是大家似乎并不知道他是谁，也不知道他代表了什么…你说相信 XXX（这里我就不提具体名字了，你可以自己脑补）的人，怎么能理解 David Chaum 在说什么呢？\nJade：这是技术层面。虚拟货币也触碰到了很多经济政治层面的主张。可我觉得大家只是在“怀旧”，没有啥新意。比如去年 shut down 的 basis 项目，基本上就是对奥派经济学和哈耶克货币的非国家化的致敬和复制。在这些经济和政治主张里，有哪些是你认同的，哪些不认同？比如，我不太认同区块链是关于“生产关系”的革命，我觉得如果生产关系能被革命，一定还是生产力先被革命了。\n霍炬：经济学我是外行，这方面我很难说哪些是新的。不过说到社会这个层面，思想本身似乎很难说新/旧来概括。一些过去没意义的想法，在今天技术的帮助下，开始变得有意义和可行。\n比特币最早的政治目的，和生产关系/生产力关系不大。它的政治目的仍然就是 Cypherpunk 的追求自由和隐私的体现，即，不依赖于任何大机构，不依赖银行，创造一种货币系统。\n比特币和加密货币最早为了解决的问题没有今天大家认为的这么大，区块链火起来之后，人们赋予了它越来越多的含义和可能性，包括生产力和生产关系的描述。当然这也很正常，互联网刚出现的时候要解决的问题也没有这么多，要解决的也就是数据传递和协作问题。但是之后顺理成章的就出现了更多新问题，进而改变了很多行业，算是间接改善了生产力吧。\n05 我要星辰大海，也要 Facebook Jade：我特别好奇，Aaron Swarts 如果还没去世，他想看到，或者他想创造一个什么样的未来世界？我说的”未来“，不是五年十年，而是 20 年以上的维度。你的”超级想象“是什么？\n霍炬：他想创造的未来，就是他死之前努力的那些，平等的知识，平等的资源，更好的隐私，更好的工具…但是他太年轻了，到他40岁的时候，面对那样的世界他会做什么？这个就难以推测了。\n不过我能确信的是，如果他今天还活着，应该会忙着反川普，解决美国国内的各种问题。应该也会活跃在区块链领域。\n从我最早回答加拿大那个问题，你就应该猜到了，我的超级想象是反乌托邦的世界。就不仔细描述了…\nJade：好，观众们自己体会。刘慈欣说，我要的是星辰大海，你却给了我 Facebook。但我有时候想想，Facebook 也许是比宇宙飞船更有意义的发明。我以前读阿西莫夫《永恒的终结》，里面有一种人可以穿越时间修改历史事件，结果一次次地改成人类并没有飞出地球征服宇宙，因为在宇宙中，发生着和地球上类似的贪婪的争夺，而人类在很多个平行时空里都被终结了。\n我不是说探索地球之外不重要，但有时候我看着一些普通人是怎样被互联网这样的工具改变了人生，我就觉得这对他们来说更重要。你觉得互联网到了人们所说的“成熟期”吗？继续下去的发展方向会是什么？\n霍炬：首先，这句话不是刘慈欣说的。10 年前我就在 TED 看到过有人演讲用过这句话（https://www.ted.com/talks/jason_pontin_can_technology_solve_our_big_problems）。\nFacebook 和宇宙飞船都很重要。但是现在互联网公司占有的利润太大了，使得“高科技”几乎被等同于互联网了。这个势头是很不好的。\n互联网应该算成熟期了，因为这些年已经很少有纯粹的互联网创新了，我是说类似 Google 和 Facebook 这样的，属于互联网本身的创新。再继续发展下去，前面说了，它创造的问题已经比它能解决的问题更大的。\n当然另外一方面，互联网在传递信息这个方面还有很多问题没解决，比如，其他行业，工业，医疗，很多领域，目前的互联网渗透率实际上是不够的，还有很多能解决的问题。\nJade：谢谢，我希望我们的谈话是启发性的，我和你主张什么并不重要，重要的是不管一个人生活在哪里，什么时代，都不应该失去提问的勇气。最后能不能推荐你最喜欢的一个互联网产品，一本书，和一部电影？\n霍炬：这个问题可真难回答…\nGithub 吧，它不应该只限于程序员，应该所有人都去试着用它。\n我最近看的最像书的书是…法语教材…\n电影，我可以推荐一部动画片“breadwinners”。\nJade：法语教材…好的。Breadwinners我也喜欢，在飞机上看的。那你再推荐一个加拿大值得去的地方吧！\n霍炬：Cape Breton，是个岛。这地方不仅有超级好的风景，还是 Alexander Graham Bell 的住所，就是电话的发明人,有个纪念馆。\n很高兴你也喜欢 breadwinners，这个动画片的原作是一个非常神奇的人，加拿大人，没有任何中东血统。完全是因为社会活动，决定到中东去探访他们的生活，走访了很多难民营，最后写出了这些故事。她也是一位社会活动家，只是不算那么纯粹的互联网活动家。\nJade：最后，能不能送一句话给大家的 2019？\n霍炬：希望大家 2019 年能尝试一些以往没试过的东西，我指那些平时觉得“太麻烦了干嘛要去学”的东西。\nJade：真棒，刚才音乐播放器恰好随机播到 Jason Mraz 的 Life is Wonderful，我觉得在这首歌里结束今天的聊天非常美。歌词是这样的：\nIt takes a crane to build a crane It takes two floors to make a story It takes an egg to make a hen It takes a hen to make an egg There is no end to what I’m saying\nIt takes a thought to make a word And it takes some words to make an action It takes some work to make it work It takes some good to make it hurt It takes some bad for satisfaction\nLife is wonderful. Life goes full circle.\n","title":"互联网之子 Aaron Swarts 想要看到的世界"},{"location":"https://codenow.me/tips/pbcopy/","text":" 用 vi 在 terminal 中复制信息 pbcopy \u0026lt; filename.extension # eg. pbcopy \u0026lt; a.txt  ","title":"vi cheet sheet: pbcopy"},{"location":"https://codenow.me/algorithm/to_lowercase/","text":" 题号：709 难度：easy 链接：原题 描述：使用 ASCII 把字母统一为小写 class Solution: \u0026#34;\u0026#34;\u0026#34;use ASCII to return the same str in lowercase \u0026#34;\u0026#34;\u0026#34; def to_lower_case(self, str: str) -\u0026gt; str: new_str = \u0026#39;\u0026#39; for c in str: if 65 \u0026lt;= ord(c) \u0026lt;= 90: # Uppercase is between 65 and 90 in ASCII table c = chr(ord(c)+32) new_str += c return new_str # one line solution #return \u0026#39;\u0026#39;.join(chr(ord(c) + 32) if 65 \u0026lt;= ord(c) \u0026lt;= 90 else c for c in str) if __name__ == \u0026#39;__main__\u0026#39;: solution = Solution() s = \u0026#39;Hero\u0026#39; solution.to_lower_case(s)","title":"Leetcode: 709 To Lower Case"},{"location":"https://codenow.me/tips/github_binds_dominname/","text":"第一步： 1. 新建一个GitHub仓库，取名为your-github_name.github.io 2. 新建一个文件，取名为CNAME,填写内容为域名。不需要添加http或https。\n第二步： 1. 在本地cmd中，ping第一步中新建的仓库名称。会返回一个ip地址，记录下该地址。\n第三步： 1. 打开购买的云服务器，因为我购买的阿里云，所有在这上面进行操作演示。 2. 在控制台中打开域名管理。\n 找到解析。添加如下信息：\nA：对应cmd中ping的地址。\nCNAME：对应新建的GitHub仓库名。  完成上述步骤后，配置完成。\n参考网址\n","title":"GitHub 绑定自己的域名"},{"location":"https://codenow.me/algorithm/leetcode_165_compare-version-numbers/","text":" 题号：165\n难度：medium\n链接：https://leetcode-cn.com/problems/compare-version-numbers/\n class Solution: def compareVersion(self, version1: str, version2: str) -\u0026gt; int: vers1 = version1.split(\u0026#34;.\u0026#34;) vers2 = version2.split(\u0026#34;.\u0026#34;) diff = len(vers1) - len(vers2) if diff \u0026gt; 0: vers2 += [\u0026#39;0\u0026#39;] * abs(diff) elif diff \u0026lt; 0: vers1 += [\u0026#39;0\u0026#39;] * abs(diff) else: pass for i, v1 in enumerate(vers1): v1, v2 = int(v1), int(vers2[i]) if v1 \u0026gt; v2: return 1 elif v1 \u0026lt; v2: return -1 else: continue return 0","title":"Leetcode 165: Compare Version Numbers"},{"location":"https://codenow.me/translation/http-propagation-context/","text":" Go 1.7 引入了一个内置的 context 类型，在系统中可以使用 Context 来传递元数据，例如不同函数或者不同线程甚至进程的传递 Request ID。\nGo 将 Context 包引入标准库以统一 context 的使用。在此之前每个框架或者库都有自己的 context 。它们之间还无法兼容，导致了碎片化，最终在各处 context 的传播上就有不少的麻烦。\n虽然在同一个处理过程中有一个通用的 context 传播机制是非常有用的，但是 Go 的 Context 包并没有提供该功能。就像上面描述的，context 会在网络中被不同的处理过程传递。例如在多服务架构中，一个请求往往会在多个地方被处理 (多个微服务，消息队列，数据库等)，直到最后响应给用户。能够在多个处理过程中传递 context 显得尤为重要。\n如果你要在 HTTP 中传播 context ，需要你对 context 进行序列化处理。类似的，在接收端也要解析，同时把值放入当前的 context 中。假设我们希望在 context 中传递 request ID。\npackage request import \u0026#34;context\u0026#34; // WithID 把 request ID 放入当前的 context 中 func WithID(ctx context.Context, id string) context.Context { return context.WithValue(ctx, contextIDKey, id) } // IDFromContext 返回从 context 中获取的 request ID // 如果 context 中没有定义就返回空值 func IDFromContext(ctx context.Context) string { v := ctx.Value(contextIDKey) if v == nil { return \u0026#34;\u0026#34; } return v.(string) } type contextIDType struct{} var contextIDKey = \u0026amp;contextIDType{} // ... WithID 允许我们把 request ID 设置到 context 中，IDFromContext 可以从 context 中读取 request ID。一旦我们有在多个处理过程，就需要手动把到 context 设置到传输中，同时在接受端解析然后写入 context。\n在 HTTP 中我们可以从 header 中获取 request ID。大多数的 context 都可以通过 header 来传播。一些传输层可能不支持 headers 或者 headers 不是传输标准 (例如有大小限制或者缺少加密措施)。在这种情况下，由具体实现来决定如何传递上下文。\nHTTP 传播 目前没有直接的方法可以在 HTTP reuqest 中的值放入 context 中。由于无法遍历出 context 的值，因此也无法一次性转换整个上下文。\nconst requestIDHeader = \u0026#34;request-id\u0026#34; // Transport 把 request context 序列化到 request headers type Transport struct { // Base 是构建请求的真实 round tripper  // 如果没有被设置，默认使用 http.DefaultTransport \tBase http.RoundTripper } // RoundTrip 转换 request context 到 headers 中 // 同时构建请求 func (t *Transport) RoundTrip(r *http.Request) (*http.Response, error) { r = cloneReq(r) // per RoundTrip interface enforces  rid := request.IDFromContext(r.Context()) if rid != \u0026#34;\u0026#34; { r.Header.Add(requestIDHeader, rid) } base := t.Base if base == nil { base = http.DefaultTransport } return base.RoundTrip(r) } 在上面的 Transport 中，如果 request ID 存在就会被当做 \u0026ldquo;request-id\u0026rdquo; header 进行传递。\n类似的方法可以解析请求，把 \u0026ldquo;request-id\u0026rdquo; 放入请求的上下文中。\n// Handler 从 request headers 反序列化到 request context 中 type Handler struct { // Base 是完成反序列化调用的真实方法  Base http.Handler } func (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) { rid := r.Header.Get(requestIDHeader) if rid != \u0026#34;\u0026#34; { r = r.WithContext(request.WithID(r.Context(), rid)) } h.Base.ServeHTTP(w, r) } 为了继续传播 context ，请确保在你的方法中把当前的 context 传递到下一个 request 。传入的 context 将会随着 request 传播到 https://endpoint。\nhttp.Handle(\u0026#34;/\u0026#34;, \u0026amp;Handler{ Base: http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;https://endpoint\u0026#34;, nil) // 传播当前的 context  req = req.WithContext(r.Context()) // Make the request.  }), })","title":"Go Context 在 HTTP 传播"},{"location":"https://codenow.me/articles/2pc/","text":" 两阶段提交协议\n在分布式系统中每个节点都可以知道自己的操作是成功还是失败，但是无法知道其他节点的状态。为了保证一个事务的 ACID 特性，一个节点发生失败就要在所有节点上执行 rollback 操作。需要引入一个 协调者 来维护各个 参与者 的状态，以保证最终一致。\n2pc 并不是万能的，需要满足一定的条件才可以使用：\n 一个节点是协调者，其他节点作为参与者，相互之前可以通信 每个节点要有 redo log 机制，而且存在持久化存储中 节点不会永久损坏，一定时间会重启恢复  Tow-phase Commit Protocol 首先引入几个概念：\n 协调者：维护所有节点 参与者：执行具体操作的节点 prepare phase：准备阶段，写入日志，资源加锁 commit phase：执行阶段，根据协调者指令执行，资源解锁  上图中 ① 和 ② 表示 prepare phase，③ 和 ④ 表示 commit phase。\n在 prepare phase 阶段，协调者发出信息让参与者准备。参与者接受到信息以后一般会做两件事情：\n 根据需要执行的操作生成 redo 日志，用于后续 commit 或者 rollback 给所需的资源上锁，防止其他程序获取  参与者完成这两个操作会把结果通知协调者。\n当协调者接受到参与者在 prepare phase 阶段的响应（无论 Yes 还是 No），就会进入 commit phase 阶段。在该阶段，如果接受到 prepare phase 的响应所有都是 Yes 时候，会发出 Commit 指令；只要收到一个 No，发出的就是 Rollback 指令。\n参与者接受到协调者发出的 Commit 或者 Rollback 指令后会做两件事情：\n 执行 Commit 或者 Rollback 清理资源，解除锁的占用  的当这两部都完成以后，会回复 ACK 个协调者。\n故障时期的处理 前面描述都的都正常情况，如果协调者或者参与者一个或者全部发生宕机时候，2pc 的处理方式在某些时候是无法保证数据一致性。\n协调者和参与者会在 ① ② ③ ④ 的前后阶段，同时发生故障，也有可能其中一方发生故障。\n① 之前发生故障，即操作还没开始：\n 协调者宕机：重新选举出一个协调者继续操作，这个宕机的节点重启后询问新的协调者继续操作 参与者宕机：重启后像协调者询问操作，然后继续即可 协调者和部分参与者宕机：同协调者宕机  ① 之后 ② 之前发生故障，即接收到是否可以 commit 询问，响应之前：\n 未执行操作前（执行过程中中断）：  协调者宕机：询问已经发出，重新选择一个新的协调者，根据参与者返结果继续操作 参与者宕机：等待恢复后询问协调者节点状态，然后继续操作 协调者和部分参与者宕机：重新选择一个新的协调者，根据参与者返结果继续操作  操作执行完成之后：  协调者宕机：重新选举一个新协调者，询问参与者当前步骤 部分参与者宕机：重启后 rollback，询问协调者后续步骤 协调者和部分参与者宕机：选举出新的协调者，重启后 rollback ，询问新协调者后续步骤   ② 之后 ③ 之前发生故障：\n 未接到参与者响应（执行过程中中断）：  协调者宕机：选出先的协调者，询问各个节点状态，宕机节点重启后 rollback，询问新协调者后续操作 部分参与者宕机：重启后 rollback 然后询问协调者状态，继续操作 协调者和部分参与者宕机：同协调者宕机  操作执行完成之后：  协调者宕机：选出新的协调者，询问各个节点状态，宕机节点重启后 rollback，询问新协调者后续操作 部分参与者宕机：重启后 rollback 然后询问协调者状态，继续操作 协调者和部分参与者宕机：同协调者宕机   ③ 之后 ④ 之前故障：\n 未接到协调者发出的 commit 或者 rollback：  协调者宕机：指令已经发出，选举一个新的协调者询问正常参与者执行情况。宕机节点重启后 rollback，然后询问新的协调者指令情况，然后继续操作 部分参与者宕机：重启 rollback 后询问协调者，继续操作即可 协调者和部分参与者宕机：同协调者宕机  接受到 commit 或者 rollback（或者执行过程中中断）：  协调者宕机：重启 rollback 后询问新协调者，继续操作即可 部分参与者宕机：重启后先 rollback，然后询问协调者操作 协调者和部分参与者宕机：这个时候存在一个极端情况，发出 commit 和接收到的参与者同时宕机，剩下的参与者 timeout 触发 rollback。重启后的宕机机器 commit 已经完成，这时候就和剩下的节点发生了数据不一致的场景，已经完成 commit 的节点也无法 rollback   从故障时期的处理可以总结出，如果是单是协调者发生故障，那么重新选举出一个新的，然后询问参与者的状态即可。如果是单是参与者发生故障，重启后询问协调者操作也可以恢复状态。协调者和参与者同时宕机，同时又在 commit phase 执行完成之后，就会发生数据不一致的场景，这个是 2pc 无法解决的。\n2pc 存在的缺点 2pc 组要存在 4 个缺点：\n 同步阻塞：在 commit phase 阶段完成之前，这部分资源无法被其他程序获取 单点问题：commit phase 阶段协调者发生故障，只能全体 rollback 数据不一致：极端情况会导致部分 commit 部分 rollback 缺乏容错机制：发生无法恢复的错误只能依赖 timeout 执行 rollback  ","title":"两阶段提交协议"},{"location":"https://codenow.me/tips/dep-visualizing-dependencies/","text":"Linux:\n$ sudo apt-get install graphviz $ dep status -dot | dot -T png | display macOS:\n$ brew install graphviz $ dep status -dot | dot -T png | open -f -a /Applications/Preview.app Windows:\n\u0026gt; choco install graphviz.portable \u0026gt; dep status -dot | dot -T png -o status.png; start status.png","title":"dep package 依赖关系图"},{"location":"https://codenow.me/articles/python3-crontab/","text":"这周需要在容器中跑一个定时脚本\n现成的方式有很多： 1. 直接使用 ubuntu:14.04 的镜像，内置 crontab 和 python3.4 2. 想用 python3.6 的话，可以用 python:3.6 的镜像装一个 crontab 也成 3. dockerhub 上别人应该也有这种需求，捞一个就成\n不过我还是想自己拼一个，要求： 1. 需要包含 crontab 和 python3.6 2. 需要能支持使用 pip 安装其他扩展包 3. 镜像要尽量小\n思路以及需要注意的地方大概是： 1. 装上各种必要的东西 2. 设置时区 3. 配置好 crontabfile 4. 运行时启动 crond，并用 tail -f 来保证容器不退出\n目前只是做了个能用的，用 python3.6-alpine 做源，往上怼了点够自己使用的东西，先实现了需求\n下一步是直接用 alpine 或者 buildpack-deps 来构建镜像，以此精简，留着 TODO 吧\n我写了个 demo 放到了 github 上: https://github.com/WokoLiu/python3-cron ，也同步到了 dockerhub 上 docker pull woko/python3-cron\n文件结构是这样的：\n. ├── Dockerfile ├── crontabfile ├── scripts.py └── requirements.txt   Dockerfile 负责构建镜像 crontabfile 是写有 crontab 内容的文件 scripts.py 是要运行的脚本文件 requirements.txt 是需要 pip 安装的扩展  关键内容是 Dockerfile，鉴于还没完善，不好意思在这里详细讲，感兴趣的话可以去看看，后面会再更新\n刚学 docker 没多久，一些地方还不清楚，有问题还请不吝赐教，多谢\n","title":"Python3 Crontab"},{"location":"https://codenow.me/algorithm/leetcode_12_integertoroman/","text":" 题号：12 难度：medium 链接：https://leetcode.com/problems/integer-to-roman 描述：阿拉伯数字转罗马数字(1-3999)\n class Solution: \u0026quot;\u0026quot;\u0026quot;这道题的点在于，1/10/100/1000是可以重复的，其他数字是不可以重复的，如果用循环处理的话，要注意这一点\u0026quot;\u0026quot;\u0026quot; def intToRoman1(self, num: int) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;照例先撸一个无脑的出来 这个还算快，不过占用空间比较大 \u0026quot;\u0026quot;\u0026quot; ten = ['', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX'] roman = '' m, num = divmod(num, 1000) roman += 'M' * m if num \u0026gt;= 900: roman += 'CM' num -= 900 elif num \u0026gt;= 500: roman += 'D' num -= 500 elif num \u0026gt;= 400: roman += 'CD' num -= 400 c, num = divmod(num, 100) roman += 'C' * c if num \u0026gt;= 90: roman += 'XC' num -= 90 elif num \u0026gt;= 50: roman += 'L' num -= 50 elif num \u0026gt;= 40: roman += 'XL' num -= 40 x, num = divmod(num, 10) roman += 'X' * x + ten[num] return roman def intToRoman(self, num: int) -\u0026gt; str: \u0026quot;\u0026quot;\u0026quot;看了一下 discuss，有个更简洁的，同样够快且占用空间大\u0026quot;\u0026quot;\u0026quot; M = ('', 'M', 'MM', 'MMM') C = ('', 'C', 'CC', 'CCC', 'CD', 'D', 'DC', 'DCC', 'DCCC', 'CM') X = ('', 'X', 'XX', 'XXX', 'XL', 'L', 'LX', 'LXX', 'LXXX', 'XC') I = ('', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII', 'IX') return M[num // 1000] + C[(num % 1000) // 100] + X[(num % 100) // 10] + I[num % 10] if __name__ == '__main__': print(Solution().intToRoman(1994))  ","title":"Leetcode_12_IntegerToRoman"},{"location":"https://codenow.me/tips/git_lg/","text":"git log 可以用来查看提交历史，但格式并不舒服，我们可以通过配置 git config 来解决这个问题：\n运行\ngit config --global alias.lg 'log --pretty=format:\u0026quot;%h - %an, %ad : %s\u0026quot; --date=format:\u0026quot;%Y-%m-%d %H:%M:%S\u0026quot;'\n之后，便可以使用 git lg 来查看精简版本的，更舒服的提交历史了，如本项目现在看的话会变成\n80504ec - jarvis, 2019-03-31 13:58:55 : add origin article link df1bcfd - jarvis, 2019-03-31 13:51:45 : Second week atts of jarvys eede6bf - chuntao.han, 2019-03-30 17:33:10 : hanchuntao translation checkin 1 769b56b - chuntao.han, 2019-03-30 13:07:45 : hanchuntao tips checkin 1 5507805 - chuntao.han, 2019-03-30 12:41:09 : add algorithm for mistake ef5c5a8 - CTubby, 2019-03-30 12:04:58 : tubby checkin d698c58 - chuntao.han, 2019-03-30 00:49:57 : hanchuntao algorithm 1 checkin 05b1e47 - chuntao.han, 2019-03-29 00:12:22 : hanchuntao article checkin 1 4792f15 - Woko, 2019-03-28 23:41:32 : Rename [Go]Exercise of A Tour of Go.md to Exercise_of_A_Tour_of_Go.md 0da03b5 - Woko, 2019-03-28 23:37:26 : Rename [mysql]Is varchar a number?.md to is_varchar_a_number.md 6b112eb - Frost Ming, 2019-03-28 14:11:46 : Quit org c1a8704 - zhengxiaowai, 2019-03-24 23:56:24 : week 1 atts by zhengxiaowai  看起来是不是舒服多了\n","title":"Git_lg"},{"location":"https://codenow.me/translation/tidb-proposal-support-skyline-pruning/","text":" 原文链接：Proposal: Support Skyline Pruning，翻译如下：\n摘要 这篇建议引入了一些启发式规则和一个针对消除访问路径 (access path) 的通用框架。通过它的帮助，优化器可以避免选择一些错误的访问路径。\n背景 目前，访问路径的选择很大程度上取决于统计信息。我们可能会因为过期的统计信息而选择错误的索引。然而，很多错误的选择是可以通过简单的规则来消除的，比如：当主键或者唯一性索引能够完全匹配的时候，我们可以直接选择它而不管统计信息。\n建议 (Proposal) 目前在选择访问路径时最大的因素是需要扫描的数据行数，是否满足物理属性 (physical property) ，以及是否需要两次扫描。在这三个因素当中，只有扫描行数依赖统计信息。那么在没有统计信息的情况下我们能够怎样比较扫描行数呢？让我们来看一下下面这个例子：\ncreate table t(a int, b int, c int, index idx1(b, a), index idx2(a)); select * from t where a = 1 and b = 1; 从查询和表结构上，我们能够看到使用索引 idx1 扫描能够覆盖 idx2，通过索引 idx1 扫描的数据行数不会比使用 idx2 多，所以在这个场景中，idx1 要比 idx2 好。\n我们如何综合这三个因素来消除访问路径呢？假如有两条访问路径 x 和 y，如果 x 在这几个方面都不比 y 差并且某个因素上 x 还好于 y，那么在使用统计数据之前，我们可以消除 y，因为 x 在任何情况下都一定比 y 更好。这就是所谓的 skyline pruning。\n基本原理 (Rationale) Skyling pruing 已经在其他数据库中实现，包括 MySQL 和 OceanBase。要是没有它，我们可能会在一些简单场景下选择错误的访问路径。\n兼容性 Skyling pruning 并不影响兼容性。\n实现 在为数据寻找最好的查询方式时，由于我们要决定使用哪一个满足物理条件的访问路径，我们需要使用 skyling pruning。大部分情况下不会有太多索引，一个简单的嵌套循环算法就足够了。任何两个访问路径的比较方式已经在 Proposal 章节里介绍过了。\n引用  The Skyline Operator  ","title":"TiDB Proposal: Support Skyline Pruning"},{"location":"https://codenow.me/tips/idea-%E5%89%8D%E8%BF%9B%E5%90%8E%E9%80%80%E5%BF%AB%E6%8D%B7%E9%94%AE/","text":"前进：Command + ]\n后退：Command + [\n在追踪比较复杂的代码时，比较有用，可以快速前进后退，不至于迷失在代码中。\n","title":"Idea 前进后退快捷键"},{"location":"https://codenow.me/articles/tidb-subquery-optimization/","text":" 根据 TiDB 中的子查询优化技术 这篇文章的介绍，TiDB 在处理关联子查询时引入了 Apply 算子。然后使用关系代数将 Apply 算子等价转换成其他算子，从而达到去关联化的目的。理论上，所有的关联子查询都可以去关联化，具体的理论知识可以看这篇博客：SQL 子查询的优化。\n本文从代码角度，梳理一下常见关联子查询的优化。处理过程主要有两个阶段：\n 重写阶段：在将语法树转换成逻辑查询计划时，将子查询重写成带有 Apply 算子的查询计划，这部分主要是由 expressionRewriter 负责 去关联化：在优化逻辑查询计划时，尝试将 Apply 算子替换成其他算子，从而去关联化，这部分主要有 decorrelateSolver 负责  expressionRewriter 简介 expressionRewriter 负责将子查询重语法树写成带有 Apply 算子的查询计划。为了实现这一功能，需要能够遍历语法树，expressionRewriter 实现了 Visitor 接口，能够遍历语法树中的各个节点，在遍历过程当中完成重写工作，它的核心的方法主要是 Enter 和 Leave。\nVisitor 接口一般会被这样使用：\nfunc (n *CompareSubqueryExpr) Accept(v Visitor) (Node, bool) { newNode, skipChildren := v.Enter(n) if skipChildren { return v.Leave(newNode) } n = newNode.(*CompareSubqueryExpr) node, ok := n.L.Accept(v) //...  n.L = node.(ExprNode) node, ok = n.R.Accept(v) //...  n.R = node.(ExprNode) return v.Leave(n) } 每个语法树节点通过调用 Accept、Enter 和 Leave 方法来实现对整个语法树节点的遍历。\nQ1 select t1.a from t t1 where 1000 in (select t2.b from t t2 where t2.a = t1.a) Q1 是最简单常见的一种子查询，通过对 Q1 的分析，我们能够理解子查询优化的框架。\n重写阶段 在 Q1 中，子查询出现在 where 当中，在构建逻辑查询计划时，会被 expressionRewriter 重写。\n构建 select 查询计划过程中，主要会执行以下几个方法，分别用来构建 DataSource、Selection、Aggregation 等逻辑查询节点：\n buildSelect  buildResultSetNode resolveGbyExprs resolveHavingAndOrderBy buildSelection buildAggregation buildProjection   buildSelect 中被调用的这些函数都使用了 expressionRewriter ，在 Q1 中，子查询重写发生在 buildSelection 当中：\nfunc (b *planBuilder) buildSelection(p LogicalPlan, where ast.ExprNode, AggMapper map[*ast.AggregateFuncExpr]int) LogicalPlan { //...  conditions := splitWhere(where) //...  for _, cond := range conditions { expr, np, err := b.rewrite(cond, p, AggMapper, false) //...  } //... } Q1 的 where 部分比较简单，只包含了一个子查询 和 in 组成条件，对应的是 PatternInExpr 类型，先简单了解一下 PatternInExpr：\ntype PatternInExpr struct { exprNode // Expr is the value expression to be compared.  Expr ExprNode // List is the list expression in compare list.  List []ExprNode // Not is true, the expression is \u0026#34;not in\u0026#34;.  Not bool // Sel is the subquery, may be rewritten to other type of expression.  Sel ExprNode } 在关联子查询中主要用到了 Sel 和 Expr 属性，Sel 对应的是子查询 select t2.b from t t2 where t2.a = t1.a ，Expr 对应的是常量 1000。\n根据 expressionRewriter 的 Enter 方法，处理逻辑主要在 handleInSubquery 当中。\n// Enter implements Visitor interface. func (er *expressionRewriter) Enter(inNode ast.Node) (ast.Node, bool) { switch v := inNode.(type) { //...  case *ast.PatternInExpr: if v.Sel != nil { return er.handleInSubquery(v) } //...  //...  } return inNode, false } expressionRewriter 还有 handleCompareSubquery、handleExistSubquery、handleScalarSubquery 等方法分别用来处理其他几种子查询。\n分析 handleInSubquery 代码，简化后的代码如下：\nfunc (er *expressionRewriter) handleInSubquery(v *ast.PatternInExpr) (ast.Node, bool) { //...  lexpr := er.ctxStack[len(er.ctxStack)-1] subq, ok := v.Sel.(*ast.SubqueryExpr) //...  np := er.buildSubquery(subq) // 构建子查询  //...  var rexpr expression.Expression //...  checkCondition, err := er.constructBinaryOpFunction(lexpr, rexpr, ast.EQ) // 构建查询条件  //...  er.p = er.b.buildSemiApply(er.p, np, expression.SplitCNFItems(checkCondition), asScalar, v.Not) // 创建 Apply 算子  //...  return v, true } expressionRewriter 先构建子查询的查询计划，然后根据 In 条件参数创建 Apply 的 conditions，最后调用 buildSemiApply 方法构建 Apply 查询计划。\n小结 为 Q1 构建查询计划过程中，与子查询重写有关的函数调用过程大致如下，expressionRewriter 简写成 er。\n buildSelect() \u0026lt;= 创建 Select 语句的查询计划  buildSelection() \u0026lt;= 创建 Selection 节点 rewrite() \u0026lt;= 重写 Q1 的子查询  exprNode.Accept(er) \u0026lt;= expressionRewriter 从这里开始遍历语法树 er.Enter()  er.handleInSubquery() er.buildSubquery() er.constructBinaryOpFunction() er.b.buildSemiApply() \u0026lt;= 创建 Apply 算子     最终得到的查询计划大致如下：\n注意，图中的 Apply 是 SemiApply。\nLogicalApply 类型 TiDB 使用 LogicalApply 来表示 Apply 算子：\ntype LogicalApply struct { LogicalJoin corCols []*expression.CorrelatedColumn } 从数据结构上也能看出，LogicalApply 和 LogicalJoin 很像，Apply 类型其实也是通过 JoinType 类型设置的（比如，SemiJoin、LeftOuterJoin、InnerJoin 等）。\n去关联化 去关联化是在逻辑查询优化过程中完成的，代码逻辑主要看 decorrelateSolver 。\n优化的思路是：尽可能把 Apply 往下推、把 Apply 下面的算子向上提，通过这一方式将关联变量变成普通变量，从而去关联化。虽然这一过程可能看起来会让查询计划的效率降低，但是去关联化以后再通过谓词下推等优化规则可以重新对整个查询计划进行优化。\nQ1 涉及到的代码如下：\n// optimize implements logicalOptRule interface. func (s *decorrelateSolver) optimize(p LogicalPlan) (LogicalPlan, error) { if apply, ok := p.(*LogicalApply); ok { outerPlan := apply.children[0] innerPlan := apply.children[1] apply.extractCorColumnsBySchema() if len(apply.corCols) == 0 { // \u0026lt;= 如果关联变量都被消除，可以将 Apply 转换成 Join  join := \u0026amp;apply.LogicalJoin join.self = join p = join } else if sel, ok := innerPlan.(*LogicalSelection); ok { // \u0026lt;= 在这个分支中消除 Selection 节点  newConds := make([]expression.Expression, 0, len(sel.Conditions)) for _, cond := range sel.Conditions { newConds = append(newConds, cond.Decorrelate(outerPlan.Schema())) } apply.attachOnConds(newConds) innerPlan = sel.children[0] apply.SetChildren(outerPlan, innerPlan) return s.optimize(p) // Selection 被消除以后，重新对 Apply 优化，在 Q1 中会触发 Apply 被转换成 Join  } else if m, ok := innerPlan.(*LogicalMaxOneRow); ok { //...  } else if proj, ok := innerPlan.(*LogicalProjection); ok { //...  } else if agg, ok := innerPlan.(*LogicalAggregation); ok { //...  } } //...  return p, nil } 参考上面提供的查询计划示意图，代码执行过程中：\n 寻找 Apply 节点，找到 Apply 节点以后，尝试对 innerPlan 子节点中的算子往上提，在 Q1 中：  将 Selection(t1.a=t2.a) 节点中的条件提到 Apply 上，消除 Selection 节点 完成这一步以后，Apply 的关联变量就被消除了，这样就可以把 Apply 转换成一个普通的 Join 了，Q1 的去关联化过程也基本完成。   整个过程如下图所示：\nQ2 select t1.a from t t1 where 1000 \u0026lt; (select min(t2.b) from t t2 where t2.a = t1.a) Q2 相对 Q1，子查询稍微复杂了一点，多了聚合函数。\n重写阶段 重写阶段和 Q1 很类似，区别主要在于查询条件不再是 PatternInExpr，变成了 BinaryOperationExpr，重写逻辑主要发生在 handleScalarSubquery 当中：\nfunc (er *expressionRewriter) handleScalarSubquery(v *ast.SubqueryExpr) (ast.Node, bool) { np, err := er.buildSubquery(v) //...  np = er.b.buildMaxOneRow(np) if len(np.extractCorrelatedCols()) \u0026gt; 0 { er.p = er.b.buildApplyWithJoinType(er.p, np, LeftOuterJoin) //...  return v, true } //... } 另外一点不同是，Apply 类型变成了 LeftOuterJoin （如何选择 Apply 的类型，可以参考开头的几篇文章）。重写完以后得到的查询计划大致如下：\n去关联化 和 Q1 相比，由于有 Aggregation 节点，Q2 的去关联化逻辑更复杂一些。对于 Q2 这类带有 aggr 的查询，decorrelateSolver 尽可能将 Aggregation 向上拉：\nfunc (s *decorrelateSolver) optimize(p LogicalPlan) (LogicalPlan, error) { if apply, ok := p.(*LogicalApply); ok { outerPlan := apply.children[0] innerPlan := apply.children[1] apply.extractCorColumnsBySchema() if len(apply.corCols) == 0 { //...  } else if sel, ok := innerPlan.(*LogicalSelection); ok { //...  } else if m, ok := innerPlan.(*LogicalMaxOneRow); ok { //...  } else if proj, ok := innerPlan.(*LogicalProjection); ok { //...  } else if agg, ok := innerPlan.(*LogicalAggregation); ok { if apply.canPullUpAgg() \u0026amp;\u0026amp; agg.canPullUp() { // 尝试将 Aggregation 上提  //...  } // 如果 Aggregation 不能上提，尝试将 Aggregation 下面的 Selection 上提，去掉关联变量  if sel, ok := agg.children[0].(*LogicalSelection); ok \u0026amp;\u0026amp; apply.JoinType == LeftOuterJoin { var ( eqCondWithCorCol []*expression.ScalarFunction remainedExpr []expression.Expression ) // 解析 Selection 中的关联条件  for _, cond := range sel.Conditions { if expr := apply.deCorColFromEqExpr(cond); expr != nil { eqCondWithCorCol = append(eqCondWithCorCol, expr.(*expression.ScalarFunction)) } else { remainedExpr = append(remainedExpr, cond) } } if len(eqCondWithCorCol) \u0026gt; 0 { //...  if len(apply.corCols) == 0 { join := \u0026amp;apply.LogicalJoin join.EqualConditions = append(join.EqualConditions, eqCondWithCorCol...) for _, eqCond := range eqCondWithCorCol { // 对于被上提的筛选条件，如果 Aggregation 没有包含对应列的分组的话  // 需要在 Aggregation 中添加上分组  clonedCol := eqCond.GetArgs()[1].Clone() // If the join key is not in the aggregation\u0026#39;s schema, add first row function.  if agg.schema.ColumnIndex(eqCond.GetArgs()[1].(*expression.Column)) == -1 { newFunc := aggregation.NewAggFuncDesc(apply.ctx, ast.AggFuncFirstRow, []expression.Expression{clonedCol}, false) agg.AggFuncs = append(agg.AggFuncs, newFunc) agg.schema.Append(clonedCol.(*expression.Column)) } // If group by cols don\u0026#39;t contain the join key, add it into this.  if agg.getGbyColIndex(eqCond.GetArgs()[1].(*expression.Column)) == -1 { agg.GroupByItems = append(agg.GroupByItems, clonedCol) } } //...  agg.collectGroupByColumns() if len(sel.Conditions) == 0 { // \u0026lt;= Selection 的条件都被删除，那么节点可以被消除了  agg.SetChildren(sel.children[0]) } //...  return s.optimize(p) // Selection 被消除以后重新对 Apply 节点进行优化，触发 Apply 转换成 Join 的逻辑  } //...  } } } } //...  return p, nil } 可惜的是，Q2 中的 Aggregation 是无法 pull up 的，貌似 TiDB 并没有完全按照开头文章中提到的方式去做子查询优化。虽然 Aggregation 无法上提，但是 decorrelator 会尝试将子节点 Selection 中的条件合并到 Apply 中，这个过程和 Q1 很像。如果 Selection 中的条件都被合并到 Apply 当中，那么 Selection 节点可以被消除了。\n在 Q2 中 Selection 节点删除后，子查询不再包含关联变量，Apply 可以被转换为 Join。去关联以后得到的查询计划大致如下：\n原博文链接\n","title":"TiDB 源码学习：常见子查询优化"},{"location":"https://codenow.me/algorithm/leetcode-210-course-schedule-ii/","text":"原题链接：210. Course Schedule II 。一道基础拓扑排序题，代码如下：\nclass Solution: def findOrder(self, numCourses: \u0026#39;int\u0026#39;, prerequisites: \u0026#39;List[List[int]]\u0026#39;) -\u0026gt; \u0026#39;List[int]\u0026#39;: degrees = [0] * numCourses graph = [[] for _ in range(numCourses)] for edge in prerequisites: source, dep = edge degrees[source] += 1 graph[dep].append(source) stack = [] for i in range(numCourses): deps = degrees[i] if deps == 0: stack.append(i) ret = [] while len(stack) \u0026gt; 0: node = stack.pop() ret.append(node) deps = graph[node] for dep in deps: degrees[dep] -= 1 if degrees[dep] == 0: stack.append(dep) if len(ret) != numCourses: return [] else: return ret","title":"Leetcode: 210 Course Schedule II"},{"location":"https://codenow.me/translation/intro_to_python_lambda_functions/","text":"原文地址: https://www.pythonforthelab.com/blog/intro-to-python-lambda-functions/\n不久前，Python在其语法中引入了使用lambda而不是def来定义函数的可能性。这些函数称为匿名函数同，在其它语言(如javascript)中非常常见。然后，在Python中，它们看起来有点晦涩，经常被忽略或误用。在本文中，我们将介绍labda函数，并讨论在何处以及如何使用它。\n要定义一个函数，可以使用以下语法：\ndef average(x, y): return (x + y) / 2 然后，如果要计算两个数字的平均值，只需执行以下操作\navg = average(2, 5) 在这种情况下，平均值将为3.5。我们也可以这样定义平均值：\naverage = lambda x, y: (x + y) / 2 如果你测试此函数，您将看到输出完全一样。必须指出，def和lambda之间语法非常不同。首先，我们定义不带括号的参数x, y。然后，我们定义要应用的操作。注意，当使用lambda函数时，返回是隐式的。\n然而，还有更根本的区别。lambda函数只能在一行上表示，并且没有docstring。如果对上面的每个定义尝试help(average)，您将看到输出非常不同，此外，无法记录average的第二版的实际操作。\n从功能上讲，定义平均值的两种方法都给出了相同的结果。到目前为止，他们之间的差异非常微妙。lambda（或匿名）函数的主要优点是它们不需要名称。此外，像我们上面所做的那样指定一个名字被认为是不好的做法，我们稍后将讨论。现在让我们看看您希望在什么上下文中使用lambda函数而不是普通函数。\n大多数教程都侧重于lambda函数来对列表进行排序。在讨论其他主题之前，我们也可以这样做。假设您有以下列表：\nvar=[1，5，-2，3，-7，4] 假设您希望对值进行排序，可以执行以下操作：\nsorted_var = sorted(var) #[-7，-2，1，3，4，5] 这很容易。但是，如果您希望根据到给定数字的距离对值进行排序，会发生什么情况呢？如果要计算到1的距离，需要对每个数字应用一个函数，例如abs（x-1），并根据输出对值进行排序。幸运的是，排序后，您可以使用关键字参数key=执行此操作。我们可以做到：\ndef distance(x): return abs(x - 1) sorted_var = sorted(var, key=distance) # [1, 3, -2, 4, 5, -7] 另一种选择是使用lambda函数：\nsorted_var = sorted(var, key=lambda x: abs(x-1)) 这两个例子将产生完全相同的输出。在使用def或lambda定义函数之间没有功能差异。我可以说第二个例子比第一个稍微短一些。此外，它使代码更具可读性，因为您可以立即看到对每个元素（abs（x-1））所做的操作，而不是通过代码挖掘来查看定义的距离。\n另一种可能是与map结合使用。map是将函数应用于列表中的每个元素的一种方法。例如，基于上面的示例，我们可以执行以下操作：\nlist(map(distance, var)) # [0, 4, 3, 2, 8, 3] 或者，使用lambda表达式\nlist(map(lambda x: abs(x-1), var)) # [0, 4, 3, 2, 8, 3] 它给出了完全相同的输出，同样，人们可以争论哪一个更容易阅读。上面的示例是您在其他教程中可能看到的。如果通过stackoverflow，可能会看到。其中一种可能是结合Pandas使用lambda函数。\npandas和lambda函数\n示例数据受此示例的启发，可以在此处找到。创建包含以下内容的文件示例example_data.csv：\nanimal,uniq_id,water_need elephant,1001,500 elephant,1002,600 elephant,1003,550 tiger,1004,300 tiger,1005,320 tiger,1006,330 tiger,1007,290 tiger,1008,310 zebra,1009,200 zebra,1010,220 zebra,1011,240 zebra,1012,230 zebra,1013,220 zebra,1014,100 zebra,1015,80 lion,1016,420 lion,1017,600 lion,1018,500 lion,1019,390 kangaroo,1020,410 kangaroo,1021,430 kangaroo,1022,410  要将数据读取为数据帧，我们只需执行以下操作：\nimport pandas as pd df = pd.read_csv(\u0026#39;example_data.csv\u0026#39;, delimiter = \u0026#39;,\u0026#39;)  假设您希望将数据框中每个动物名称的第一个字母大写，您可以执行以下操作：\ndf[\u0026#39;animal\u0026#39;]=df[\u0026#39;animal\u0026#39;].apply((lambda x:x.capitalize()) print(df.head()) 你会看到结果。当然，lambda函数可能变得更加复杂。您可以将它们应用于整个系列，而不是单个值，您可以将它们与其他库（如numpy或scipy）组合，并对数据执行复杂的转换。\nlambda函数最大的优点之一是，如果您使用的是Jupyter notebools，那么您可以立即看到这些变化。你不需要打开另一个文件，运行一个不同的，单元格等。如果你去Pandas的文档，你会看到，lambdas经常被使用。\nQt Slots\n使用lambdas的另一个常见示例是与qt库结合使用。我们过去写过一篇关于qt的介绍性文章。如果您不熟悉构建用户界面的工作方式，可以随意浏览它。一个非常简单的例子，只显示一个按钮，它看起来像这样：\nfrom PyQt5.QtWidgets import QApplication, QPushButton app = QApplication([]) button = QPushButton(\u0026#39;Press Me\u0026#39;) button.show() app.exit(app.exec()) 如果要在按下按钮时触发某个操作，则必须将该操作定义为一个函数。如果我们想在按下按钮时将某些内容打印到屏幕上，我们只需在app.exit之前添加以下行：\nbutton.clicked.connect(lambda x: print(\u0026#39;Pressed!\u0026#39;)) 如果您再次运行程序，每次按下按钮，您都会看到已按下！出现在屏幕上。同样，使用lambda函数作为信号的插槽可以加快编码速度，使程序更容易阅读。但是，lambda函数也需要谨慎考虑。\nlambda函数的使用位置\nlambda函数只能有一行。这迫使开发人员只能在没有复杂语法的情况下使用它们。在上面的示例中，您可以看到lambda函数非常简单。如果它需要打开一个套接字，交换一些信息，处理接收到的数据等，那么它可能不可能在一条线上完成。\n可以使用lambda函数的自然情况是作为其他需要可调用参数的函数的参数。例如，应用pandas数据帧需要一个函数作为参数。连接qt中的信号还需要一个函数。如果我们要应用或执行的函数很简单，并且我们不打算重复使用它，那么将其编写为匿名函数可能是一种非常方便的方法。\n不使用lambda函数的位置\nlambda函数是匿名的，因此，如果您要为其分配名称，例如在执行以下操作时：\naverage = lambda x，y:（x+y）/2 这意味着你做错了什么。如果需要为函数指定一个名称，以便在程序的不同位置使用它，请使用标准的def语法。在这个博客中有一个关于Python中lambda函数滥用的冗长讨论。我经常看到的，尤其是刚学过lambdas的人，是这样的：\nsorted_var = sorted(var, key=lambda x: abs(x)) 如果这是第一次看到lambda函数，那么这个无辜的示例可能很难包装起来。但是，您所拥有的是将一个函数（abs）包装在另一个函数中。它应该是这样的：\ndef func(x): return abs(x) 与仅仅做abs（x）相比有什么优势？实际上，没有优势，这意味着我们也可以这样\nsorted_var = sorted(var, key=abs) 如果您注意我们前面开发的示例，我们使用abs（x-1）来避免这种冗余。\n结论\nlambda（或匿名）函数是一种在Python程序中逐渐流行的工具。这就是为什么你能理解它的含义是非常重要的。您必须记住，lambda语法不允许您这样做，没有它们是不可能做到的。更重要的是它的方便性、语法经济性和可读性。\n在其他编程语言（如javascript）中，匿名函数的使用频率非常高，并且具有比Python更丰富的语法。我不相信Python也会这样做，但无论如何，它们是一种工具，不仅可以帮助您使用当前的程序，而且还可以帮助您了解如果您修补其他语言的话会发生什么。\n","title":"Intro to Python Lambda Functions"},{"location":"https://codenow.me/tips/python_code_static_analysis_tool_summary/","text":" 1.Pylint  Pylint是Python代码的一个静态检查工具，它能够检测一系列的代码错误，代码坏味道和格式错误。\nPylint使用的编码格式类似于PEP-8。\n它的最新版本还提供代码复杂度的相关统计数据，并能打印相应报告。\n不过在检查之前，Pylint需要先执行代码。\n具体可以参考http://pylint.org\n 2. Pyflakes  Pyflakes相对于Pylint而言出现的时间较晚，不同于Pylint的是，它不需要在检查之前执行代码来获取代码中的错误。\nPyflakes不检查代码的格式错误，只检查逻辑错误。\n具体可以参考http://launchpad.net/pyflakes\n 3. McCabe  McCabe是一个脚本，根据McCabe指标检查代码复杂性并打印报告。\n具体可以参考https://pypi.org/project/mccabe/\n 4. Pycodestyle  Pycodestyle是一个按照PEP-8的部分内容检查Python代码的一个工具\n这个工具之前叫PEP-8。\n具体可以参考https://github.com/pycqa/pycodestyle\n 5. Flake8  Flake8封装了Pyflakes、McCabe和Pycodestyle工具，它可以执行这三个工具提供的检查\n具体可以参考https://github.com/pycqa/flake8\n 6. Pychecker  PyChecker是Python代码的静态分析工具，它能够帮助查找Python代码的bug，而且能够对代码的复杂度和格式等提出警告。\nPyChecker会导入所检查文件中包含的模块，检查导入是否正确，同时检查文件中的函数、类和方法等。\n具体可以参考https://pypi.org/project/PyChecker/\n 7. Black  Black 号称是不妥协的 Python 代码格式化工具。之所以成为“不妥协”是因为它检测到不符合规范的代码风格直接就帮你全部格式化好，根本不需要你确定，直接替你做好决定。而作为回报，Black 提供了快速的速度。\nBlack 通过产生最小的差异来更快地进行代码审查。\nBlack 的使用非常简单，安装成功后，和其他系统命令一样使用，只需在 black 命令后面指定需要格式化的文件或者目录即可。\n具体可以参考https://atom.io/packages/python-black\n ","title":"Python_code_static_analysis_tool_summary"},{"location":"https://codenow.me/articles/spark-broadcast_accumulator/","text":" 累加器 累加器提供将工作节点的值聚合到驱动器程序中的功能，且实现语法简单。\n示例图：\n#python中累加空行 file = sc.textFile(inputfile) blankLines = sc.accumulator(0) # 创建Accumulator(Int) def extractCallSigns(line): global blankLines if line == \u0026#34;\u0026#34;: blankLines += 1 return line.split(\u0026#39; \u0026#39;) callSigns = file.flatMap(extractCallSigns) callSigns.saveAsTextFile(outputPath) print(\u0026#39;blank Lines : %d\u0026#39; %blankLines.value) 实际使用中可以创建多个累加器进行计数\nvalidSignCount = sc.Accumulator(0) invalidSignCount = sc.Accumulator(0) 广播变量 简介 正常情况中，spark的task会在执行任务时，将变量进行拷贝。当每个task都从主节点拷贝时，程序的通信和内存负担很重。 使用广播变量后，主节点会将变量拷贝至工作节点，任务从工作节点获得变量，而不用再次拷贝，此时变量被拷贝的次数取决于工作节点的个数。\n#在Python中使用广播变量 signPrefixes = sc.broadcast(loadCallSignTable()) def processSignCount(sign_count, signPrefixes): country = lookupCountry(sign_count[0], signPrefixes.value) count = sign_count[1] return (country, count) countryContactCounts = (contactCounts.map(processSignCount).reduceByKey((lambda x, y:x+y))) countryContactCounts.saveAsTextFile(ooutputPath) 基于分区进行操作 基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。 Spark提供基于分区的map和foreach。\nPython基于分区操作 def func(file): pass def fetchCallSigns(input): return input.mapPartitions(lambda x: func(file=x)) #使用mapPartitons执行对分区的操作 do_func = fetchCallSigns(inputfile) 常见分区操作函数\n   函数名 调用所提供的 返回的 对于RDD[T]的函数签名     mapPartitions() 该分区中元素的迭代器 返回的元素的迭代器 f:(Iter ator[T]——\u0026gt;Iterator[U])   mapPartitionsWithIndex() 分区序号，以及每个分区中的元素的迭代器 返回的元素的迭代器 f:(Int, Iterator[T]——\u0026gt;Iterator[U])   forceachPartitions() 元素迭代器 无 f:(Iterator[T]——\u0026gt;Unit)    数值RDD的操作 简介 Spark对包含数据的RDD提供了一些描述性的操作。 Spark的数据操作是通过流式算法实现的，允许以每次一个元素的方式构建模型。这些统计数据会在调用stats()时通过一次遍历计算出来，并以StatsCounter对象返回。\n数值操作 StatsCounter中可用的汇总统计数据\n   方法 含义     count() RDD中的元素的个数   mean() 元素的平均值   sum() 总和   max() 最大值   min() 最小值   variance() 元素的方差   sampleVariance() 从采样中计算出的方差   stdev() 标准差   sampleStdev() 采用的标准差    Python中使用数据操作\ndistanceNumerics = distances.map(lambda s: float(s)) stats = distanceNumerics.stats() stdev = stats.stdev() #计算标准差 mean = stats.mean() resonableDistances = distanceNumerics.filter(lambda x: math.fabs(x - mean)\u0026lt;3 * stdev) print(resonableDistances.collet())","title":"Spark累加器和广播变量"},{"location":"https://codenow.me/translation/scikit-learn/","text":" Scikit-learn: Machine Learning in Python 最近在学习机器学习算法和深度学习的部分内容。于是将Scikit-Learn的相关介绍论文看了看，翻译了一部分。原文地址\n摘要 Scikit-learn是一个Python模块，集成了各种最先进的机器学习算法，适用于中等规模和无监督的问题。该软件包侧重于使用通用高级语言将机器学习引入非专业人员。重点在于易于使用，性能，文档以及API的一致性。它具有最小的依赖性，并在简化的BSD许可下分发，鼓励在学术和商业环境中使用它。二进制文件和文档可以从http://scikit-learn.sourceforge.net 下载。\n介绍 Python编程语言正在成为最流行的科学计算语言之一。由于其高水平的交互性和成熟的科学库生态系统，Python在算法开发和探索数据分析领域成为极有吸引力的选择。然而，作为一种通用语言，它不仅越来越多的应用于学术领域，也应用于工业。Scikit-learn利用这种环境提供许多出名的机器学习算法的最先进的实现方式，同时保持易于使用的界面以及和Python语言紧密集成。这满足了软件和网络行业的非专业人员以及计算机科学以外领域（如生物学或物理学）对统计数据分析的日益增长的需求。\nScikit-learn不同于其他Python的机器学习库的原因在于：\n 它根据BSD许可证分发。 与DMP和pybrain不同，它结合了编译代码来提升效率。 它仅仅依赖于Numpy和Scipy来促进易于分发。不像pymvpa那样拥有例如R和shogun这样的可选依赖项。 与使用数据流框架的pybrain不同，它侧重于命令式编程。虽然该软件包主要是用Python编写的，但它包含了C ++库LibSVM和LibLinear，它们提供了SVMS的参考实现和具有兼容许可的广义线性模型。二进制包可在包括Windows和任何POSIX平台在内的丰富平台上使用。此外，由于其自由许可，它已被广泛分发为主要的免费软件发行版，如Ubuntu，Debian，Mandriva，NetBSD和商业广告诸如“Enthought Python Distributions”之类的发行版。  项目愿景 代码质量。该项目的目标不是提供尽可能多的功能，而是提供可靠的实施能力。通过单元测试来保证代码质量。在发布的0.8版本，测试覆盖率为81%，与此同时，使用静态分析工具例如pyflakes和PEP8.最后，我们严格遵守Python编程指南和Numpy样式文档中使用的函数与参数命名，努力保持一致性。\nBSD许可。大多数Python生态系统都是用非copyleft许可证进行许可。虽然这种政策有利于商业项目采用这些工具，但它确实施加了一些限制：我们无法使用某些现有的科学代码，例如GSL。\n裸骨设计和API。 为了降低进入门槛，我们避免使用框架代码并将不同对象的数量保持在最低限度，依赖于数据容器的numpy数组。\n社区驱动的发展。 我们的开发基于git，GitHub和公共邮件列表等协作工具。 欢迎并鼓励外部捐助。\n开发者文档。Scikit-learn提供了约300页的用户指南，包括叙述文档，类参考，教程，安装说明，以及60多个示例，其中一些包含实际应用程序。 我们尽量减少机器学习术语的使用，同时主要训练精度与所使用的算法有关。\n基础技术 Numpy：数据和模型参数的基础数据结构用户。 输入数据表示为numpy数组，因此可以与其他科学Python库无缝集成。 Numpy的基于视图的内存模型限制了副本，即使与编译代码绑定也是如此。它还提供基本的算术运算。\nScipy：线性代数的有效算法，稀疏矩阵表示，特殊函数和基本统计函数。 Scipy具有许多基于Fortran的标准数字包的绑定，例如LAPACK。 这对于易于安装和可移植性非常重要，因为围绕Fortran代码提供库在各种平台上都具有挑战性。\nCython：一种在Python中组合C的语言。 Cython使用类似Python的语法和高级操作轻松实现编译语言的性能。 它还用于绑定已编译的库，从而消除了Python / C扩展的样板代码。\n总结 Scikit-learn使用一致的，面向任务的界面，公开了各种机器学习算法，包括监督和非监督，从而可以轻松地比较给定应用程序的方法。 由于它依赖于科学的Python生态系统，因此可以轻松地将其集成到传统统计数据分析范围之外的应用程序中。 重要的是，以高级语言实现的算法可以用作特定于用例的方法的构建块，例如，在医学成像中。 未来的工作包括在线学习，扩展到大型数据集。\n","title":"Scikit-Learn"},{"location":"https://codenow.me/algorithm/decision_tree/","text":" 决策树 学习并构建决策树。\n决策树的一个重要任务是为了数据中心所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则。在这些机器根据数据创建规则时，就是机器学习的过程。专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。\n决策树示例：\n决策树函数组成部分    优缺点 说明     优点 计算复杂度不高、输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据   缺点 可能会产生过度匹配问题   适用数据类型 数值型和标称型     寻找最佳划分特征值\n构造决策树时，需要考虑的第一个问题：当前数据集中哪个特征在划分数据分类时起到决定作用。\n为了找到这个特征，需要评估每一个特征，完成评测后，原始数据就会被划分为几个数据子集。然后遍历每个数据子集，若是都为同类，则该数据集结束分类，否则在该数据集中重新执行评估，二次分类。依次执行，直到数据被划分完毕或特征使用完毕时停止。\n创建分支的伪代码函数createBranch如下图所示：\n检测数据集中的每个子项是否属于同一分类： if so return 类标签： else: 寻找划分数据集的最好特征 划分数据集 创建分支节点 for每个划分的子集 调用函数createBranch并增加返回结果到分支节点中 return 分支节点  信息增益\n划分数据集最大的原则是：将无序的数据变得更加有序。本章选取信息论度量信息。\n在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最好的特征就是最好的选择。\n1).计算给定数据集的香农熵 计算熵的公式： $$ H = -\\sum{i=1}^{n}P(x{i})log{2}^{P(x{i})} $$\nfrom math import log def calcShannonEnt(dataSet): numEntries = len(dataSet) # 获取数据集中实例总数 labelCounts = {} # 创建数据字典，键值为数据集最后一列的值，即标签  for featVec in dataSet: currentLabel = featVec[-1] # 获取标签 if currentLabel not in labelCounts.keys(): # 如果标签不在字典中，则将其添加进去 labelCounts[currentLabel] =0 labelCounts[currentLabel] += 1 # 如果标签存在，则对应的数值加1 shannonEnt = 0.0 for LabelKey in labelCounts: prob = float(labelCounts[LabelKey]) / numEntries # 计算数值进入该分类的概率 shannonEnt -= prob * log(prob, 2) # 计算熵 return shannonEnt 示例数据：\n    不浮出水面 是否有脚蹼 属于鱼类     1 是 是 是   2 是 是 是   3 是 否 否   4 否 是 否   5 否 是 否    对应数据集：\ndataSet = [ [1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no'] ]  划分数据集 划分数据集，度量划分数据集的熵，以便判断当前是否正确划分了数据集。 通过对每个特征划分数据集的结果度量熵，判断哪个特征划分数据集是最好的划分方式。\ndef splitDateSet(dataSet, axis, value): # dataSet：待划分的数据集；axis：划分数据的特征索引；value：对应划分的特征值 retDateSet = [] for featVec in dataSet: if featVec[axis] == value: reduceFeatVec = featVec[:axis] reduceFeatVec.extend(featVec[axis+1:]) retDataSet.append(reduceFeatVec) return retDateSet 选择最好的数据集划分方式 通过该函数选取特征，划分数据集，计算出最好的划分数据集的特征。\ndef chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) -1 bestEntropy = calcShannonEnt(dataSet) # 计算原始香农熵 bestInfoGain = 0.0 # 信息增益 bestFeature = -1 # 原始特征值索引 for i in range(numFeatures): featList = [example[i] for example in dataSet] #创建第i个特征值组成的列表 uniqueVals = set(featList ) #创建特征值集合，去除重复元素 newEntropy = 0.0 for value in uniqueVals: # 依次读取特征值，计算香农熵 subDataSet = splitDataSet(dataSet, i, value) prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy #计算信息增益 if infoGain \u0026gt; bestInfoGain: # 如果信息增益变大，则代表该特征值更好 bestInfoGain = infoGain bestFeature = i return bestFeature # 返回最佳特征值的索引 递归构造决策树 递归构造决策树原理：根据数据集选择最好的特征划分数据集，由于特征可能多于两个，故分支节点可能有多个。第一次划分后，子数据集中，可能还需要进行划分，故需要在子数据集中，递归调用决策树进行分类。\n 构建叶子节点分类函数 在本章构建决策树时，选择最佳特征值后，会删除特征。假设所有的特征使用完毕后，在某些叶子结点中，并不是都是一类，此时需要使用多数表决来分类。 下述函数将对叶子结点中所有的数据进行分类统计，最后选出数量最多的类别并返回其标签作为叶子结点分类标签。\ndef majorituCnt(classList): classCount = {} for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCountp[0][0] 构建决策树\ndef createTree(data, labels): classList = [example[i] for example in dataSet] if classList.count(classList[0]) == len(classList): # 当类别完全相同时停止继续划分 return classList[0] if len(dataSet[0]) == 1: # 遍历完所有特征时返回出现次数最多 return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 选取最佳划分特征 bestFeatLabel = labels[bestFeat] myTree = {bestFeatLabel:{}} # 构建节点 del(labels[bestFeat]) featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] # 将类标签复制，防止使用过程中类标签被改变。 myTree[bestFFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # 递归调用函数 return myTree  使用Matplotlib绘制决策树 import matplotlib.pyplot as plt from pylab import * mpl.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] decisionNode = dict(boxstyle=\u0026#39;sawtooth\u0026#39;, fc=\u0026#39;0.8\u0026#39;) leafNode = dict(boxstyle=\u0026#39;round4\u0026#39;, fc=\u0026#39;0.8\u0026#39;) arrow_args = dict(arrowstyle=\u0026#39;\u0026lt;-\u0026#39;) def getNumLeafs(MyTree): # 获取叶子节点数 NumLeafs = 0 firstStr = list(MyTree.keys())[0] secondDict = MyTree[firstStr] for TreeKey in secondDict.keys(): if isinstance(secondDict[TreeKey], dict): NumLeafs += getNumLeafs(secondDict[TreeKey]) else: NumLeafs += 1 return NumLeafs def getTreeDepth(MyTree): # 获取树深度 maxDepth = 0 thisDepth = 0 firstStr = list(MyTree.keys())[0] secondDict = MyTree[firstStr] for TreeKey in secondDict.keys(): if isinstance(secondDict[TreeKey], dict): thisDepth = 1 + getTreeDepth(secondDict[TreeKey]) else: thisDepth += 1 if thisDepth \u0026gt; maxDepth: maxDepth = thisDepth return maxDepth def plotMidText(cntrPt, parentPt, txtString): xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1] createPlot.axl.text(xMid, yMid, txtString) def plotTree(MyTree, parentPt, nodeTxt): numLeafs = getNumLeafs(MyTree) depth = getTreeDepth(MyTree) firstStr = list(MyTree.keys())[0] cntrPt = (plotTree.x0ff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.y0ff) plotMidText(cntrPt, parentPt, nodeTxt) plotNode(firstStr, cntrPt, parentPt, decisionNode) secondDict = MyTree[firstStr] plotTree.y0ff = plotTree.y0ff - 1.0/plotTree.totalD for TreeKey in secondDict.keys(): if isinstance(secondDict[TreeKey], dict): plotTree(secondDict[TreeKey], cntrPt, str(TreeKey)) else: plotTree.x0ff = plotTree.x0ff + 1.0/plotTree.totalW plotNode(secondDict[TreeKey], (plotTree.x0ff, plotTree.y0ff), cntrPt, leafNode) plotMidText((plotTree.x0ff, plotTree.y0ff), cntrPt, str(TreeKey)) plotTree.y0ff = plotTree.y0ff + 1.0/plotTree.totalD def plotNode(nodeTxt, centerPt, parentPt, nodeType): createPlot.axl.annotate(nodeTxt, xy=parentPt, xycoords=\u0026#39;axes fraction\u0026#39;, xytext=centerPt, textcoords=\u0026#39;axes fraction\u0026#39;, va=\u0026#39;center\u0026#39;, ha=\u0026#39;center\u0026#39;, bbox=nodeType, arrowprops=arrow_args) def createPlot(inTree): fig = plt.figure(1, facecolor=\u0026#39;white\u0026#39;) fig.clf() anprops = dict(xticks=[], yticks=[]) createPlot.axl = plt.subplot(111, frameon=False, **anprops) plotTree.totalW = float(getNumLeafs(inTree)) plotTree.totalD = float(getTreeDepth(inTree)) plotTree.x0ff = -0.5/plotTree.totalW plotTree.y0ff = 1.0 plotTree(inTree, (0.5, 1.0), \u0026#39;\u0026#39;) plt.show()","title":"DecisionTree"},{"location":"https://codenow.me/tips/differen_rsa/","text":"在同一个电脑上为不同的GitHub账号创建rsa并实现关联。 操作命令行。\n#为第一个账号创建rsa文件 ssh-keygen -t rsa -C \u0026#34;your email\u0026#34; -f ~/.ssh/id_rsa_for_account1 #为第二个账号创建rsa文件 ssh-keygen -t rsa -C \u0026#34;your email\u0026#34; -f ~/.ssh/id_rsa_for_account2 在.ssh文件夹下创建config文件 输入如下内容：\n#Default GitHub Host account1 HostName github.com User git IdentityFile ~/.ssh/id_rsa_for_account1 Host account2 HostName github.com User git IdentityFile ~/.ssh/id_rsa_for_account2  然后分别在account1和account2GitHub中添加公钥。 验证\nssh -T account1 ssh -T account2  ","title":"Different rsa for different github account in the same computer"},{"location":"https://codenow.me/algorithm/leetcode_905_sort_array_by_parity/","text":" 题号：905\n难度：Easy\n链接：https://leetcode.com/problems/sort-array-by-parity/\n 如下是 python3 代码:\n#!/usr/bin/python class Solution: def sortArrayByParity(self, A: \u0026#39;List[int]\u0026#39;) -\u0026gt; \u0026#39;List[int]\u0026#39;: lens = len(A) store_list = [None] * lens head = 0 tail = lens - 1 for i in range(lens): if A[i] % 2 == 0: store_list[head] = A[i] head += 1 else: store_list[tail] = A[i] tail -= 1 return store_list if __name__ == \u0026#39;__main__\u0026#39;: test_list = [3, 1, 2, 4] print(Solution().sortArrayByParity(test_list))","title":"Leetcode：905 Sort Array ByParity"},{"location":"https://codenow.me/articles/how-to-use-hugo/","text":" 一、介绍 1. 优点   Hugo是一个用Go语言编写的静态网站生成器，它使用起来非常简单，相对于Jekyll复杂的安装设置来说，Hugo仅需要一个二进制文件hugo(hugo.exe)即可轻松用于本地调试和生成静态页面。 Hugo生成静态页面的效率很高，几乎是瞬间完成的，而之前用Jekyll需要等待。 Hugo自带watch的调试模式，可以在我修改MarkDown文章之后切换到浏览器，页面会检测到更新并且自动刷新，呈现出最终效果，能极大的提高博客书写效率。 再加上Hugo是使用Go语言编写，已经没有任何理由不使用Hugo来代替Jekyll作为我的个人博客站点生成器了。   2. 静态网站文件的两种方式：   放到自己的服务器上提供服务：需要自己购买服务器 把网站托管到 GitHub Pages：需要将静态页面文件 push 到 GitHub 的博客项目的 gh-pages 分支并确保根目录下有 index.html 文件。   3. 官网   Hugo语言官方中文文档地址：http://www.gohugo.org/ Hugo官方主页：https://gohugo.io/   二、安装Hugo 1. 二进制安装（推荐：简单、快速） 到 Hugo Releases (https://github.com/gohugoio/hugo/releases)下载对应的操作系统版本的Hugo二进制文件（hugo或者hugo.exe）\n 下载解压后添加到 Windows 的系统环境变量的 PATH 中即可，不需安装。 可以直接放在C:\\Users\\chunt\\go\\bin下，这样就不需要添加系统环境变量  Mac下直接使用 Homebrew 安装：\n brew install hugo 二进制在 $GOPATH/bin/, 即C:\\Users\\chunt\\go\\bin  2. 源码安装(不好用，go get有些下载不下来) 源码编译安装，首先安装好依赖的工具：\n Git Go 1.3+ (Go 1.4+ on Windows)  设置好 GOPATH 环境变量，获取源码并编译：\n export GOPATH=$HOME/go go get -v github.com/spf13/hugo  源码会下载到 $GOPATH/src 目录, 即C:\\Go\\src\n如果需要更新所有Hugo的依赖库，增加 -u 参数：\n go get -u -v github.com/spf13/hugo   The -u flag instructs get to use the network to update the named packages and their dependencies. By default, get uses the network to check out missing packages but does not use it to look for updates to existing packages.\nThe -v flag enables verbose progress and debug output.\n 3. 查看安装结果 可知hugo已经正常安装: 三、创建hugo项目 使用Hugo快速生成站点，比如希望生成到 /path/to/site | C:\\code\\hugo路径：\n linux: $ hugo new site /path/to/site windows: hugo new site C:\\code\\hugo  这样就在 /path/to/site | C:\\code\\hugo目录里生成了初始站点，进去目录：\n cd /path/to/site cd C:\\code\\hugo  站点目录结构：\n ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml  config.toml是网站的配置文件，这是一个TOML文件，全称是Tom’s Obvious, Minimal Language， 这是它的作者GitHub联合创始人Tom Preston-Werner 觉得YAML不够优雅，捣鼓出来的一个新格式。 如果你不喜欢这种格式，你可以将config.toml替换为YAML格式的config.yaml，或者json格式的config.json。hugo都支持。\n content目录里放的是你写的markdown文章，layouts目录里放的是网站的模板文件，static目录里放的是一些图片、css、js等资源。\n 四、创建文章 1. 创建一个 about 页面： 进入到C:\\code\\hugo\n $ hugo new about.md  about.md 自动生成到了 content/about.md ，打开 about.md 看下：\n内容是 Markdown 格式的，+++ 之间的内容是 TOML 格式的，根据你的喜好，你可以换成 YAML 格式（使用 \u0026mdash; 标记）或者 JSON 格式。\n2. 创建第一篇文章，放到 post 目录，方便之后生成聚合页面。 $ hugo new post/first.md\n打开编辑 post/first.md ：\n五、安装皮肤 去 themes.gohugo.io 选择喜欢的主题，下载到 themes 目录中，配置可见theme说明\n1. 下载方法一 在 themes 目录里把皮肤 git clone 下来： $ pwd /c/code/hugo $ mkdir themes # 创建 themes 目录 $ cd themes $ git clone https://github.com/digitalcraftsman/hugo-material-docs.git  2. 下载方法二 也可以添加到git的submodule中，优点是后面讲到用 travis 自动部署时比较方便。 如果需要对主题做更改，最好fork主题再做改动。 git submodule add https://github.com/digitalcraftsman/hugo-material-docs.git themes/hugo-material-docs  3. 使用皮肤 将\\blog\\themes\\hugo-fabric\\exampleSite\\config.toml 替换 \\blog\\config.toml 注：config.toml文件是核心，对网站的配置多数需要修改该文件，而每个主题的配置又不完全一样。\n4. 修改皮肤 如果需要调整更改主题，需要在 themes/hugo-material-docs 目录下重新 build cd themes/hugo-material-docs \u0026amp;\u0026amp; npm i \u0026amp;\u0026amp; npm start 生成主题资源文件（hugo-fabric为主题名） D:\\git\\blog\u0026gt;hugo -t hugo-fabric Started building sites ... Built site for language en: 0 of 3 drafts rendered 0 future content 0 expired content 8 regular pages created 12 other pages created 0 non-page files copied 2 paginator pages created 1 tags created 1 categories created total in 35 ms 将\\blog\\themes\\hugo-fabric\\exampleSite\\config.toml 替换 \\blog\\config.toml  5. 修改配置文件 根据个人实际情况，修改config.toml  五、启动 hugo 自带的服务器 1. 在你的站点根目录执行 Hugo 命令进行调试：  回到hugo站点目录C:\\code\\hugo $ hugo server \u0026ndash;theme=hugo-material-docs \u0026ndash;buildDrafts  注明：v0.15 版本之后，不再需要使用 \u0026ndash;watch 参数了 浏览器里打开： http://localhost:1313\n2. 在项目根目录下，通过 hugo server 命令可以使用hugo内置服务器调试预览博客。 --theme 选项可以指定主题。也可用-t --watch 选项可以在修改文件后自动刷新浏览器。也可用-w --buildDrafts 包括标记为草稿（draft）的内容。也可以用-D  六、 部署到github 1. 新建仓库 假设你需要部署在GitHub Pages上，首先在GitHub上创建一个Repository， 命名为：hanchuntao.github.io （hanchuntao替换为你的github用户名）。  注意 baseUrl要在仓库setting里面查看，有可能跟仓库名不一样。 例如：https://SYSUcarey.github.io/变成了https://sysucarey.github.io/\n2. 在项目根目录执行Hugo命令生成HTML静态页面 $ hugo --theme=hugo-material-docs --baseUrl=\u0026quot;https://hanchuntao.github.io/\u0026quot;  \u0026ndash;theme 选项指定主题， \u0026ndash;baseUrl 指定了项目的网站\n注意 以上命令并不会生成草稿页面，如果未生成任何文章，请去掉文章头部的 draft=true 再重新生成。 文件默认内容在，draft 表示是否是草稿，编辑完成后请将其改为 false，否则编译会跳过草稿文件。\n3. 查看生成的页面 如果一切顺利，所有静态页面都会生成到public目录\n4. 将pubilc目录里所有文件push到刚创建的Repository的master分支。 $ cd public $ git init $ git remote add origin https://github.com/hanchuntao/hanchuntao.github.io.git $ git add -A $ git commit -m \u0026quot;first commit\u0026quot; $ git push -u origin master   浏览器里访问：https://hanchuntao.github.io/  七、错误处理 1. Unable to locate Config file 启动 hugo 内置服务器时，会在当前目录执行的目录中寻找项目的配置文件。所以，需要在项目根目录中执行这个命令，否则报错如下： C:\\Users\\kika\\kikakika\\themes\u0026gt;hugo server --theme=hugo-bootstrap --buildDrafts --watch Error: Unable to locate Config file. Perhaps you need to create a new site. Run `hugo help new` for details. (Config File \u0026quot;config\u0026quot; Not Found in \u0026quot;[C:\\\\Users\\\\kika\\\\kikakika\\\\themes]\u0026quot;)  2. Unable to find theme Directory hugo 默认在项目中的 themes 目录中寻找指定的主题。所有下载的主题都要放在这个目录中才能使用，否则报错如下： C:\\Users\\kika\\kikakika\u0026gt;hugo server --theme=hugo-bootstrap --buildDrafts --watch Error: Unable to find theme Directory: C:\\Users\\kika\\kikakika\\themes\\hugo-bootstrap  3. 生成的网站没有文章 生成静态网站时，hugo 会忽略所有通过 draft: true 标记为草稿的文件。必须改为 draft: false 才会编译进 HTML 文件。  4. 默认的ServerSide的代码着色会有问题，有些字的颜色会和背景色一样导致看不见。 解决方法：使用ClientSide的代码着色方案即可解决。（见：Client-side Syntax Highlighting）  5. URL全部被转成了小写，如果是旧博客迁移过来，将是无法接受的。 解决方法：我是直接改了Hugo的代码，将URL强制转换为小写那段逻辑去掉了，之后考虑在config里提供配置开关，然后给Hugo提一个PR。如果是Windows用户可以直接https://github.com/coderzh/ConvertToHugo 下载到我修改后的版本myhugo.exe。 Update(2015-09-03): 已经提交PR并commit到Hugo，最新版本只需要在config里增加： disablePathToLower: true  6. 文章的内容里不能像Jekyll一样可以内嵌代码模板了。最终会生成哪些页面，有一套相对固定而复杂的规则，你会发现想创建一个自定义界面会非常的困难。 解决方法：无，看文档，了解它的规则。博客程序一般也不需要特别的自定义界面。Hugo本身已经支持了类似posts, tags, categories等内容聚合的页面，同时支持rss.xml，404.html等。如果你的博客程序复杂到需要其他的页面，好好想想是否必须吧。  7. 如何将rss.xml替换为feed.xml？ 解决方法：在config.yaml里加入： rssuri: “feed.xml”  8. 部署到github上后, 无内容 个人原因 hugo \u0026ndash;theme=hyde \u0026ndash;baseUrl=\u0026ldquo;https://hanchuntao.github.io/\u0026quot;生成静态页面后，public中会产生相应的目录，没有把这些目录push 到远端\n9. 部署到github上后一直不显示CSS样试 发现是 \u0026ndash;baseUrl=\u0026ldquo;http://hanchuntao.github.io/\u0026quot;的问题，要用 \u0026ndash;baseUrl=\u0026ldquo;https://hanchuntao.github.io/\u0026quot;\n从github上看到的markdown没有显示图片 原因： 图片要保存在static目录下，并显在引用图片时，使用static的相对位置(例如：/how-to-use-hugo/1.png) 生成静态网页后，需要把图片也上传到github\n","title":"How to Use Hugo"},{"location":"https://codenow.me/tips/go-pptof/","text":"使用 go tool pptof 可以 debug 程序\n需要在程序中先 import\nimport _ \u0026#34;net/http/pprof\u0026#34; 然后启动一个 goroutine 用于远程访问\ngo func() { log.Println(http.ListenAndServe(\u0026#34;localhost:6060\u0026#34;, nil)) }() 最后我们就可使用 http 抓取一些关键指标\n go tool pprof http://localhost:6060/debug/pprof/heap go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30 go tool pprof http://localhost:6060/debug/pprof/block wget http://localhost:6060/debug/pprof/trace?seconds=5 go tool pprof http://localhost:6060/debug/pprof/mutex  ","title":"Go tool pptof"},{"location":"https://codenow.me/translation/migrating-projects-from-dep-to-go-modules/","text":" 原文地址\nGo Modules 是 Go 管理的未来方向。已经在 Go 1.11 中可以试用，将会是 Go 1.13 中的默认行为。\n我不会在这篇文章中描述包管理工具的工作流程。我会主要讨论的是如何把现有的项目中 dep 迁移的 Go Module。\n在我的实例中，我会使用一个私有的仓库地址 github.com/kuinta/luigi ，它是使用 Go 语言编写，在好几个项目中被使用，是一个绝佳的候选人。\n首先，我们需要初始化 Module：\ncd github.com/kounta/luigi go mod init github.com/kounta/luigi 完成后只会有两行输出：\ngo: create now go.mod: module github.com/kounta/luigi go: copying requirments from Gopkg.lock 是的，这样就对了。这样就已经完成从 dep 迁移了。\n现在你只要看一眼新生成的文件 go.mod 就像下面这样：\nmodule github.com/kounta/luigi go 1.12 require ( github.com/elliotchance/tf v1.5.0 github.com/gin-gonic/gin v1.3.0 github.com/go-redis/redis v6.15.0+incompatible )  其实在 require 中还有更多的内容，为了保持整洁我把他们删除了。\n就像 dep 区分 toml 和 lock 文件一样。我们需要生成 go.sum 文件，只要执行：\ngo build 现在你可以删除 Gopkg.lock 和 Gopkg.toml 文件，然后提交 go.mod 和 go.sum 文件。\nTravis CI 如果你使用 Travis CI，你需要在 Go 1.13 之前通过设置环境变量来启用该功能。\nGO111MODULE=on  私有仓库 如果你要导入私有仓库，你可以会发现这个错误：\ninvalid module version \u0026quot;v6.5.0\u0026quot;: unknown revision v6.5.0  这是一个误导。它真正想说的，无法识别这个 URL (在这里是指的是 github.com)。无法找到这个仓库是因为 Github 没有权限确认仓库的存在。\n修复这个问题也很简单：\n 登录 Github 账号，然后到 Setting -\u0026gt; Personal access tokens 创建一个有访问私有仓库权限的 token 然后执行  export GITHUB_TOKEN=xxx git config --global url.\u0026#34;https://${GITHUB_TOKEN}:x-oauth-basic@github.com/kounta\u0026#34;.insteadOf \u0026#34;https://github.com/kounta\u0026#34;","title":"把项目从 Dep 迁移到 Go Modules"},{"location":"https://codenow.me/algorithm/leetcode_146_lru_cached/","text":" 题号：146\n难度：hard\n链接：https://leetcode-cn.com/problems/lru-cache/\n 使用双向链表+map，O(1) 时间复杂度内完成 get 和 put 操作\nclass Node: \u0026#34;\u0026#34;\u0026#34; 双链表节点 \u0026#34;\u0026#34;\u0026#34; def __init__(self, key, val): self.val = val self.key = key self.next = None self.prev = None class LRUCache: def __init__(self, capacity: int): self.capacity = capacity self.head = None self.tail = None self.index = {} def get(self, key: int) -\u0026gt; int: node = self.index.get(key) if node == None: return -1 if node.prev == None: # 这是一个表头节点 return node.val if node.next == None: # 这是一个尾节点，需要移动到头结点 if len(self.index) == 2: # 如果这是只有两个节点的链表 self.head = node self.tail = node.prev self.head.next = self.tail self.tail.prev = self.head else: self.tail = node.prev self.tail.next = None node.next = self.head self.head.prev = node self.head = node return self.head.val # 中间节点 node.prev.next = node.next node.next.prev = node.prev node.prev = None node.next = self.head self.head.prev = node self.head = node return self.head.val def put(self, key: int, value: int) -\u0026gt; None: node = self.index.get(key) if node: # 如果存在先删除 if len(self.index) == 1: # 如果只有一个直接删除就好 self.head = None self.tail = None self.index.pop(node.key) elif node.next == None: # 删除尾节点，需要修复一下 self.tail self.tail = node.prev self.tail.next = None self.index.pop(node.key) elif node.prev == None: # 删除头结点，需要修复一下 self.head self.head = node.next self.head.prev = None self.index.pop(node.key) else: # 删除中间节点 node.prev.next = node.next node.next.prev = node.prev self.index.pop(node.key) else: # 如果 capacity 不够要删除尾节点 if len(self.index) \u0026gt;= self.capacity: if len(self.index) == 1: self.head = None self.tail = None self.index = {} else: node = self.tail self.tail = node.prev self.tail.next = None self.index.pop(node.key) # 构建一个新的节点，插入到头部 node = Node(key, value) if len(self.index) == 0: self.head = node self.tail = node self.index[key] = node elif len(self.index) == 1: # 如果当前只有一个节点 self.head = node self.head.next = self.tail self.tail.prev = self.head self.index[key] = node else: node.next = self.head self.head.prev = node self.head = node self.index[key] = node ","title":"Leetcode: 146 LRU Cache"},{"location":"https://codenow.me/articles/rmdbs-tree-datastruct/","text":"在关系型数据库中存储树形结构是比较麻烦的事情，因为数据库都是基于行存储的结构，要满足树形数据结构的添加、删除、查询、修改是一件比较棘手的事情。\n已经有一些解决方案可以解决：\n这篇文章介绍一下，使用「闭包表」来处理树形结构存储。\n选择「闭包表」主要是基于查询、插入、删除、移动都比较简单，更要的是都可以使用一条 SQL 就能处理完成。\nCREATE TABLE Comments ( comment_id SERIAL PRIMARY KEY, comment TEXT NOT NULL ); 树形结构典型就是评论和部门成员关系，以评论为例，我们同时又要支持完整增删改查的功能，大致结构如下： 为了满足这种复杂的关系，需要有另外一个表来存储这种结构。\nCREATE TABLE TreePaths ( ancestor BIGINT NOT NULL, descendant BIGINT NOT NULL, PRIMARY KEY(ancestor, descendant), FOREIGN KEY (ancestor) REFERENCES Comments(comment_id), FOREIGN KEY (descendant) REFERENCES Comments(comment_id) ); ancestor 作为每个评论节点的祖先，descendant 作为每个评论节点的后代。\n 这里的祖先和后代都是泛指所有祖先和后代，而不是特指直接的祖先和后代\n 接着构造一批数据插入 Comments 和 Tree Paths 中\ninsert into comments(comment_id, comment) values (1, \u0026#39;这个 Bug 的成因 是什么\u0026#39;); insert into comments(comment_id, comment) values (2, \u0026#39;我觉得是一个空指针\u0026#39;); insert into comments(comment_id, comment) values (3, \u0026#39;不，我查过了\u0026#39;); insert into comments(comment_id, comment) values (4, \u0026#39;我们需要查无效输入\u0026#39;); insert into comments(comment_id, comment) values (5, \u0026#39;是的，那是个问题\u0026#39;); insert into comments(comment_id, comment) values (6, \u0026#39;好，查一下吧\u0026#39;); insert into comments(comment_id, comment) values (7, \u0026#39;解决了\u0026#39;); insert into treepaths(ancestor, descendant) values (1, 1); insert into treepaths(ancestor, descendant) values (1, 2); insert into treepaths(ancestor, descendant) values (1, 3); insert into treepaths(ancestor, descendant) values (1, 4); insert into treepaths(ancestor, descendant) values (1, 5); insert into treepaths(ancestor, descendant) values (1, 6); insert into treepaths(ancestor, descendant) values (1, 7); insert into treepaths(ancestor, descendant) values (2, 2); insert into treepaths(ancestor, descendant) values (2, 3); insert into treepaths(ancestor, descendant) values (3, 3); insert into treepaths(ancestor, descendant) values (4, 4); insert into treepaths(ancestor, descendant) values (4, 5); insert into treepaths(ancestor, descendant) values (4, 6); insert into treepaths(ancestor, descendant) values (4, 7); insert into treepaths(ancestor, descendant) values (5, 5); insert into treepaths(ancestor, descendant) values (6, 6); insert into treepaths(ancestor, descendant) values (6, 7); insert into treepaths(ancestor, descendant) values (7, 7); 这里需要解释一下 treepaths 存储关系的逻辑：\n 每个节点和自己建立一个关系，也就是 ancestor 和 descendant 都是自己 每个节点和自己祖先建立关系，也就是 ancestor 指向所有祖先节点 每个节点和自己后代建立关系，也就是 descendant 指向所有的后代节点  以上关系建立完毕之后，就能以树形关系查询 comments 表中的数据，比如要查询 comment_id = 4 所有的子节点：\nSELECT c.* FROM Comments AS c JOIN TreePaths AS t ON c.comment_id = t.descendant WHERE t.ancestor = 4; 或者要查询 comment_id = 4 所有的父节点：\nSELECT c.* FROM Comments AS c JOIN TreePaths AS t ON c.comment_id = t.ancestor WHERE t.descendant = 4; 假如要在 comment_id= 5 后插入一个新的节点，先要插入关联到自己的关系，然后从 TreePaths 找出中 descendant 为 5 节点。意思就是找出 comment_id = 5 的祖先和新节点在 TreePaths 关联上.\ninsert into comments(comment_id, comment) values (8, \u0026#39;对的是这个问题，我已经修复了\u0026#39;); INSERT INTO TreePaths (ancestor, descendant) SELECT t.ancestor, 8 FROM TreePaths AS t WHERE t.descendant = 5 UNION ALL SELECT 8, 8; 如果要删除 comment_id = 7 这个节点，只需要在 TreePaths 删除 descendant = 7 的记录即可，这时候不用我们维护节点和节点之间的关系，所以很方便\nDELETE FROM TreePaths WHERE descendant = 7; 假如要删除 comment_id = 4 这颗完整的树，只需要找出这个 root 节点所有的后代删除即可。\nDELETE FROM TreePaths WHERE descendant IN (SELECT descendant FROM TreePaths WHERE ancestor = 4); 如果是移动一个节点，只需要删除然后再添加即可，这时候自身的引用可以不用删除。\n比较复杂的是移动一棵树，要先找到这棵树的根节点，然后移除所有子节点和他们祖先的关系，比如把 comment_id = 6 移动到 commint_id = 3 下。\n首先把在 TreePaths 把所有关系移除\nDELETE FROM TreePaths WHERE descendant IN (SELECT descendant FROM TreePaths WHERE ancestor = 6) AND ancestor IN (SELECT ancestor FROM TreePaths WHERE descendant = 6 AND ancestor != descendant); 然后在 commint_id = 3 插入新关系，同时所有子节点要和 commint_id = 3 的祖先建立关系\nINSERT INTO TreePaths (ancestor, descendant) SELECT supertree.ancestor, subtree.descendant FROM TreePaths AS supertree CROSS JOIN TreePaths AS subtree WHERE supertree.descendant = 3 AND subtree.ancestor = 6; 使用一开始查询的 SQL，可以看出移动过去了\n","title":"使用 RMDBS 存在树结构数据"},{"location":"https://codenow.me/articles/exercise_of_a_tour_of_go/","text":" 这周学了学 golang，做个记录\n学习网站：https://tour.golang.org\n对应的中文版：https://tour.go-zh.org\n这周主要学习内容是刷了一遍上面这个教程，虽然够官方，但讲解并不细致，很多需要自行 google\n顺便，第一次打开教程和在线运行代码都需要科学上网，但打开一次后所有内容就都被缓存下来了，火车上都可以翻页学习。也不方便的话可以用中文版，或者本地安装，教程上也都有说。\n知识点记录 go 项目结构  必须要有 package import 用的是字符串 首字母大写的是导出名(exported name)，可以被别的包使用，有点类似于 python 的 all 只有 package main 可以被直接运行 运行入口 func main() {}  基础部件  函数以 func 定义，每个参数后必须带类型，必须规定返回值类型，可返回多个值，返回值可预先命名，函数是第一类对象(first class object) 变量以 var 定义，定义时必须规定类型，可在定义时赋值，函数内的变量可以不用 var 而用 := 来定义+赋值 常量以 const 定义，不能使用 := 语法，仅支持基础类型 基础类型是 bool, string 和各种数字，byte = uint8, tune = int32 类似于 null, None 的，是 nil  语法  if 不需要小括号，但必须有大括号；if 中可以有一条定义变量的语句，此变量仅在 if 和 else 中可用 for 是唯一的循环结构，用法基本等同于 Java 里的 for + while，同样没有小括号，但有大括号，for {} 是无限循环 switch 的每个 case 后等同于自带 break，但可以用 fallthrough 直接跳过判断而执行下一条 case 内的语句；没有匹配到任何一个 case 时会运行 default 里的内容；没有条件的 switch 可以便于改写 if-elseif-else defer 可将其后的函数推迟到外层函数返回之后再执行，多个 defer 会被压入栈中，后进先出执行 select-case 语句可同时等待多个 chan，并在所有准备好的 case 中随机选一个执行 for-range 可以对 array, map, slice, string, chan 进行遍历 make 可用来为 slice, map, chan 类型分配内存及初始化对象，返回一个引用，对这三种类型使用make时，后续参数含义均不同  其他数据类型  pointer 类似 C，没有指针运算 struct 内的字段使用 . 访问 array 必须有长度，且内部所有值类型必须相同 slice 类似数组的引用，可动态调整长度，有 len 和 cap 两个属性，零值是 nil，用 append 函数可以追加元素及自动扩展 cap map 在获取值的时候可以用 value, ok = m[key] 来校验 key 是否存在 method 与 function 略有不同，需要有一个 receiver，若 receiver 为指针，则可以在方法中修改其指向的值。只能为定义在当前包的类型定义 method interface 类型被实现时无需显示说明，任何类型只要实现了其所有方法就认为其实现了此接口，没有 implements 关键字；可以用空接口来接收任意值 interface{} interface value 是一个tuple(value, type)，可以用 t, i = i.(T) 来校验类型，switch 中可以用 v := i.(type) 来判断其类型 channel 用来在 goroutines 直接传递信息，被 close 后可以用 for-range 遍历 常见 interface: stringer, error, reader, writer  goroutine  用 go 来启动一个 goroutine 用 chan 来在不同 goroutine 之间交流 select-case 语句可以同时等待多个 chan，并在所有准备好的 case 中随机选一个运行 sync.Mutex 互斥锁可用来保证多个 goroutine 中每次只有一个能够访问共享的变量  其他规则  所有的大括号里，前括号不允许单独成一行 推荐使用 tab 而非空格 没有 class 概念，用 struct + method 实现 变量定义了就必须要使用，否则通不过编译  A Tour of Go 里的内容大概就这些，可能还有些遗漏的细节，我也不打算再去补充上了\n这里还提供了 11 个练习，我也顺着做了过来，感觉就跟上学时的课后习题一样，熟悉的感觉让人感动\n我的解答以及相关内容放到了 WokoLiu/go-tour-exercise\n如果有朋友要学 golang 的话，希望能有些帮助\n","title":"[Go]Exercise of a Tour of Go"},{"location":"https://codenow.me/tips/is_varchar_a_number/","text":"判断 MySQL 里一个 varchar 字段的内容是否为数字：\nselect * from table_name where length(0+name) = length(name);","title":"[Mysql]Is Varchar a Number?"},{"location":"https://codenow.me/algorithm/leetcode_11_container_with_most_water/","text":" 题号：11\n难度：medium\n链接：https://leetcode.com/problems/container-with-most-water 如下是 python3 代码\n from typing import List class Solution(object): def maxArea01(self, height: List[int]) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;先撸一个暴力的\u0026#34;\u0026#34;\u0026#34; max_area = 0 for i, a1 in enumerate(height): for j, a2 in enumerate(height[i + 1:]): max_area = max(max_area, min(a1, a2) * (j + 1)) return max_area def maxArea02(self, height: List[int]) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;从左右往中间压缩。由于总面积是较短的一根决定的 考虑到，如果 height[left] \u0026lt; height[right] 那么即使 right -= 1，max_area 也不会超过当前面积， 反而 left += 1，面积还有可能更大，因此此时应 left += 1 另一个方向的判断同理 \u0026#34;\u0026#34;\u0026#34; max_area = 0 left = 0 right = len(height) - 1 while left \u0026lt; right: if height[left] \u0026lt; height[right]: max_area = max(max_area, height[left] * (right - left)) left += 1 else: max_area = max(max_area, height[right] * (right - left)) right -= 1 return max_area if __name__ == \u0026#39;__main__\u0026#39;: data = [1, 8, 6, 2, 5, 4, 8, 3, 7] print(Solution().maxArea02(data))","title":"Leetcode: 11 Container with most water"},{"location":"https://codenow.me/tips/tips-for-adding-debug-logs/","text":"之所以整理这方面的小技巧，主要是 golang 的开源项目都是像 TiDB、etcd 这种偏低层的分布式服务。用 debugger 来跟踪代码是比较困难的，容易出错，而且还容易遇到坑，比如：有的 golang 版本无法正确输出调试信息，mac 上有些开源项目调试模式无法正常运行等等。用日志的话，更简单直接，不容易遇到坑。只不过，在查看变量、查看调用栈方面是真不太方便，下面几个小技巧能够弥补一些吧。\n查看调用栈\n可以使用 debug.Stack() 方法获取调用栈信息，比如像下面这样：\nlog.Printf(\u0026#34;stack of function xxx: %v\u0026#34;, string(debug.Stack())) 不过，在日志中打印调用栈的方法还是要慎用，输出内容有时候太长了，影响日志的连贯性。可以考虑将栈信息再做一下处理，只保留最上面几层的调用信息。\n查看变量类型\n可以使用 %T 来查看变量类型，很多时候可以像下面这样简单查看一下变量的类型和取值：\nlog.Printf(\u0026#34;DEBUG: node type: %T, value: %v\u0026#34;, n, n) 使用 buffer 来收集要查看的变量信息\n有的时候，我们需要查看的不是一个变量，可能是多个变量或者一个复杂数据结构中的一部分字段，如果代码中没有给出满足需求的 String 方法的话，可以考虑用 buffer，自己一点点收集，就像下面这样：\nbuf := bytes.NewBufferString() fmt.Fprintf(buf, \u0026#34;a: %v, \u0026#34;, a) fmt.Fprintf(buf, \u0026#34;b.child: %v, \u0026#34;, b) fmt.Fprintf(buf, \u0026#34;c.parent: %v, \u0026#34;, c.parent) log.Printf(\u0026#34;%v\u0026#34;, buf.String())","title":"golang 项目添加 debug 日志的小技巧"},{"location":"https://codenow.me/articles/code-reading-coverage/","text":"最近本人在阅读一些开源项目的代码，说到如何阅读开源代码，特别是超出自己能力范围的开源项目，可以说的内容还是挺多的。今天分享一个比较「偏」的：代码「阅读」覆盖率。\n看一个代码库，刚开始可能是一头雾水，再咬咬牙坚持一下，一般能梳理出大致的脉络，比如服务的启动流程是怎样的，服务主要由那几个组件构成，它们之间是如何通信协作的。再往后则是一点一点了解代码是如何支持各种不同场景的，加深对代码的理解。代码「阅读」覆盖率在第三个阶段会有一定的帮助。\n所谓的代码「阅读」覆盖率，和代码测试覆盖率概念类似，后者统计的是运行测试时哪些代码被运行过，所占比例是多少，前者统计的则是哪些行代码已经理解了，哪些还不理解。通过阅读覆盖率的统计，我们能更好衡量对代码库的了解程度，增加我们深入阅读代码的乐趣。\n为了实现阅读覆盖率的统计，我开发了一个简陋的浏览器插件，主要有以下功能：\n功能一：基于 github，支持在 github 代码页面中标记哪些代码已经理解，效果如下图所示：\n直接借助 github 代码页面来显示代码理解情况，直接扩展 github 自带的菜单，增加标记功能 （图中的 mark as read 和 mark as unread 菜单项），这样能够减少一些工作量。\n功能二：统计代码阅读覆盖率\n效果如下所示：\n在文件列表和代码界面显示百分比。\n目前插件还很简陋，不过实现方式很简单，就不分享代码了，感兴趣的同学可以自己试着开发一个。\n小结，阅读学习开源代码是一种比较硬核的游戏，增加阅读覆盖率的统计，是为了给这个硬核游戏添加一些可视化元素，就像塞尔达荒野之息里的地图，你能通过它看到自己探索了哪些神庙。这类手段可以延长游戏成就带来的快感，每次当我理解了一些代码后去把它们标记出来，还是很开心的，每次对代码渐渐失去兴趣时，看到统计的百分比还比较低，就又有了研究的动力。\n","title":"代码「阅读」覆盖率"},{"location":"https://codenow.me/translation/group-by-and-aggregation-elimination/","text":"原文链接：Group-by and Aggregation Elimination 是一篇关于数据库查询优化的文章，有几句话实在不知道咋翻译好，也影响不大，直接留下原句了。翻译如下：\nI get a fair number of questions on query transformations, and it’s especially true at the moment because we’re in the middle of the Oracle Database 12c Release 2 beta program. 有时用户可能会发现在一个执行计划里有些环节消失了或者有些反常，然后会意识到查询发生了转换 (transformation) 。举个例子，有时你会惊讶的发现，查询语句里的表和它的索引可能压根就没有出现在查询计划当中，这是连接消除 (Join Elimination) 机制在起作用。\n我相信，你已经发现查询转换是查询优化中很重要的一环，因为它经常能够通过消除一些像连接（join）、排序（sort）的步骤来降低查询的代价。有时修改查询的形式可以让查询使用不同的访问路径（access path），不同类型的连接和甚至完全不同的查询方式。在每个发布版本附带的优化器白皮书中（比如 Oracle 12c One 的），我们都介绍了大多数的查询转换模式。\n在 Oracle 12.1.0.1 中，我们增加了一种新的转换模式，叫做 Group-by and Aggregation Elimination ，之前一直没有提到。它在 Oracle 优化器中是最简单的一种查询转换模式了，很多人应该都已经很了解了。你们可能在 Mike Dietrich’s upgrade blog 中看到过关于它的介绍。让我们来看一下这种转换模式到底做了什么。\n很多应用都有用过这么一种查询，这是一种单表分组查询的形式，数据是由另一个底层的分组查询形成的视图来提供的。比如下面这个例子：\nSELECT v.column1, v.column2, MAX(v.sm), SUM(v.sm) FROM (SELECT t1.column1, t1.column2, SUM(t1.item_count) AS sm FROM t1, t2 WHERE t1.column4 \u0026gt; 3 AND t1.id = t2.id AND t2.column5 \u0026gt; 10 GROUP BY t1.column1, t1.column2) V GROUP BY v.column1, v.column2; 如果没有查询转换，这个语句可能是下面这样的查询计划。每张表里有十万行数据，查询要运行 2.09 秒:\n------------------------------------------------------ Id | Operation | Name | Rows | Bytes | ------------------------------------------------------ | 0 | SELECT STATEMENT | | | | | 1 | HASH GROUP BY | | 66521 | 1494K| | 2 | VIEW | | 66521 | 1494K| | 3 | HASH GROUP BY | | 66521 | 2143K| | 4 | HASH JOIN | | 99800 | 3216K| | 5 | TABLE ACCESS FULL| T2 | 99800 | 877K| | 6 | TABLE ACCESS FULL| T1 | 99998 | 2343K| ------------------------------------------------------  从上面的计划中，你会看到有两个 Hash Group By 步骤，一个是为了视图，一个是为了外层的查询。我用的是 12.1.0.2 版本的数据库，通过设置隐藏参数 _optimizer_aggr_groupby_elim 为 false 的方式禁用了查询转换。\n下面我们看一下查询转换生效时被转换的查询，你会发现只有一个 Hash Group By 步骤。查询时间也少了很多，只有 1.29 秒：\n---------------------------------------------------- Id | Operation | Name | Rows | Bytes | ---------------------------------------------------- | 0 | SELECT STATEMENT | | | | | 1 | HASH GROUP BY | | 66521 | 2143K| | 2 | HASH JOIN | | 99800 | 3216K| | 3 | TABLE ACCESS FULL| T2 | 99800 | 877K| | 4 | TABLE ACCESS FULL| T1 | 99998 | 2343K| ----------------------------------------------------  上面这个例子是相对比较好理解的，因为在视图中分组查询的列信息和外层查询是一样的。不一定非要是这样的形式才行。有的时候即使外层的 Group By 是视图中 Group By 的子集也是可以的。比如下面这个例子：\nSELECT v.column1, v.column3, MAX(v.column1), SUM(v.sm) FROM (SELECT t1.column1, t1.column2, t1.column3, SUM(t1.item_count) AS sm FROM t1, t2 WHERE t1.column4 \u0026gt; 3 AND t1.id = t2.id AND t2.column5 \u0026gt; 10 GROUP BY t1.column1, t1.column2, t1.column3) V GROUP BY v.column1, v.column3; ---------------------------------------------------- Id | Operation | Name | Rows | Bytes | ---------------------------------------------------- | 0 | SELECT STATEMENT | | | | | 1 | HASH GROUP BY | | 49891 | 1607K| |* 2 | HASH JOIN | | 99800 | 3216K| |* 3 | TABLE ACCESS FULL| T2 | 99800 | 877K| |* 4 | TABLE ACCESS FULL| T1 | 99998 | 2343K| ----------------------------------------------------  你不需要额外做什么操作来开启这个查询转换。它默认是被开启的，当某个查询符合条件的时候就会自动被转换。在实际的企业级系统中，这种方式一定会带来很多显著的优化。不过要注意，这种转换模式在使用了 rollup 和 cube 的分组函数时是不起作用的。\n这种转换有没有什么问题呢？是有的（这也是 Mike Dietrich 提到它的原因）。为了做这个转换，Oracle 优化器必须判断出来什么时候可以用什么时候不可以用，这背后的逻辑可能会很复杂。The bottom line is that there were some cases where the transformation was being applied and it shouldn’t have been. Generally, this was where the outer group-by query was truncating or casting columns used by the inner group-by. This is now fixed and it’s covered by patch number 21826068. Please use MOS to check availability for your platform and database version.\n","title":"Group by and Aggregation Elimination"},{"location":"https://codenow.me/algorithm/leetcode_62._unique_paths_by_jarvys/","text":"62. Unique Paths 是一道基础动规题，递推公式：f(x,y) = f(x+1,y) + f(x, y+1)。我用递归 + memo 的方式完成的，代码如下：\nclass Solution(object): def fn(self, i, j, rows, cols, memo): if j \u0026gt;= cols or i \u0026gt;= rows: return 0 if j == cols - 1 or i == rows - 1: return 1 if memo[i][j] is None: memo[i][j] = self.fn(i+1,j,rows,cols,memo) + self.fn(i,j+1,rows,cols,memo) return memo[i][j] def uniquePaths(self, m, n): \u0026#34;\u0026#34;\u0026#34; :type m: int :type n: int :rtype: int \u0026#34;\u0026#34;\u0026#34; if m == 1 and n == 1: return 1 memo = [] for i in range(n): r = [] for j in range(m): r.append(None) memo.append(r) return self.fn(0, 0, n, m, memo) ","title":"Leetcode: 62. Unique Paths by jarvys"},{"location":"https://codenow.me/joinus/","text":" 如何加入 email 联系: h1x2y3awalm@gmail.com\n发送你的 github 邮箱，到邮件正文，邀请到 github 组织中。\n成员列表    Github Username Email 完成次数 加入时间     zhegnxiaowai h1x2y3awalm@gmail.com 0 2019年3月18日   jarvys leguroky@gmail.com 0 2019年3月18日   tubby tubby.xue@gmail.com 0 2019年3月18日   WokoLiu banbooliu@gmail.com 0 2019年3月18日   hanchuntao h2a0n0k8@gmail.com 0 2019年3月18日   kingkoma kingkoma8@gmail.com 0 2019年4月07日    ","title":"加入我们"},{"location":"https://codenow.me/rules/","text":" 食用指南 这是一个自发的学习小组，主要以学习为目的，提升自己的能力。\n 学习是泯灭人性的行为，所以我们要有一些强制手段来逼迫自己。\n 我们的学习计划以每周为一个周期，分成四个任务：\n 每周一个算法题：算法题可以来自任何 OJ 网站（比如 Leetcode 等），难度根据每人情况不同自己选择。 每周一篇文章翻译：翻译文章可长可短，只要和计算机相关的知识就可以。 每周一个 Tips：Tip 就是一个很小的记录，比如这周学会了什么小技巧。 每周一个分享：需要整理出一篇文章，内容不限，但是要和计算机相关，篇幅在 500 - 800 字最好。分享能成一整个系列最好。  任务时间为，周一 0 点整至周日 24 点整。统计方式以 commit 时间为准，合并到分支后会触发 CI 自动部署到该网站。\n惩罚措施 这是一个 强制 的学习计划，所以我们会有惩罚措施。\u0008惩罚措施 的 Base 为 100 元，每周的 4 个任务未完成一个需要惩罚 25 元。 惩罚的金额将会进入 惩罚基金，基金中所有的钱将会用于购买 学习资料。\n最后就是退出机制，任何人可以在 任何时候 退出或者在你未完成任务时候又不愿意缴纳惩罚时候，将会被强制清理出学习小组。\n惩罚基金在 未使用 时候会一并退回，同时删除 repo 中有关退出者的内容；如果惩罚基金已经被使用，将不会退回，需要注意。\n提交方法 首先要克隆本仓库到本地：\ngit clone --recurse-submodules https://github.com/atts-group/atts.git 任何时间都可以直接 push master 提交。\n 开始使用前你需要先安装 hugo，下载对应平台的版本即可。\n 目前有 4 个分类（type）：\n algorithm articles tips translation  新建文章需要使用 hugo new {type}/{title}.md 的方式新建，不能直接创建文件。\n 这里的 {type} 就是上面 4 个分类的其中一个，{title} 就是你写的标题\n 当写完后用使用 hugo server 来观察一下你新添加的是否正确，然后用 git push 推送的 master 分支，即可完成。\n 禁止修改非你创建的文件，特别的隐藏文件，除非你知道为什么要这么修改\n ","title":"玩法"},{"location":"https://codenow.me/","text":"","title":"ATTS Group"},{"location":"https://codenow.me/categories/","text":"","title":"Categories"},{"location":"https://codenow.me/tags/","text":"","title":"Tags"}]