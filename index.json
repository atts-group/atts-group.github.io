[{"location":"https://codenow.me/","text":"","title":"ATTS Group"},{"location":"https://codenow.me/articles/","text":"","title":"文章"},{"location":"https://codenow.me/algorithm/","text":"","title":"算法"},{"location":"https://codenow.me/translation/","text":"","title":"翻译"},{"location":"https://codenow.me/tips/","text":"","title":"Tips"},{"location":"https://codenow.me/rules/","text":"","title":"玩法"},{"location":"https://codenow.me/joinus/","text":"","title":"加入我们"},{"location":"https://codenow.me/translation/tidb-proposal-support-skyline-pruning/","text":" 原文链接：Proposal: Support Skyline Pruning，翻译如下：\n摘要 这篇建议引入了一些启发式规则和一个针对消除访问路径 (access path) 的通用框架。通过它的帮助，优化器可以避免选择一些错误的访问路径。\n背景 目前，访问路径的选择很大程度上取决于统计信息。我们可能会因为过期的统计信息而选择错误的索引。然而，很多错误的选择是可以通过简单的规则来消除的，比如：当主键或者唯一性索引能够完全匹配的时候，我们可以直接选择它而不管统计信息。\n建议 (Proposal) 目前在选择访问路径时最大的因素是需要扫描的数据行数，是否满足物理属性 (physical property) ，以及是否需要两次扫描。在这三个因素当中，只有扫描行数依赖统计信息。那么在没有统计信息的情况下我们能够怎样比较扫描行数呢？让我们来看一下下面这个例子：\ncreate table t(a int, b int, c int, index idx1(b, a), index idx2(a)); select * from t where a = 1 and b = 1; 从查询和表结构上，我们能够看到使用索引 idx1 扫描能够覆盖 idx2，通过索引 idx1 扫描的数据行数不会比使用 idx2 多，所以在这个场景中，idx1 要比 idx2 好。\n我们如何综合这三个因素来消除访问路径呢？假如有两条访问路径 x 和 y，如果 x 在这几个方面都不比 y 差并且某个因素上 x 还好于 y，那么在使用统计数据之前，我们可以消除 y，因为 x 在任何情况下都一定比 y 更好。这就是所谓的 skyline pruning。\n基本原理 (Rationale) Skyling pruing 已经在其他数据库中实现，包括 MySQL 和 OceanBase。要是没有它，我们可能会在一些简单场景下选择错误的访问路径。\n兼容性 Skyling pruning 并不影响兼容性。\n实现 在为数据寻找最好的查询方式时，由于我们要决定使用哪一个满足物理条件的访问路径，我们需要使用 skyling pruning。大部分情况下不会有太多索引，一个简单的嵌套循环算法就足够了。任何两个访问路径的比较方式已经在 Proposal 章节里介绍过了。\n引用  The Skyline Operator  ","title":"TiDB Proposal: Support Skyline Pruning"},{"location":"https://codenow.me/tips/idea-%E5%89%8D%E8%BF%9B%E5%90%8E%E9%80%80%E5%BF%AB%E6%8D%B7%E9%94%AE/","text":"前进：Command + ]\n后退：Command + [\n在追踪比较复杂的代码时，比较有用，可以快速前进后退，不至于迷失在代码中。\n","title":"Idea 前进后退快捷键"},{"location":"https://codenow.me/articles/tidb-%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E5%AD%90%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/","text":" 根据 TiDB 中的子查询优化技术 这篇文章的介绍，TiDB 在处理关联子查询时引入了 Apply 算子。然后使用关系代数将 Apply 算子等价转换成其他算子，从而达到去关联化的目的。理论上，所有的关联子查询都可以去关联化，具体的理论知识可以看这篇博客：SQL 子查询的优化。\n本文从代码角度，梳理一下常见关联子查询的优化。处理过程主要有两个阶段：\n 重写阶段：在将语法树转换成逻辑查询计划时，将子查询重写成带有 Apply 算子的查询计划，这部分主要是由 expressionRewriter 负责 去关联化：在优化逻辑查询计划时，尝试将 Apply 算子替换成其他算子，从而去关联化，这部分主要有 decorrelateSolver 负责  expressionRewriter 简介 expressionRewriter 负责将子查询重语法树写成带有 Apply 算子的查询计划。为了实现这一功能，需要能够遍历语法树，expressionRewriter 实现了 Visitor 接口，能够遍历语法树中的各个节点，在遍历过程当中完成重写工作，它的核心的方法主要是 Enter 和 Leave。\nVisitor 接口一般会被这样使用：\nfunc (n *CompareSubqueryExpr) Accept(v Visitor) (Node, bool) { newNode, skipChildren := v.Enter(n) if skipChildren { return v.Leave(newNode) } n = newNode.(*CompareSubqueryExpr) node, ok := n.L.Accept(v) //...  n.L = node.(ExprNode) node, ok = n.R.Accept(v) //...  n.R = node.(ExprNode) return v.Leave(n) } 每个语法树节点通过调用 Accept、Enter 和 Leave 方法来实现对整个语法树节点的遍历。\nQ1 select t1.a from t t1 where 1000 in (select t2.b from t t2 where t2.a = t1.a) Q1 是最简单常见的一种子查询，通过对 Q1 的分析，我们能够理解子查询优化的框架。\n重写阶段 在 Q1 中，子查询出现在 where 当中，在构建逻辑查询计划时，会被 expressionRewriter 重写。\n构建 select 查询计划过程中，主要会执行以下几个方法，分别用来构建 DataSource、Selection、Aggregation 等逻辑查询节点：\n buildSelect  buildResultSetNode resolveGbyExprs resolveHavingAndOrderBy buildSelection buildAggregation buildProjection   buildSelect 中被调用的这些函数都使用了 expressionRewriter ，在 Q1 中，子查询重写发生在 buildSelection 当中：\nfunc (b *planBuilder) buildSelection(p LogicalPlan, where ast.ExprNode, AggMapper map[*ast.AggregateFuncExpr]int) LogicalPlan { //...  conditions := splitWhere(where) //...  for _, cond := range conditions { expr, np, err := b.rewrite(cond, p, AggMapper, false) //...  } //... } Q1 的 where 部分比较简单，只包含了一个子查询 和 in 组成条件，对应的是 PatternInExpr 类型，先简单了解一下 PatternInExpr：\ntype PatternInExpr struct { exprNode // Expr is the value expression to be compared.  Expr ExprNode // List is the list expression in compare list.  List []ExprNode // Not is true, the expression is \u0026#34;not in\u0026#34;.  Not bool // Sel is the subquery, may be rewritten to other type of expression.  Sel ExprNode } 在关联子查询中主要用到了 Sel 和 Expr 属性，Sel 对应的是子查询 select t2.b from t t2 where t2.a = t1.a ，Expr 对应的是常量 1000。\n根据 expressionRewriter 的 Enter 方法，处理逻辑主要在 handleInSubquery 当中。\n// Enter implements Visitor interface. func (er *expressionRewriter) Enter(inNode ast.Node) (ast.Node, bool) { switch v := inNode.(type) { //...  case *ast.PatternInExpr: if v.Sel != nil { return er.handleInSubquery(v) } //...  //...  } return inNode, false } expressionRewriter 还有 handleCompareSubquery、handleExistSubquery、handleScalarSubquery 等方法分别用来处理其他几种子查询。\n分析 handleInSubquery 代码，简化后的代码如下：\nfunc (er *expressionRewriter) handleInSubquery(v *ast.PatternInExpr) (ast.Node, bool) { //...  lexpr := er.ctxStack[len(er.ctxStack)-1] subq, ok := v.Sel.(*ast.SubqueryExpr) //...  np := er.buildSubquery(subq) // 构建子查询  //...  var rexpr expression.Expression //...  checkCondition, err := er.constructBinaryOpFunction(lexpr, rexpr, ast.EQ) // 构建查询条件  //...  er.p = er.b.buildSemiApply(er.p, np, expression.SplitCNFItems(checkCondition), asScalar, v.Not) // 创建 Apply 算子  //...  return v, true } expressionRewriter 先构建子查询的查询计划，然后根据 In 条件参数创建 Apply 的 conditions，最后调用 buildSemiApply 方法构建 Apply 查询计划。\n小结 为 Q1 构建查询计划过程中，与子查询重写有关的函数调用过程大致如下，expressionRewriter 简写成 er。\n buildSelect() \u0026lt;= 创建 Select 语句的查询计划  buildSelection() \u0026lt;= 创建 Selection 节点 rewrite() \u0026lt;= 重写 Q1 的子查询  exprNode.Accept(er) \u0026lt;= expressionRewriter 从这里开始遍历语法树 er.Enter()  er.handleInSubquery() er.buildSubquery() er.constructBinaryOpFunction() er.b.buildSemiApply() \u0026lt;= 创建 Apply 算子     最终得到的查询计划大致如下：\n注意，图中的 Apply 是 SemiApply。\nLogicalApply 类型 TiDB 使用 LogicalApply 来表示 Apply 算子：\ntype LogicalApply struct { LogicalJoin corCols []*expression.CorrelatedColumn } 从数据结构上也能看出，LogicalApply 和 LogicalJoin 很像，Apply 类型其实也是通过 JoinType 类型设置的（比如，SemiJoin、LeftOuterJoin、InnerJoin 等）。\n去关联化 去关联化是在逻辑查询优化过程中完成的，代码逻辑主要看 decorrelateSolver 。\n优化的思路是：尽可能把 Apply 往下推、把 Apply 下面的算子向上提，通过这一方式将关联变量变成普通变量，从而去关联化。虽然这一过程可能看起来会让查询计划的效率降低，但是去关联化以后再通过谓词下推等优化规则可以重新对整个查询计划进行优化。\nQ1 涉及到的代码如下：\n// optimize implements logicalOptRule interface. func (s *decorrelateSolver) optimize(p LogicalPlan) (LogicalPlan, error) { if apply, ok := p.(*LogicalApply); ok { outerPlan := apply.children[0] innerPlan := apply.children[1] apply.extractCorColumnsBySchema() if len(apply.corCols) == 0 { // \u0026lt;= 如果关联变量都被消除，可以将 Apply 转换成 Join  join := \u0026amp;apply.LogicalJoin join.self = join p = join } else if sel, ok := innerPlan.(*LogicalSelection); ok { // \u0026lt;= 在这个分支中消除 Selection 节点  newConds := make([]expression.Expression, 0, len(sel.Conditions)) for _, cond := range sel.Conditions { newConds = append(newConds, cond.Decorrelate(outerPlan.Schema())) } apply.attachOnConds(newConds) innerPlan = sel.children[0] apply.SetChildren(outerPlan, innerPlan) return s.optimize(p) // Selection 被消除以后，重新对 Apply 优化，在 Q1 中会触发 Apply 被转换成 Join  } else if m, ok := innerPlan.(*LogicalMaxOneRow); ok { //...  } else if proj, ok := innerPlan.(*LogicalProjection); ok { //...  } else if agg, ok := innerPlan.(*LogicalAggregation); ok { //...  } } //...  return p, nil } 参考上面提供的查询计划示意图，代码执行过程中：\n 寻找 Apply 节点，找到 Apply 节点以后，尝试对 innerPlan 子节点中的算子往上提，在 Q1 中：  将 Selection(t1.a=t2.a) 节点中的条件提到 Apply 上，消除 Selection 节点 完成这一步以后，Apply 的关联变量就被消除了，这样就可以把 Apply 转换成一个普通的 Join 了，Q1 的去关联化过程也基本完成。   整个过程如下图所示：\nQ2 select t1.a from t t1 where 1000 \u0026lt; (select min(t2.b) from t t2 where t2.a = t1.a) Q2 相对 Q1，子查询稍微复杂了一点，多了聚合函数。\n重写阶段 重写阶段和 Q1 很类似，区别主要在于查询条件不再是 PatternInExpr，变成了 BinaryOperationExpr，重写逻辑主要发生在 handleScalarSubquery 当中：\nfunc (er *expressionRewriter) handleScalarSubquery(v *ast.SubqueryExpr) (ast.Node, bool) { np, err := er.buildSubquery(v) //...  np = er.b.buildMaxOneRow(np) if len(np.extractCorrelatedCols()) \u0026gt; 0 { er.p = er.b.buildApplyWithJoinType(er.p, np, LeftOuterJoin) //...  return v, true } //... } 另外一点不同是，Apply 类型变成了 LeftOuterJoin （如何选择 Apply 的类型，可以参考开头的几篇文章）。重写完以后得到的查询计划大致如下：\n去关联化 和 Q1 相比，由于有 Aggregation 节点，Q2 的去关联化逻辑更复杂一些。对于 Q2 这类带有 aggr 的查询，decorrelateSolver 尽可能将 Aggregation 向上拉：\nfunc (s *decorrelateSolver) optimize(p LogicalPlan) (LogicalPlan, error) { if apply, ok := p.(*LogicalApply); ok { outerPlan := apply.children[0] innerPlan := apply.children[1] apply.extractCorColumnsBySchema() if len(apply.corCols) == 0 { //...  } else if sel, ok := innerPlan.(*LogicalSelection); ok { //...  } else if m, ok := innerPlan.(*LogicalMaxOneRow); ok { //...  } else if proj, ok := innerPlan.(*LogicalProjection); ok { //...  } else if agg, ok := innerPlan.(*LogicalAggregation); ok { if apply.canPullUpAgg() \u0026amp;\u0026amp; agg.canPullUp() { // 尝试将 Aggregation 上提  //...  } // 如果 Aggregation 不能上提，尝试将 Aggregation 下面的 Selection 上提，去掉关联变量  if sel, ok := agg.children[0].(*LogicalSelection); ok \u0026amp;\u0026amp; apply.JoinType == LeftOuterJoin { var ( eqCondWithCorCol []*expression.ScalarFunction remainedExpr []expression.Expression ) // 解析 Selection 中的关联条件  for _, cond := range sel.Conditions { if expr := apply.deCorColFromEqExpr(cond); expr != nil { eqCondWithCorCol = append(eqCondWithCorCol, expr.(*expression.ScalarFunction)) } else { remainedExpr = append(remainedExpr, cond) } } if len(eqCondWithCorCol) \u0026gt; 0 { //...  if len(apply.corCols) == 0 { join := \u0026amp;apply.LogicalJoin join.EqualConditions = append(join.EqualConditions, eqCondWithCorCol...) for _, eqCond := range eqCondWithCorCol { // 对于被上提的筛选条件，如果 Aggregation 没有包含对应列的分组的话  // 需要在 Aggregation 中添加上分组  clonedCol := eqCond.GetArgs()[1].Clone() // If the join key is not in the aggregation\u0026#39;s schema, add first row function.  if agg.schema.ColumnIndex(eqCond.GetArgs()[1].(*expression.Column)) == -1 { newFunc := aggregation.NewAggFuncDesc(apply.ctx, ast.AggFuncFirstRow, []expression.Expression{clonedCol}, false) agg.AggFuncs = append(agg.AggFuncs, newFunc) agg.schema.Append(clonedCol.(*expression.Column)) } // If group by cols don\u0026#39;t contain the join key, add it into this.  if agg.getGbyColIndex(eqCond.GetArgs()[1].(*expression.Column)) == -1 { agg.GroupByItems = append(agg.GroupByItems, clonedCol) } } //...  agg.collectGroupByColumns() if len(sel.Conditions) == 0 { // \u0026lt;= Selection 的条件都被删除，那么节点可以被消除了  agg.SetChildren(sel.children[0]) } //...  return s.optimize(p) // Selection 被消除以后重新对 Apply 节点进行优化，触发 Apply 转换成 Join 的逻辑  } //...  } } } } //...  return p, nil } 可惜的是，Q2 中的 Aggregation 是无法 pull up 的，貌似 TiDB 并没有完全按照开头文章中提到的方式去做子查询优化。虽然 Aggregation 无法上提，但是 decorrelator 会尝试将子节点 Selection 中的条件合并到 Apply 中，这个过程和 Q1 很像。如果 Selection 中的条件都被合并到 Apply 当中，那么 Selection 节点可以被消除了。\n在 Q2 中 Selection 节点删除后，子查询不再包含关联变量，Apply 可以被转换为 Join。去关联以后得到的查询计划大致如下：\n","title":"TiDB 源码学习：常见子查询优化"},{"location":"https://codenow.me/algorithm/leetcode-210.-course-schedule-ii/","text":"原题链接：210. Course Schedule II 。一道基础拓扑排序题，代码如下：\nclass Solution: def findOrder(self, numCourses: \u0026#39;int\u0026#39;, prerequisites: \u0026#39;List[List[int]]\u0026#39;) -\u0026gt; \u0026#39;List[int]\u0026#39;: degrees = [0] * numCourses graph = [[] for _ in range(numCourses)] for edge in prerequisites: source, dep = edge degrees[source] += 1 graph[dep].append(source) stack = [] for i in range(numCourses): deps = degrees[i] if deps == 0: stack.append(i) ret = [] while len(stack) \u0026gt; 0: node = stack.pop() ret.append(node) deps = graph[node] for dep in deps: degrees[dep] -= 1 if degrees[dep] == 0: stack.append(dep) if len(ret) != numCourses: return [] else: return ret","title":"Leetcode: 210 Course Schedule II"},{"location":"https://codenow.me/translation/intro_to_python_lambda_functions/","text":"原文地址: https://www.pythonforthelab.com/blog/intro-to-python-lambda-functions/\n不久前，Python在其语法中引入了使用lambda而不是def来定义函数的可能性。这些函数称为匿名函数同，在其它语言(如javascript)中非常常见。然后，在Python中，它们看起来有点晦涩，经常被忽略或误用。在本文中，我们将介绍labda函数，并讨论在何处以及如何使用它。\n要定义一个函数，可以使用以下语法：\ndef average(x, y): return (x + y) / 2 然后，如果要计算两个数字的平均值，只需执行以下操作\navg = average(2, 5) 在这种情况下，平均值将为3.5。我们也可以这样定义平均值：\naverage = lambda x, y: (x + y) / 2 如果你测试此函数，您将看到输出完全一样。必须指出，def和lambda之间语法非常不同。首先，我们定义不带括号的参数x, y。然后，我们定义要应用的操作。注意，当使用lambda函数时，返回是隐式的。\n然而，还有更根本的区别。lambda函数只能在一行上表示，并且没有docstring。如果对上面的每个定义尝试help(average)，您将看到输出非常不同，此外，无法记录average的第二版的实际操作。\n从功能上讲，定义平均值的两种方法都给出了相同的结果。到目前为止，他们之间的差异非常微妙。lambda（或匿名）函数的主要优点是它们不需要名称。此外，像我们上面所做的那样指定一个名字被认为是不好的做法，我们稍后将讨论。现在让我们看看您希望在什么上下文中使用lambda函数而不是普通函数。\n大多数教程都侧重于lambda函数来对列表进行排序。在讨论其他主题之前，我们也可以这样做。假设您有以下列表：\nvar=[1，5，-2，3，-7，4] 假设您希望对值进行排序，可以执行以下操作：\nsorted_var = sorted(var) #[-7，-2，1，3，4，5] 这很容易。但是，如果您希望根据到给定数字的距离对值进行排序，会发生什么情况呢？如果要计算到1的距离，需要对每个数字应用一个函数，例如abs（x-1），并根据输出对值进行排序。幸运的是，排序后，您可以使用关键字参数key=执行此操作。我们可以做到：\ndef distance(x): return abs(x - 1) sorted_var = sorted(var, key=distance) # [1, 3, -2, 4, 5, -7] 另一种选择是使用lambda函数：\nsorted_var = sorted(var, key=lambda x: abs(x-1)) 这两个例子将产生完全相同的输出。在使用def或lambda定义函数之间没有功能差异。我可以说第二个例子比第一个稍微短一些。此外，它使代码更具可读性，因为您可以立即看到对每个元素（abs（x-1））所做的操作，而不是通过代码挖掘来查看定义的距离。\n另一种可能是与map结合使用。map是将函数应用于列表中的每个元素的一种方法。例如，基于上面的示例，我们可以执行以下操作：\nlist(map(distance, var)) # [0, 4, 3, 2, 8, 3] 或者，使用lambda表达式\nlist(map(lambda x: abs(x-1), var)) # [0, 4, 3, 2, 8, 3] 它给出了完全相同的输出，同样，人们可以争论哪一个更容易阅读。上面的示例是您在其他教程中可能看到的。如果通过stackoverflow，可能会看到。其中一种可能是结合Pandas使用lambda函数。\npandas和lambda函数\n示例数据受此示例的启发，可以在此处找到。创建包含以下内容的文件示例example_data.csv：\nanimal,uniq_id,water_need elephant,1001,500 elephant,1002,600 elephant,1003,550 tiger,1004,300 tiger,1005,320 tiger,1006,330 tiger,1007,290 tiger,1008,310 zebra,1009,200 zebra,1010,220 zebra,1011,240 zebra,1012,230 zebra,1013,220 zebra,1014,100 zebra,1015,80 lion,1016,420 lion,1017,600 lion,1018,500 lion,1019,390 kangaroo,1020,410 kangaroo,1021,430 kangaroo,1022,410  要将数据读取为数据帧，我们只需执行以下操作：\nimport pandas as pd df = pd.read_csv(\u0026#39;example_data.csv\u0026#39;, delimiter = \u0026#39;,\u0026#39;)  假设您希望将数据框中每个动物名称的第一个字母大写，您可以执行以下操作：\ndf[\u0026#39;animal\u0026#39;]=df[\u0026#39;animal\u0026#39;].apply((lambda x:x.capitalize()) print(df.head()) 你会看到结果。当然，lambda函数可能变得更加复杂。您可以将它们应用于整个系列，而不是单个值，您可以将它们与其他库（如numpy或scipy）组合，并对数据执行复杂的转换。\nlambda函数最大的优点之一是，如果您使用的是Jupyter notebools，那么您可以立即看到这些变化。你不需要打开另一个文件，运行一个不同的，单元格等。如果你去Pandas的文档，你会看到，lambdas经常被使用。\nQt Slots\n使用lambdas的另一个常见示例是与qt库结合使用。我们过去写过一篇关于qt的介绍性文章。如果您不熟悉构建用户界面的工作方式，可以随意浏览它。一个非常简单的例子，只显示一个按钮，它看起来像这样：\nfrom PyQt5.QtWidgets import QApplication, QPushButton app = QApplication([]) button = QPushButton(\u0026#39;Press Me\u0026#39;) button.show() app.exit(app.exec()) 如果要在按下按钮时触发某个操作，则必须将该操作定义为一个函数。如果我们想在按下按钮时将某些内容打印到屏幕上，我们只需在app.exit之前添加以下行：\nbutton.clicked.connect(lambda x: print(\u0026#39;Pressed!\u0026#39;)) 如果您再次运行程序，每次按下按钮，您都会看到已按下！出现在屏幕上。同样，使用lambda函数作为信号的插槽可以加快编码速度，使程序更容易阅读。但是，lambda函数也需要谨慎考虑。\nlambda函数的使用位置\nlambda函数只能有一行。这迫使开发人员只能在没有复杂语法的情况下使用它们。在上面的示例中，您可以看到lambda函数非常简单。如果它需要打开一个套接字，交换一些信息，处理接收到的数据等，那么它可能不可能在一条线上完成。\n可以使用lambda函数的自然情况是作为其他需要可调用参数的函数的参数。例如，应用pandas数据帧需要一个函数作为参数。连接qt中的信号还需要一个函数。如果我们要应用或执行的函数很简单，并且我们不打算重复使用它，那么将其编写为匿名函数可能是一种非常方便的方法。\n不使用lambda函数的位置\nlambda函数是匿名的，因此，如果您要为其分配名称，例如在执行以下操作时：\naverage = lambda x，y:（x+y）/2 这意味着你做错了什么。如果需要为函数指定一个名称，以便在程序的不同位置使用它，请使用标准的def语法。在这个博客中有一个关于Python中lambda函数滥用的冗长讨论。我经常看到的，尤其是刚学过lambdas的人，是这样的：\nsorted_var = sorted(var, key=lambda x: abs(x)) 如果这是第一次看到lambda函数，那么这个无辜的示例可能很难包装起来。但是，您所拥有的是将一个函数（abs）包装在另一个函数中。它应该是这样的：\ndef func(x): return abs(x) 与仅仅做abs（x）相比有什么优势？实际上，没有优势，这意味着我们也可以这样\nsorted_var = sorted(var, key=abs) 如果您注意我们前面开发的示例，我们使用abs（x-1）来避免这种冗余。\n结论\nlambda（或匿名）函数是一种在Python程序中逐渐流行的工具。这就是为什么你能理解它的含义是非常重要的。您必须记住，lambda语法不允许您这样做，没有它们是不可能做到的。更重要的是它的方便性、语法经济性和可读性。\n在其他编程语言（如javascript）中，匿名函数的使用频率非常高，并且具有比Python更丰富的语法。我不相信Python也会这样做，但无论如何，它们是一种工具，不仅可以帮助您使用当前的程序，而且还可以帮助您了解如果您修补其他语言的话会发生什么。\n","title":"Intro to Python Lambda Functions"},{"location":"https://codenow.me/tips/python_code_static_analysis_tool_summary/","text":" 1.Pylint  Pylint是Python代码的一个静态检查工具，它能够检测一系列的代码错误，代码坏味道和格式错误。\nPylint使用的编码格式类似于PEP-8。\n它的最新版本还提供代码复杂度的相关统计数据，并能打印相应报告。\n不过在检查之前，Pylint需要先执行代码。\n具体可以参考http://pylint.org\n 2. Pyflakes  Pyflakes相对于Pylint而言出现的时间较晚，不同于Pylint的是，它不需要在检查之前执行代码来获取代码中的错误。\nPyflakes不检查代码的格式错误，只检查逻辑错误。\n具体可以参考http://launchpad.net/pyflakes\n 3. McCabe  McCabe是一个脚本，根据McCabe指标检查代码复杂性并打印报告。\n具体可以参考https://pypi.org/project/mccabe/\n 4. Pycodestyle  Pycodestyle是一个按照PEP-8的部分内容检查Python代码的一个工具\n这个工具之前叫PEP-8。\n具体可以参考https://github.com/pycqa/pycodestyle\n 5. Flake8  Flake8封装了Pyflakes、McCabe和Pycodestyle工具，它可以执行这三个工具提供的检查\n具体可以参考https://github.com/pycqa/flake8\n 6. Pychecker  PyChecker是Python代码的静态分析工具，它能够帮助查找Python代码的bug，而且能够对代码的复杂度和格式等提出警告。\nPyChecker会导入所检查文件中包含的模块，检查导入是否正确，同时检查文件中的函数、类和方法等。\n具体可以参考https://pypi.org/project/PyChecker/\n 7. Black  Black 号称是不妥协的 Python 代码格式化工具。之所以成为“不妥协”是因为它检测到不符合规范的代码风格直接就帮你全部格式化好，根本不需要你确定，直接替你做好决定。而作为回报，Black 提供了快速的速度。\nBlack 通过产生最小的差异来更快地进行代码审查。\nBlack 的使用非常简单，安装成功后，和其他系统命令一样使用，只需在 black 命令后面指定需要格式化的文件或者目录即可。\n具体可以参考https://atom.io/packages/python-black\n ","title":"Python_code_static_analysis_tool_summary"},{"location":"https://codenow.me/articles/spark-broadcast_accumulator/","text":" 累加器 累加器提供将工作节点的值聚合到驱动器程序中的功能，且实现语法简单。\n示例图：\n#python中累加空行 file = sc.textFile(inputfile) blankLines = sc.accumulator(0) # 创建Accumulator(Int) def extractCallSigns(line): global blankLines if line == \u0026#34;\u0026#34;: blankLines += 1 return line.split(\u0026#39; \u0026#39;) callSigns = file.flatMap(extractCallSigns) callSigns.saveAsTextFile(outputPath) print(\u0026#39;blank Lines : %d\u0026#39; %blankLines.value) 实际使用中可以创建多个累加器进行计数\nvalidSignCount = sc.Accumulator(0) invalidSignCount = sc.Accumulator(0) 广播变量 简介 正常情况中，spark的task会在执行任务时，将变量进行拷贝。当每个task都从主节点拷贝时，程序的通信和内存负担很重。 使用广播变量后，主节点会将变量拷贝至工作节点，任务从工作节点获得变量，而不用再次拷贝，此时变量被拷贝的次数取决于工作节点的个数。\n#在Python中使用广播变量 signPrefixes = sc.broadcast(loadCallSignTable()) def processSignCount(sign_count, signPrefixes): country = lookupCountry(sign_count[0], signPrefixes.value) count = sign_count[1] return (country, count) countryContactCounts = (contactCounts.map(processSignCount).reduceByKey((lambda x, y:x+y))) countryContactCounts.saveAsTextFile(ooutputPath) 基于分区进行操作 基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。 Spark提供基于分区的map和foreach。\nPython基于分区操作 def func(file): pass def fetchCallSigns(input): return input.mapPartitions(lambda x: func(file=x)) #使用mapPartitons执行对分区的操作 do_func = fetchCallSigns(inputfile) 常见分区操作函数\n   函数名 调用所提供的 返回的 对于RDD[T]的函数签名     mapPartitions() 该分区中元素的迭代器 返回的元素的迭代器 f:(Iter ator[T]——\u0026gt;Iterator[U])   mapPartitionsWithIndex() 分区序号，以及每个分区中的元素的迭代器 返回的元素的迭代器 f:(Int, Iterator[T]——\u0026gt;Iterator[U])   forceachPartitions() 元素迭代器 无 f:(Iterator[T]——\u0026gt;Unit)    数值RDD的操作 简介 Spark对包含数据的RDD提供了一些描述性的操作。 Spark的数据操作是通过流式算法实现的，允许以每次一个元素的方式构建模型。这些统计数据会在调用stats()时通过一次遍历计算出来，并以StatsCounter对象返回。\n数值操作 StatsCounter中可用的汇总统计数据\n   方法 含义     count() RDD中的元素的个数   mean() 元素的平均值   sum() 总和   max() 最大值   min() 最小值   variance() 元素的方差   sampleVariance() 从采样中计算出的方差   stdev() 标准差   sampleStdev() 采用的标准差    Python中使用数据操作\ndistanceNumerics = distances.map(lambda s: float(s)) stats = distanceNumerics.stats() stdev = stats.stdev() #计算标准差 mean = stats.mean() resonableDistances = distanceNumerics.filter(lambda x: math.fabs(x - mean)\u0026lt;3 * stdev) print(resonableDistances.collet())","title":"Spark累加器和广播变量"},{"location":"https://codenow.me/translation/scikit-learn/","text":" Scikit-learn: Machine Learning in Python 最近在学习机器学习算法和深度学习的部分内容。于是将Scikit-Learn的相关介绍论文看了看，翻译了一部分。原文地址\n摘要 Scikit-learn是一个Python模块，集成了各种最先进的机器学习算法，适用于中等规模和无监督的问题。该软件包侧重于使用通用高级语言将机器学习引入非专业人员。重点在于易于使用，性能，文档以及API的一致性。它具有最小的依赖性，并在简化的BSD许可下分发，鼓励在学术和商业环境中使用它。二进制文件和文档可以从http://scikit-learn.sourceforge.net 下载。\n介绍 Python编程语言正在成为最流行的科学计算语言之一。由于其高水平的交互性和成熟的科学库生态系统，Python在算法开发和探索数据分析领域成为极有吸引力的选择。然而，作为一种通用语言，它不仅越来越多的应用于学术领域，也应用于工业。Scikit-learn利用这种环境提供许多出名的机器学习算法的最先进的实现方式，同时保持易于使用的界面以及和Python语言紧密集成。这满足了软件和网络行业的非专业人员以及计算机科学以外领域（如生物学或物理学）对统计数据分析的日益增长的需求。\nScikit-learn不同于其他Python的机器学习库的原因在于：\n 它根据BSD许可证分发。 与DMP和pybrain不同，它结合了编译代码来提升效率。 它仅仅依赖于Numpy和Scipy来促进易于分发。不像pymvpa那样拥有例如R和shogun这样的可选依赖项。 与使用数据流框架的pybrain不同，它侧重于命令式编程。虽然该软件包主要是用Python编写的，但它包含了C ++库LibSVM和LibLinear，它们提供了SVMS的参考实现和具有兼容许可的广义线性模型。二进制包可在包括Windows和任何POSIX平台在内的丰富平台上使用。此外，由于其自由许可，它已被广泛分发为主要的免费软件发行版，如Ubuntu，Debian，Mandriva，NetBSD和商业广告诸如“Enthought Python Distributions”之类的发行版。  项目愿景 代码质量。该项目的目标不是提供尽可能多的功能，而是提供可靠的实施能力。通过单元测试来保证代码质量。在发布的0.8版本，测试覆盖率为81%，与此同时，使用静态分析工具例如pyflakes和PEP8.最后，我们严格遵守Python编程指南和Numpy样式文档中使用的函数与参数命名，努力保持一致性。\nBSD许可。大多数Python生态系统都是用非copyleft许可证进行许可。虽然这种政策有利于商业项目采用这些工具，但它确实施加了一些限制：我们无法使用某些现有的科学代码，例如GSL。\n裸骨设计和API。 为了降低进入门槛，我们避免使用框架代码并将不同对象的数量保持在最低限度，依赖于数据容器的numpy数组。\n社区驱动的发展。 我们的开发基于git，GitHub和公共邮件列表等协作工具。 欢迎并鼓励外部捐助。\n开发者文档。Scikit-learn提供了约300页的用户指南，包括叙述文档，类参考，教程，安装说明，以及60多个示例，其中一些包含实际应用程序。 我们尽量减少机器学习术语的使用，同时主要训练精度与所使用的算法有关。\n基础技术 Numpy：数据和模型参数的基础数据结构用户。 输入数据表示为numpy数组，因此可以与其他科学Python库无缝集成。 Numpy的基于视图的内存模型限制了副本，即使与编译代码绑定也是如此。它还提供基本的算术运算。\nScipy：线性代数的有效算法，稀疏矩阵表示，特殊函数和基本统计函数。 Scipy具有许多基于Fortran的标准数字包的绑定，例如LAPACK。 这对于易于安装和可移植性非常重要，因为围绕Fortran代码提供库在各种平台上都具有挑战性。\nCython：一种在Python中组合C的语言。 Cython使用类似Python的语法和高级操作轻松实现编译语言的性能。 它还用于绑定已编译的库，从而消除了Python / C扩展的样板代码。\n总结 Scikit-learn使用一致的，面向任务的界面，公开了各种机器学习算法，包括监督和非监督，从而可以轻松地比较给定应用程序的方法。 由于它依赖于科学的Python生态系统，因此可以轻松地将其集成到传统统计数据分析范围之外的应用程序中。 重要的是，以高级语言实现的算法可以用作特定于用例的方法的构建块，例如，在医学成像中。 未来的工作包括在线学习，扩展到大型数据集。\n","title":"Scikit-Learn"},{"location":"https://codenow.me/algorithm/decision_tree/","text":" 决策树 学习并构建决策树。\n决策树的一个重要任务是为了数据中心所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则。在这些机器根据数据创建规则时，就是机器学习的过程。专家系统中经常使用决策树，而且决策树给出结果往往可以匹敌在当前领域具有几十年工作经验的人类专家。\n决策树示例：\n决策树函数组成部分    优缺点 说明     优点 计算复杂度不高、输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据   缺点 可能会产生过度匹配问题   适用数据类型 数值型和标称型     寻找最佳划分特征值\n构造决策树时，需要考虑的第一个问题：当前数据集中哪个特征在划分数据分类时起到决定作用。\n为了找到这个特征，需要评估每一个特征，完成评测后，原始数据就会被划分为几个数据子集。然后遍历每个数据子集，若是都为同类，则该数据集结束分类，否则在该数据集中重新执行评估，二次分类。依次执行，直到数据被划分完毕或特征使用完毕时停止。\n创建分支的伪代码函数createBranch如下图所示：\n检测数据集中的每个子项是否属于同一分类： if so return 类标签： else: 寻找划分数据集的最好特征 划分数据集 创建分支节点 for每个划分的子集 调用函数createBranch并增加返回结果到分支节点中 return 分支节点  信息增益\n划分数据集最大的原则是：将无序的数据变得更加有序。本章选取信息论度量信息。\n在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最好的特征就是最好的选择。\n1).计算给定数据集的香农熵 计算熵的公式： $$ H = -\\sum{i=1}^{n}P(x{i})log{2}^{P(x{i})} $$\nfrom math import log def calcShannonEnt(dataSet): numEntries = len(dataSet) # 获取数据集中实例总数 labelCounts = {} # 创建数据字典，键值为数据集最后一列的值，即标签  for featVec in dataSet: currentLabel = featVec[-1] # 获取标签 if currentLabel not in labelCounts.keys(): # 如果标签不在字典中，则将其添加进去 labelCounts[currentLabel] =0 labelCounts[currentLabel] += 1 # 如果标签存在，则对应的数值加1 shannonEnt = 0.0 for LabelKey in labelCounts: prob = float(labelCounts[LabelKey]) / numEntries # 计算数值进入该分类的概率 shannonEnt -= prob * log(prob, 2) # 计算熵 return shannonEnt 示例数据：\n    不浮出水面 是否有脚蹼 属于鱼类     1 是 是 是   2 是 是 是   3 是 否 否   4 否 是 否   5 否 是 否    对应数据集：\ndataSet = [ [1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no'] ]  划分数据集 划分数据集，度量划分数据集的熵，以便判断当前是否正确划分了数据集。 通过对每个特征划分数据集的结果度量熵，判断哪个特征划分数据集是最好的划分方式。\ndef splitDateSet(dataSet, axis, value): # dataSet：待划分的数据集；axis：划分数据的特征索引；value：对应划分的特征值 retDateSet = [] for featVec in dataSet: if featVec[axis] == value: reduceFeatVec = featVec[:axis] reduceFeatVec.extend(featVec[axis+1:]) retDataSet.append(reduceFeatVec) return retDateSet 选择最好的数据集划分方式 通过该函数选取特征，划分数据集，计算出最好的划分数据集的特征。\ndef chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) -1 bestEntropy = calcShannonEnt(dataSet) # 计算原始香农熵 bestInfoGain = 0.0 # 信息增益 bestFeature = -1 # 原始特征值索引 for i in range(numFeatures): featList = [example[i] for example in dataSet] #创建第i个特征值组成的列表 uniqueVals = set(featList ) #创建特征值集合，去除重复元素 newEntropy = 0.0 for value in uniqueVals: # 依次读取特征值，计算香农熵 subDataSet = splitDataSet(dataSet, i, value) prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob * calcShannonEnt(subDataSet) infoGain = baseEntropy - newEntropy #计算信息增益 if infoGain \u0026gt; bestInfoGain: # 如果信息增益变大，则代表该特征值更好 bestInfoGain = infoGain bestFeature = i return bestFeature # 返回最佳特征值的索引 递归构造决策树 递归构造决策树原理：根据数据集选择最好的特征划分数据集，由于特征可能多于两个，故分支节点可能有多个。第一次划分后，子数据集中，可能还需要进行划分，故需要在子数据集中，递归调用决策树进行分类。\n 构建叶子节点分类函数 在本章构建决策树时，选择最佳特征值后，会删除特征。假设所有的特征使用完毕后，在某些叶子结点中，并不是都是一类，此时需要使用多数表决来分类。 下述函数将对叶子结点中所有的数据进行分类统计，最后选出数量最多的类别并返回其标签作为叶子结点分类标签。\ndef majorituCnt(classList): classCount = {} for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCountp[0][0] 构建决策树\ndef createTree(data, labels): classList = [example[i] for example in dataSet] if classList.count(classList[0]) == len(classList): # 当类别完全相同时停止继续划分 return classList[0] if len(dataSet[0]) == 1: # 遍历完所有特征时返回出现次数最多 return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) # 选取最佳划分特征 bestFeatLabel = labels[bestFeat] myTree = {bestFeatLabel:{}} # 构建节点 del(labels[bestFeat]) featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] # 将类标签复制，防止使用过程中类标签被改变。 myTree[bestFFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # 递归调用函数 return myTree  使用Matplotlib绘制决策树 import matplotlib.pyplot as plt from pylab import * mpl.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;SimHei\u0026#39;] decisionNode = dict(boxstyle=\u0026#39;sawtooth\u0026#39;, fc=\u0026#39;0.8\u0026#39;) leafNode = dict(boxstyle=\u0026#39;round4\u0026#39;, fc=\u0026#39;0.8\u0026#39;) arrow_args = dict(arrowstyle=\u0026#39;\u0026lt;-\u0026#39;) def getNumLeafs(MyTree): # 获取叶子节点数 NumLeafs = 0 firstStr = list(MyTree.keys())[0] secondDict = MyTree[firstStr] for TreeKey in secondDict.keys(): if isinstance(secondDict[TreeKey], dict): NumLeafs += getNumLeafs(secondDict[TreeKey]) else: NumLeafs += 1 return NumLeafs def getTreeDepth(MyTree): # 获取树深度 maxDepth = 0 thisDepth = 0 firstStr = list(MyTree.keys())[0] secondDict = MyTree[firstStr] for TreeKey in secondDict.keys(): if isinstance(secondDict[TreeKey], dict): thisDepth = 1 + getTreeDepth(secondDict[TreeKey]) else: thisDepth += 1 if thisDepth \u0026gt; maxDepth: maxDepth = thisDepth return maxDepth def plotMidText(cntrPt, parentPt, txtString): xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1] createPlot.axl.text(xMid, yMid, txtString) def plotTree(MyTree, parentPt, nodeTxt): numLeafs = getNumLeafs(MyTree) depth = getTreeDepth(MyTree) firstStr = list(MyTree.keys())[0] cntrPt = (plotTree.x0ff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.y0ff) plotMidText(cntrPt, parentPt, nodeTxt) plotNode(firstStr, cntrPt, parentPt, decisionNode) secondDict = MyTree[firstStr] plotTree.y0ff = plotTree.y0ff - 1.0/plotTree.totalD for TreeKey in secondDict.keys(): if isinstance(secondDict[TreeKey], dict): plotTree(secondDict[TreeKey], cntrPt, str(TreeKey)) else: plotTree.x0ff = plotTree.x0ff + 1.0/plotTree.totalW plotNode(secondDict[TreeKey], (plotTree.x0ff, plotTree.y0ff), cntrPt, leafNode) plotMidText((plotTree.x0ff, plotTree.y0ff), cntrPt, str(TreeKey)) plotTree.y0ff = plotTree.y0ff + 1.0/plotTree.totalD def plotNode(nodeTxt, centerPt, parentPt, nodeType): createPlot.axl.annotate(nodeTxt, xy=parentPt, xycoords=\u0026#39;axes fraction\u0026#39;, xytext=centerPt, textcoords=\u0026#39;axes fraction\u0026#39;, va=\u0026#39;center\u0026#39;, ha=\u0026#39;center\u0026#39;, bbox=nodeType, arrowprops=arrow_args) def createPlot(inTree): fig = plt.figure(1, facecolor=\u0026#39;white\u0026#39;) fig.clf() anprops = dict(xticks=[], yticks=[]) createPlot.axl = plt.subplot(111, frameon=False, **anprops) plotTree.totalW = float(getNumLeafs(inTree)) plotTree.totalD = float(getTreeDepth(inTree)) plotTree.x0ff = -0.5/plotTree.totalW plotTree.y0ff = 1.0 plotTree(inTree, (0.5, 1.0), \u0026#39;\u0026#39;) plt.show()","title":"DecisionTree"},{"location":"https://codenow.me/tips/differen_rsa/","text":"在同一个电脑上为不同的GitHub账号创建rsa并实现关联。 操作命令行。\n#为第一个账号创建rsa文件 ssh-keygen -t rsa -C \u0026#34;your email\u0026#34; -f ~/.ssh/id_rsa_for_account1 #为第二个账号创建rsa文件 ssh-keygen -t rsa -C \u0026#34;your email\u0026#34; -f ~/.ssh/id_rsa_for_account2 在.ssh文件夹下创建config文件 输入如下内容：\n#Default GitHub Host account1 HostName github.com User git IdentityFile ~/.ssh/id_rsa_for_account1 Host account2 HostName github.com User git IdentityFile ~/.ssh/id_rsa_for_account2  然后分别在account1和account2GitHub中添加公钥。 验证\nssh -T account1 ssh -T account2  ","title":"Different rsa for different github account in the same computer"},{"location":"https://codenow.me/algorithm/leetcode_905_sort_array_by_parity/","text":" 题号：905\n难度：Easy\n链接：https://leetcode.com/problems/sort-array-by-parity/\n 如下是 python3 代码:\n#!/usr/bin/python class Solution: def sortArrayByParity(self, A: \u0026#39;List[int]\u0026#39;) -\u0026gt; \u0026#39;List[int]\u0026#39;: lens = len(A) store_list = [None] * lens head = 0 tail = lens - 1 for i in range(lens): if A[i] % 2 == 0: store_list[head] = A[i] head += 1 else: store_list[tail] = A[i] tail -= 1 return store_list if __name__ == \u0026#39;__main__\u0026#39;: test_list = [3, 1, 2, 4] print(Solution().sortArrayByParity(test_list))","title":"Leetcode：905 Sort Array ByParity"},{"location":"https://codenow.me/articles/how-to-use-hugo/","text":" 一、介绍 1. 优点   Hugo是一个用Go语言编写的静态网站生成器，它使用起来非常简单，相对于Jekyll复杂的安装设置来说，Hugo仅需要一个二进制文件hugo(hugo.exe)即可轻松用于本地调试和生成静态页面。 Hugo生成静态页面的效率很高，几乎是瞬间完成的，而之前用Jekyll需要等待。 Hugo自带watch的调试模式，可以在我修改MarkDown文章之后切换到浏览器，页面会检测到更新并且自动刷新，呈现出最终效果，能极大的提高博客书写效率。 再加上Hugo是使用Go语言编写，已经没有任何理由不使用Hugo来代替Jekyll作为我的个人博客站点生成器了。   2. 静态网站文件的两种方式：   放到自己的服务器上提供服务：需要自己购买服务器 把网站托管到 GitHub Pages：需要将静态页面文件 push 到 GitHub 的博客项目的 gh-pages 分支并确保根目录下有 index.html 文件。   3. 官网   Hugo语言官方中文文档地址：http://www.gohugo.org/ Hugo官方主页：https://gohugo.io/   二、安装Hugo 1. 二进制安装（推荐：简单、快速） 到 Hugo Releases (https://github.com/gohugoio/hugo/releases)下载对应的操作系统版本的Hugo二进制文件（hugo或者hugo.exe）\n 下载解压后添加到 Windows 的系统环境变量的 PATH 中即可，不需安装。 可以直接放在C:\\Users\\chunt\\go\\bin下，这样就不需要添加系统环境变量  Mac下直接使用 Homebrew 安装：\n brew install hugo 二进制在 $GOPATH/bin/, 即C:\\Users\\chunt\\go\\bin  2. 源码安装(不好用，go get有些下载不下来) 源码编译安装，首先安装好依赖的工具：\n Git Go 1.3+ (Go 1.4+ on Windows)  设置好 GOPATH 环境变量，获取源码并编译：\n export GOPATH=$HOME/go go get -v github.com/spf13/hugo  源码会下载到 $GOPATH/src 目录, 即C:\\Go\\src\n如果需要更新所有Hugo的依赖库，增加 -u 参数：\n go get -u -v github.com/spf13/hugo   The -u flag instructs get to use the network to update the named packages and their dependencies. By default, get uses the network to check out missing packages but does not use it to look for updates to existing packages.\nThe -v flag enables verbose progress and debug output.\n 3. 查看安装结果 可知hugo已经正常安装: 三、创建hugo项目 使用Hugo快速生成站点，比如希望生成到 /path/to/site | C:\\code\\hugo路径：\n linux: $ hugo new site /path/to/site windows: hugo new site C:\\code\\hugo  这样就在 /path/to/site | C:\\code\\hugo目录里生成了初始站点，进去目录：\n cd /path/to/site cd C:\\code\\hugo  站点目录结构：\n ▸ archetypes/ ▸ content/ ▸ layouts/ ▸ static/ config.toml  config.toml是网站的配置文件，这是一个TOML文件，全称是Tom’s Obvious, Minimal Language， 这是它的作者GitHub联合创始人Tom Preston-Werner 觉得YAML不够优雅，捣鼓出来的一个新格式。 如果你不喜欢这种格式，你可以将config.toml替换为YAML格式的config.yaml，或者json格式的config.json。hugo都支持。\n content目录里放的是你写的markdown文章，layouts目录里放的是网站的模板文件，static目录里放的是一些图片、css、js等资源。\n 四、创建文章 1. 创建一个 about 页面： 进入到C:\\code\\hugo\n $ hugo new about.md  about.md 自动生成到了 content/about.md ，打开 about.md 看下：\n内容是 Markdown 格式的，+++ 之间的内容是 TOML 格式的，根据你的喜好，你可以换成 YAML 格式（使用 \u0026mdash; 标记）或者 JSON 格式。\n2. 创建第一篇文章，放到 post 目录，方便之后生成聚合页面。 $ hugo new post/first.md\n打开编辑 post/first.md ：\n五、安装皮肤 去 themes.gohugo.io 选择喜欢的主题，下载到 themes 目录中，配置可见theme说明\n1. 下载方法一 在 themes 目录里把皮肤 git clone 下来： $ pwd /c/code/hugo $ mkdir themes # 创建 themes 目录 $ cd themes $ git clone https://github.com/digitalcraftsman/hugo-material-docs.git  2. 下载方法二 也可以添加到git的submodule中，优点是后面讲到用 travis 自动部署时比较方便。 如果需要对主题做更改，最好fork主题再做改动。 git submodule add https://github.com/digitalcraftsman/hugo-material-docs.git themes/hugo-material-docs  3. 使用皮肤 将\\blog\\themes\\hugo-fabric\\exampleSite\\config.toml 替换 \\blog\\config.toml 注：config.toml文件是核心，对网站的配置多数需要修改该文件，而每个主题的配置又不完全一样。\n4. 修改皮肤 如果需要调整更改主题，需要在 themes/hugo-material-docs 目录下重新 build cd themes/hugo-material-docs \u0026amp;\u0026amp; npm i \u0026amp;\u0026amp; npm start 生成主题资源文件（hugo-fabric为主题名） D:\\git\\blog\u0026gt;hugo -t hugo-fabric Started building sites ... Built site for language en: 0 of 3 drafts rendered 0 future content 0 expired content 8 regular pages created 12 other pages created 0 non-page files copied 2 paginator pages created 1 tags created 1 categories created total in 35 ms 将\\blog\\themes\\hugo-fabric\\exampleSite\\config.toml 替换 \\blog\\config.toml  5. 修改配置文件 根据个人实际情况，修改config.toml  五、启动 hugo 自带的服务器 1. 在你的站点根目录执行 Hugo 命令进行调试：  回到hugo站点目录C:\\code\\hugo $ hugo server \u0026ndash;theme=hugo-material-docs \u0026ndash;buildDrafts  注明：v0.15 版本之后，不再需要使用 \u0026ndash;watch 参数了 浏览器里打开： http://localhost:1313\n2. 在项目根目录下，通过 hugo server 命令可以使用hugo内置服务器调试预览博客。 --theme 选项可以指定主题。也可用-t --watch 选项可以在修改文件后自动刷新浏览器。也可用-w --buildDrafts 包括标记为草稿（draft）的内容。也可以用-D  六、 部署到github 1. 新建仓库 假设你需要部署在GitHub Pages上，首先在GitHub上创建一个Repository， 命名为：hanchuntao.github.io （hanchuntao替换为你的github用户名）。  注意 baseUrl要在仓库setting里面查看，有可能跟仓库名不一样。 例如：https://SYSUcarey.github.io/变成了https://sysucarey.github.io/\n2. 在项目根目录执行Hugo命令生成HTML静态页面 $ hugo --theme=hugo-material-docs --baseUrl=\u0026quot;https://hanchuntao.github.io/\u0026quot;  \u0026ndash;theme 选项指定主题， \u0026ndash;baseUrl 指定了项目的网站\n注意 以上命令并不会生成草稿页面，如果未生成任何文章，请去掉文章头部的 draft=true 再重新生成。 文件默认内容在，draft 表示是否是草稿，编辑完成后请将其改为 false，否则编译会跳过草稿文件。\n3. 查看生成的页面 如果一切顺利，所有静态页面都会生成到public目录\n4. 将pubilc目录里所有文件push到刚创建的Repository的master分支。 $ cd public $ git init $ git remote add origin https://github.com/hanchuntao/hanchuntao.github.io.git $ git add -A $ git commit -m \u0026quot;first commit\u0026quot; $ git push -u origin master   浏览器里访问：https://hanchuntao.github.io/  七、错误处理 1. Unable to locate Config file 启动 hugo 内置服务器时，会在当前目录执行的目录中寻找项目的配置文件。所以，需要在项目根目录中执行这个命令，否则报错如下： C:\\Users\\kika\\kikakika\\themes\u0026gt;hugo server --theme=hugo-bootstrap --buildDrafts --watch Error: Unable to locate Config file. Perhaps you need to create a new site. Run `hugo help new` for details. (Config File \u0026quot;config\u0026quot; Not Found in \u0026quot;[C:\\\\Users\\\\kika\\\\kikakika\\\\themes]\u0026quot;)  2. Unable to find theme Directory hugo 默认在项目中的 themes 目录中寻找指定的主题。所有下载的主题都要放在这个目录中才能使用，否则报错如下： C:\\Users\\kika\\kikakika\u0026gt;hugo server --theme=hugo-bootstrap --buildDrafts --watch Error: Unable to find theme Directory: C:\\Users\\kika\\kikakika\\themes\\hugo-bootstrap  3. 生成的网站没有文章 生成静态网站时，hugo 会忽略所有通过 draft: true 标记为草稿的文件。必须改为 draft: false 才会编译进 HTML 文件。  4. 默认的ServerSide的代码着色会有问题，有些字的颜色会和背景色一样导致看不见。 解决方法：使用ClientSide的代码着色方案即可解决。（见：Client-side Syntax Highlighting）  5. URL全部被转成了小写，如果是旧博客迁移过来，将是无法接受的。 解决方法：我是直接改了Hugo的代码，将URL强制转换为小写那段逻辑去掉了，之后考虑在config里提供配置开关，然后给Hugo提一个PR。如果是Windows用户可以直接https://github.com/coderzh/ConvertToHugo 下载到我修改后的版本myhugo.exe。 Update(2015-09-03): 已经提交PR并commit到Hugo，最新版本只需要在config里增加： disablePathToLower: true  6. 文章的内容里不能像Jekyll一样可以内嵌代码模板了。最终会生成哪些页面，有一套相对固定而复杂的规则，你会发现想创建一个自定义界面会非常的困难。 解决方法：无，看文档，了解它的规则。博客程序一般也不需要特别的自定义界面。Hugo本身已经支持了类似posts, tags, categories等内容聚合的页面，同时支持rss.xml，404.html等。如果你的博客程序复杂到需要其他的页面，好好想想是否必须吧。  7. 如何将rss.xml替换为feed.xml？ 解决方法：在config.yaml里加入： rssuri: “feed.xml”  8. 部署到github上后, 无内容 个人原因 hugo \u0026ndash;theme=hyde \u0026ndash;baseUrl=\u0026ldquo;https://hanchuntao.github.io/\u0026quot;生成静态页面后，public中会产生相应的目录，没有把这些目录push 到远端\n9. 部署到github上后一直不显示CSS样试 发现是 \u0026ndash;baseUrl=\u0026ldquo;http://hanchuntao.github.io/\u0026quot;的问题，要用 \u0026ndash;baseUrl=\u0026ldquo;https://hanchuntao.github.io/\u0026quot;\n从github上看到的markdown没有显示图片 原因： 图片要保存在static目录下，并显在引用图片时，使用static的相对位置(例如：/how-to-use-hugo/1.png) 生成静态网页后，需要把图片也上传到github\n","title":"How to Use Hugo"},{"location":"https://codenow.me/tips/go-pptof/","text":"使用 go tool pptof 可以 debug 程序\n需要在程序中先 import\nimport _ \u0026#34;net/http/pprof\u0026#34; 然后启动一个 goroutine 用于远程访问\ngo func() { log.Println(http.ListenAndServe(\u0026#34;localhost:6060\u0026#34;, nil)) }() 最后我们就可使用 http 抓取一些关键指标\n go tool pprof http://localhost:6060/debug/pprof/heap go tool pprof http://localhost:6060/debug/pprof/profile?seconds=30 go tool pprof http://localhost:6060/debug/pprof/block wget http://localhost:6060/debug/pprof/trace?seconds=5 go tool pprof http://localhost:6060/debug/pprof/mutex  ","title":"Go tool pptof"},{"location":"https://codenow.me/translation/migrating-projects-from-dep-to-go-modules/","text":" 原文地址\nGo Modules 是 Go 管理的未来方向。已经在 Go 1.11 中可以试用，将会是 Go 1.13 中的默认行为。\n我不会在这篇文章中描述包管理工具的工作流程。我会主要讨论的是如何把现有的项目中 dep 迁移的 Go Module。\n在我的实例中，我会使用一个私有的仓库地址 github.com/kuinta/luigi ，它是使用 Go 语言编写，在好几个项目中被使用，是一个绝佳的候选人。\n首先，我们需要初始化 Module：\ncd github.com/kounta/luigi go mod init github.com/kounta/luigi 完成后只会有两行输出：\ngo: create now go.mod: module github.com/kounta/luigi go: copying requirments from Gopkg.lock 是的，这样就对了。这样就已经完成从 dep 迁移了。\n现在你只要看一眼新生成的文件 go.mod 就像下面这样：\nmodule github.com/kounta/luigi go 1.12 require ( github.com/elliotchance/tf v1.5.0 github.com/gin-gonic/gin v1.3.0 github.com/go-redis/redis v6.15.0+incompatible )  其实在 require 中还有更多的内容，为了保持整洁我把他们删除了。\n就像 dep 区分 toml 和 lock 文件一样。我们需要生成 go.sum 文件，只要执行：\ngo build 现在你可以删除 Gopkg.lock 和 Gopkg.toml 文件，然后提交 go.mod 和 go.sum 文件。\nTravis CI 如果你使用 Travis CI，你需要在 Go 1.13 之前通过设置环境变量来启用该功能。\nGO111MODULE=on  私有仓库 如果你要导入私有仓库，你可以会发现这个错误：\ninvalid module version \u0026quot;v6.5.0\u0026quot;: unknown revision v6.5.0  这是一个误导。它真正想说的，无法识别这个 URL (在这里是指的是 github.com)。无法找到这个仓库是因为 Github 没有权限确认仓库的存在。\n修复这个问题也很简单：\n 登录 Github 账号，然后到 Setting -\u0026gt; Personal access tokens 创建一个有访问私有仓库权限的 token 然后执行  export GITHUB_TOKEN=xxx git config --global url.\u0026#34;https://${GITHUB_TOKEN}:x-oauth-basic@github.com/kounta\u0026#34;.insteadOf \u0026#34;https://github.com/kounta\u0026#34;","title":"把项目从 Dep 迁移到 Go Modules"},{"location":"https://codenow.me/algorithm/leetcode_146_lru_cached/","text":" 题号：146\n难度：hard\n链接：https://leetcode-cn.com/problems/lru-cache/\n 使用双向链表+map，O(1) 时间复杂度内完成 get 和 put 操作\nclass Node: \u0026#34;\u0026#34;\u0026#34; 双链表节点 \u0026#34;\u0026#34;\u0026#34; def __init__(self, key, val): self.val = val self.key = key self.next = None self.prev = None class LRUCache: def __init__(self, capacity: int): self.capacity = capacity self.head = None self.tail = None self.index = {} def get(self, key: int) -\u0026gt; int: node = self.index.get(key) if node == None: return -1 if node.prev == None: # 这是一个表头节点 return node.val if node.next == None: # 这是一个尾节点，需要移动到头结点 if len(self.index) == 2: # 如果这是只有两个节点的链表 self.head = node self.tail = node.prev self.head.next = self.tail self.tail.prev = self.head else: self.tail = node.prev self.tail.next = None node.next = self.head self.head.prev = node self.head = node return self.head.val # 中间节点 node.prev.next = node.next node.next.prev = node.prev node.prev = None node.next = self.head self.head.prev = node self.head = node return self.head.val def put(self, key: int, value: int) -\u0026gt; None: node = self.index.get(key) if node: # 如果存在先删除 if len(self.index) == 1: # 如果只有一个直接删除就好 self.head = None self.tail = None self.index.pop(node.key) elif node.next == None: # 删除尾节点，需要修复一下 self.tail self.tail = node.prev self.tail.next = None self.index.pop(node.key) elif node.prev == None: # 删除头结点，需要修复一下 self.head self.head = node.next self.head.prev = None self.index.pop(node.key) else: # 删除中间节点 node.prev.next = node.next node.next.prev = node.prev self.index.pop(node.key) else: # 如果 capacity 不够要删除尾节点 if len(self.index) \u0026gt;= self.capacity: if len(self.index) == 1: self.head = None self.tail = None self.index = {} else: node = self.tail self.tail = node.prev self.tail.next = None self.index.pop(node.key) # 构建一个新的节点，插入到头部 node = Node(key, value) if len(self.index) == 0: self.head = node self.tail = node self.index[key] = node elif len(self.index) == 1: # 如果当前只有一个节点 self.head = node self.head.next = self.tail self.tail.prev = self.head self.index[key] = node else: node.next = self.head self.head.prev = node self.head = node self.index[key] = node ","title":"Leetcode: 146 LRU Cache"},{"location":"https://codenow.me/articles/rmdbs-tree-datastruct/","text":"在关系型数据库中存储树形结构是比较麻烦的事情，因为数据库都是基于行存储的结构，要满足树形数据结构的添加、删除、查询、修改是一件比较棘手的事情。\n已经有一些解决方案可以解决：\n这篇文章介绍一下，使用「闭包表」来处理树形结构存储。\n选择「闭包表」主要是基于查询、插入、删除、移动都比较简单，更要的是都可以使用一条 SQL 就能处理完成。\nCREATE TABLE Comments ( comment_id SERIAL PRIMARY KEY, comment TEXT NOT NULL ); 树形结构典型就是评论和部门成员关系，以评论为例，我们同时又要支持完整增删改查的功能，大致结构如下： 为了满足这种复杂的关系，需要有另外一个表来存储这种结构。\nCREATE TABLE TreePaths ( ancestor BIGINT NOT NULL, descendant BIGINT NOT NULL, PRIMARY KEY(ancestor, descendant), FOREIGN KEY (ancestor) REFERENCES Comments(comment_id), FOREIGN KEY (descendant) REFERENCES Comments(comment_id) ); ancestor 作为每个评论节点的祖先，descendant 作为每个评论节点的后代。\n 这里的祖先和后代都是泛指所有祖先和后代，而不是特指直接的祖先和后代\n 接着构造一批数据插入 Comments 和 Tree Paths 中\ninsert into comments(comment_id, comment) values (1, \u0026#39;这个 Bug 的成因 是什么\u0026#39;); insert into comments(comment_id, comment) values (2, \u0026#39;我觉得是一个空指针\u0026#39;); insert into comments(comment_id, comment) values (3, \u0026#39;不，我查过了\u0026#39;); insert into comments(comment_id, comment) values (4, \u0026#39;我们需要查无效输入\u0026#39;); insert into comments(comment_id, comment) values (5, \u0026#39;是的，那是个问题\u0026#39;); insert into comments(comment_id, comment) values (6, \u0026#39;好，查一下吧\u0026#39;); insert into comments(comment_id, comment) values (7, \u0026#39;解决了\u0026#39;); insert into treepaths(ancestor, descendant) values (1, 1); insert into treepaths(ancestor, descendant) values (1, 2); insert into treepaths(ancestor, descendant) values (1, 3); insert into treepaths(ancestor, descendant) values (1, 4); insert into treepaths(ancestor, descendant) values (1, 5); insert into treepaths(ancestor, descendant) values (1, 6); insert into treepaths(ancestor, descendant) values (1, 7); insert into treepaths(ancestor, descendant) values (2, 2); insert into treepaths(ancestor, descendant) values (2, 3); insert into treepaths(ancestor, descendant) values (3, 3); insert into treepaths(ancestor, descendant) values (4, 4); insert into treepaths(ancestor, descendant) values (4, 5); insert into treepaths(ancestor, descendant) values (4, 6); insert into treepaths(ancestor, descendant) values (4, 7); insert into treepaths(ancestor, descendant) values (5, 5); insert into treepaths(ancestor, descendant) values (6, 6); insert into treepaths(ancestor, descendant) values (6, 7); insert into treepaths(ancestor, descendant) values (7, 7); 这里需要解释一下 treepaths 存储关系的逻辑：\n 每个节点和自己建立一个关系，也就是 ancestor 和 descendant 都是自己 每个节点和自己祖先建立关系，也就是 ancestor 指向所有祖先节点 每个节点和自己后代建立关系，也就是 descendant 指向所有的后代节点  以上关系建立完毕之后，就能以树形关系查询 comments 表中的数据，比如要查询 comment_id = 4 所有的子节点：\nSELECT c.* FROM Comments AS c JOIN TreePaths AS t ON c.comment_id = t.descendant WHERE t.ancestor = 4; 或者要查询 comment_id = 4 所有的父节点：\nSELECT c.* FROM Comments AS c JOIN TreePaths AS t ON c.comment_id = t.ancestor WHERE t.descendant = 4; 假如要在 comment_id= 5 后插入一个新的节点，先要插入关联到自己的关系，然后从 TreePaths 找出中 descendant 为 5 节点。意思就是找出 comment_id = 5 的祖先和新节点在 TreePaths 关联上.\ninsert into comments(comment_id, comment) values (8, \u0026#39;对的是这个问题，我已经修复了\u0026#39;); INSERT INTO TreePaths (ancestor, descendant) SELECT t.ancestor, 8 FROM TreePaths AS t WHERE t.descendant = 5 UNION ALL SELECT 8, 8; 如果要删除 comment_id = 7 这个节点，只需要在 TreePaths 删除 descendant = 7 的记录即可，这时候不用我们维护节点和节点之间的关系，所以很方便\nDELETE FROM TreePaths WHERE descendant = 7; 假如要删除 comment_id = 4 这颗完整的树，只需要找出这个 root 节点所有的后代删除即可。\nDELETE FROM TreePaths WHERE descendant IN (SELECT descendant FROM TreePaths WHERE ancestor = 4); 如果是移动一个节点，只需要删除然后再添加即可，这时候自身的引用可以不用删除。\n比较复杂的是移动一棵树，要先找到这棵树的根节点，然后移除所有子节点和他们祖先的关系，比如把 comment_id = 6 移动到 commint_id = 3 下。\n首先把在 TreePaths 把所有关系移除\nDELETE FROM TreePaths WHERE descendant IN (SELECT descendant FROM TreePaths WHERE ancestor = 6) AND ancestor IN (SELECT ancestor FROM TreePaths WHERE descendant = 6 AND ancestor != descendant); 然后在 commint_id = 3 插入新关系，同时所有子节点要和 commint_id = 3 的祖先建立关系\nINSERT INTO TreePaths (ancestor, descendant) SELECT supertree.ancestor, subtree.descendant FROM TreePaths AS supertree CROSS JOIN TreePaths AS subtree WHERE supertree.descendant = 3 AND subtree.ancestor = 6; 使用一开始查询的 SQL，可以看出移动过去了\n","title":"使用 RMDBS 存在树结构数据"},{"location":"https://codenow.me/articles/exercise_of_a_tour_of_go/","text":" 这周学了学 golang，做个记录\n学习网站：https://tour.golang.org\n对应的中文版：https://tour.go-zh.org\n这周主要学习内容是刷了一遍上面这个教程，虽然够官方，但讲解并不细致，很多需要自行 google\n顺便，第一次打开教程和在线运行代码都需要科学上网，但打开一次后所有内容就都被缓存下来了，火车上都可以翻页学习。也不方便的话可以用中文版，或者本地安装，教程上也都有说。\n知识点记录 go 项目结构  必须要有 package import 用的是字符串 首字母大写的是导出名(exported name)，可以被别的包使用，有点类似于 python 的 all 只有 package main 可以被直接运行 运行入口 func main() {}  基础部件  函数以 func 定义，每个参数后必须带类型，必须规定返回值类型，可返回多个值，返回值可预先命名，函数是第一类对象(first class object) 变量以 var 定义，定义时必须规定类型，可在定义时赋值，函数内的变量可以不用 var 而用 := 来定义+赋值 常量以 const 定义，不能使用 := 语法，仅支持基础类型 基础类型是 bool, string 和各种数字，byte = uint8, tune = int32 类似于 null, None 的，是 nil  语法  if 不需要小括号，但必须有大括号；if 中可以有一条定义变量的语句，此变量仅在 if 和 else 中可用 for 是唯一的循环结构，用法基本等同于 Java 里的 for + while，同样没有小括号，但有大括号，for {} 是无限循环 switch 的每个 case 后等同于自带 break，但可以用 fallthrough 直接跳过判断而执行下一条 case 内的语句；没有匹配到任何一个 case 时会运行 default 里的内容；没有条件的 switch 可以便于改写 if-elseif-else defer 可将其后的函数推迟到外层函数返回之后再执行，多个 defer 会被压入栈中，后进先出执行 select-case 语句可同时等待多个 chan，并在所有准备好的 case 中随机选一个执行 for-range 可以对 array, map, slice, string, chan 进行遍历 make 可用来为 slice, map, chan 类型分配内存及初始化对象，返回一个引用，对这三种类型使用make时，后续参数含义均不同  其他数据类型  pointer 类似 C，没有指针运算 struct 内的字段使用 . 访问 array 必须有长度，且内部所有值类型必须相同 slice 类似数组的引用，可动态调整长度，有 len 和 cap 两个属性，零值是 nil，用 append 函数可以追加元素及自动扩展 cap map 在获取值的时候可以用 value, ok = m[key] 来校验 key 是否存在 method 与 function 略有不同，需要有一个 receiver，若 receiver 为指针，则可以在方法中修改其指向的值。只能为定义在当前包的类型定义 method interface 类型被实现时无需显示说明，任何类型只要实现了其所有方法就认为其实现了此接口，没有 implements 关键字；可以用空接口来接收任意值 interface{} interface value 是一个tuple(value, type)，可以用 t, i = i.(T) 来校验类型，switch 中可以用 v := i.(type) 来判断其类型 channel 用来在 goroutines 直接传递信息，被 close 后可以用 for-range 遍历 常见 interface: stringer, error, reader, writer  goroutine  用 go 来启动一个 goroutine 用 chan 来在不同 goroutine 之间交流 select-case 语句可以同时等待多个 chan，并在所有准备好的 case 中随机选一个运行 sync.Mutex 互斥锁可用来保证多个 goroutine 中每次只有一个能够访问共享的变量  其他规则  所有的大括号里，前括号不允许单独成一行 推荐使用 tab 而非空格 没有 class 概念，用 struct + method 实现 变量定义了就必须要使用，否则通不过编译  A Tour of Go 里的内容大概就这些，可能还有些遗漏的细节，我也不打算再去补充上了\n这里还提供了 11 个练习，我也顺着做了过来，感觉就跟上学时的课后习题一样，熟悉的感觉让人感动\n我的解答以及相关内容放到了 WokoLiu/go-tour-exercise\n如果有朋友要学 golang 的话，希望能有些帮助\n","title":"[Go]Exercise of a Tour of Go"},{"location":"https://codenow.me/tips/is_varchar_a_number/","text":"判断 MySQL 里一个 varchar 字段的内容是否为数字：\nselect * from table_name where length(0+name) = length(name);","title":"[Mysql]Is Varchar a Number?"},{"location":"https://codenow.me/algorithm/leetcode_11_container_with_most_water/","text":" 题号：11\n难度：medium\n链接：https://leetcode.com/problems/container-with-most-water 如下是 python3 代码\n from typing import List class Solution(object): def maxArea01(self, height: List[int]) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;先撸一个暴力的\u0026#34;\u0026#34;\u0026#34; max_area = 0 for i, a1 in enumerate(height): for j, a2 in enumerate(height[i + 1:]): max_area = max(max_area, min(a1, a2) * (j + 1)) return max_area def maxArea02(self, height: List[int]) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;从左右往中间压缩。由于总面积是较短的一根决定的 考虑到，如果 height[left] \u0026lt; height[right] 那么即使 right -= 1，max_area 也不会超过当前面积， 反而 left += 1，面积还有可能更大，因此此时应 left += 1 另一个方向的判断同理 \u0026#34;\u0026#34;\u0026#34; max_area = 0 left = 0 right = len(height) - 1 while left \u0026lt; right: if height[left] \u0026lt; height[right]: max_area = max(max_area, height[left] * (right - left)) left += 1 else: max_area = max(max_area, height[right] * (right - left)) right -= 1 return max_area if __name__ == \u0026#39;__main__\u0026#39;: data = [1, 8, 6, 2, 5, 4, 8, 3, 7] print(Solution().maxArea02(data))","title":"Leetcode: 11 Container with most water"},{"location":"https://codenow.me/tips/tips-for-adding-debug-logs/","text":"之所以整理这方面的小技巧，主要是 golang 的开源项目都是像 TiDB、etcd 这种偏低层的分布式服务。用 debugger 来跟踪代码是比较困难的，容易出错，而且还容易遇到坑，比如：有的 golang 版本无法正确输出调试信息，mac 上有些开源项目调试模式无法正常运行等等。用日志的话，更简单直接，不容易遇到坑。只不过，在查看变量、查看调用栈方面是真不太方便，下面几个小技巧能够弥补一些吧。\n查看调用栈\n可以使用 debug.Stack() 方法获取调用栈信息，比如像下面这样：\nlog.Printf(\u0026#34;stack of function xxx: %v\u0026#34;, string(debug.Stack())) 不过，在日志中打印调用栈的方法还是要慎用，输出内容有时候太长了，影响日志的连贯性。可以考虑将栈信息再做一下处理，只保留最上面几层的调用信息。\n查看变量类型\n可以使用 %T 来查看变量类型，很多时候可以像下面这样简单查看一下变量的类型和取值：\nlog.Printf(\u0026#34;DEBUG: node type: %T, value: %v\u0026#34;, n, n) 使用 buffer 来收集要查看的变量信息\n有的时候，我们需要查看的不是一个变量，可能是多个变量或者一个复杂数据结构中的一部分字段，如果代码中没有给出满足需求的 String 方法的话，可以考虑用 buffer，自己一点点收集，就像下面这样：\nbuf := bytes.NewBufferString() fmt.Fprintf(buf, \u0026#34;a: %v, \u0026#34;, a) fmt.Fprintf(buf, \u0026#34;b.child: %v, \u0026#34;, b) fmt.Fprintf(buf, \u0026#34;c.parent: %v, \u0026#34;, c.parent) log.Printf(\u0026#34;%v\u0026#34;, buf.String())","title":"golang 项目添加 debug 日志的小技巧"},{"location":"https://codenow.me/articles/code-reading-coverage/","text":"最近本人在阅读一些开源项目的代码，说到如何阅读开源代码，特别是超出自己能力范围的开源项目，可以说的内容还是挺多的。今天分享一个比较「偏」的：代码「阅读」覆盖率。\n看一个代码库，刚开始可能是一头雾水，再咬咬牙坚持一下，一般能梳理出大致的脉络，比如服务的启动流程是怎样的，服务主要由那几个组件构成，它们之间是如何通信协作的。再往后则是一点一点了解代码是如何支持各种不同场景的，加深对代码的理解。代码「阅读」覆盖率在第三个阶段会有一定的帮助。\n所谓的代码「阅读」覆盖率，和代码测试覆盖率概念类似，后者统计的是运行测试时哪些代码被运行过，所占比例是多少，前者统计的则是哪些行代码已经理解了，哪些还不理解。通过阅读覆盖率的统计，我们能更好衡量对代码库的了解程度，增加我们深入阅读代码的乐趣。\n为了实现阅读覆盖率的统计，我开发了一个简陋的浏览器插件，主要有以下功能：\n功能一：基于 github，支持在 github 代码页面中标记哪些代码已经理解，效果如下图所示：\n直接借助 github 代码页面来显示代码理解情况，直接扩展 github 自带的菜单，增加标记功能 （图中的 mark as read 和 mark as unread 菜单项），这样能够减少一些工作量。\n功能二：统计代码阅读覆盖率\n效果如下所示：\n在文件列表和代码界面显示百分比。\n目前插件还很简陋，不过实现方式很简单，就不分享代码了，感兴趣的同学可以自己试着开发一个。\n小结，阅读学习开源代码是一种比较硬核的游戏，增加阅读覆盖率的统计，是为了给这个硬核游戏添加一些可视化元素，就像塞尔达荒野之息里的地图，你能通过它看到自己探索了哪些神庙。这类手段可以延长游戏成就带来的快感，每次当我理解了一些代码后去把它们标记出来，还是很开心的，每次对代码渐渐失去兴趣时，看到统计的百分比还比较低，就又有了研究的动力。\n","title":"代码「阅读」覆盖率"},{"location":"https://codenow.me/translation/group-by-and-aggregation-elimination/","text":"原文链接：Group-by and Aggregation Elimination 是一篇关于数据库查询优化的文章，有几句话实在不知道咋翻译好，也影响不大，直接留下原句了。翻译如下：\nI get a fair number of questions on query transformations, and it’s especially true at the moment because we’re in the middle of the Oracle Database 12c Release 2 beta program. 有时用户可能会发现在一个执行计划里有些环节消失了或者有些反常，然后会意识到查询发生了转换 (transformation) 。举个例子，有时你会惊讶的发现，查询语句里的表和它的索引可能压根就没有出现在查询计划当中，这是连接消除 (Join Elimination) 机制在起作用。\n我相信，你已经发现查询转换是查询优化中很重要的一环，因为它经常能够通过消除一些像连接（join）、排序（sort）的步骤来降低查询的代价。有时修改查询的形式可以让查询使用不同的访问路径（access path），不同类型的连接和甚至完全不同的查询方式。在每个发布版本附带的优化器白皮书中（比如 Oracle 12c One 的），我们都介绍了大多数的查询转换模式。\n在 Oracle 12.1.0.1 中，我们增加了一种新的转换模式，叫做 Group-by and Aggregation Elimination ，之前一直没有提到。它在 Oracle 优化器中是最简单的一种查询转换模式了，很多人应该都已经很了解了。你们可能在 Mike Dietrich’s upgrade blog 中看到过关于它的介绍。让我们来看一下这种转换模式到底做了什么。\n很多应用都有用过这么一种查询，这是一种单表分组查询的形式，数据是由另一个底层的分组查询形成的视图来提供的。比如下面这个例子：\nSELECT v.column1, v.column2, MAX(v.sm), SUM(v.sm) FROM (SELECT t1.column1, t1.column2, SUM(t1.item_count) AS sm FROM t1, t2 WHERE t1.column4 \u0026gt; 3 AND t1.id = t2.id AND t2.column5 \u0026gt; 10 GROUP BY t1.column1, t1.column2) V GROUP BY v.column1, v.column2; 如果没有查询转换，这个语句可能是下面这样的查询计划。每张表里有十万行数据，查询要运行 2.09 秒:\n------------------------------------------------------ Id | Operation | Name | Rows | Bytes | ------------------------------------------------------ | 0 | SELECT STATEMENT | | | | | 1 | HASH GROUP BY | | 66521 | 1494K| | 2 | VIEW | | 66521 | 1494K| | 3 | HASH GROUP BY | | 66521 | 2143K| | 4 | HASH JOIN | | 99800 | 3216K| | 5 | TABLE ACCESS FULL| T2 | 99800 | 877K| | 6 | TABLE ACCESS FULL| T1 | 99998 | 2343K| ------------------------------------------------------  从上面的计划中，你会看到有两个 Hash Group By 步骤，一个是为了视图，一个是为了外层的查询。我用的是 12.1.0.2 版本的数据库，通过设置隐藏参数 _optimizer_aggr_groupby_elim 为 false 的方式禁用了查询转换。\n下面我们看一下查询转换生效时被转换的查询，你会发现只有一个 Hash Group By 步骤。查询时间也少了很多，只有 1.29 秒：\n---------------------------------------------------- Id | Operation | Name | Rows | Bytes | ---------------------------------------------------- | 0 | SELECT STATEMENT | | | | | 1 | HASH GROUP BY | | 66521 | 2143K| | 2 | HASH JOIN | | 99800 | 3216K| | 3 | TABLE ACCESS FULL| T2 | 99800 | 877K| | 4 | TABLE ACCESS FULL| T1 | 99998 | 2343K| ----------------------------------------------------  上面这个例子是相对比较好理解的，因为在视图中分组查询的列信息和外层查询是一样的。不一定非要是这样的形式才行。有的时候即使外层的 Group By 是视图中 Group By 的子集也是可以的。比如下面这个例子：\nSELECT v.column1, v.column3, MAX(v.column1), SUM(v.sm) FROM (SELECT t1.column1, t1.column2, t1.column3, SUM(t1.item_count) AS sm FROM t1, t2 WHERE t1.column4 \u0026gt; 3 AND t1.id = t2.id AND t2.column5 \u0026gt; 10 GROUP BY t1.column1, t1.column2, t1.column3) V GROUP BY v.column1, v.column3; ---------------------------------------------------- Id | Operation | Name | Rows | Bytes | ---------------------------------------------------- | 0 | SELECT STATEMENT | | | | | 1 | HASH GROUP BY | | 49891 | 1607K| |* 2 | HASH JOIN | | 99800 | 3216K| |* 3 | TABLE ACCESS FULL| T2 | 99800 | 877K| |* 4 | TABLE ACCESS FULL| T1 | 99998 | 2343K| ----------------------------------------------------  你不需要额外做什么操作来开启这个查询转换。它默认是被开启的，当某个查询符合条件的时候就会自动被转换。在实际的企业级系统中，这种方式一定会带来很多显著的优化。不过要注意，这种转换模式在使用了 rollup 和 cube 的分组函数时是不起作用的。\n这种转换有没有什么问题呢？是有的（这也是 Mike Dietrich 提到它的原因）。为了做这个转换，Oracle 优化器必须判断出来什么时候可以用什么时候不可以用，这背后的逻辑可能会很复杂。The bottom line is that there were some cases where the transformation was being applied and it shouldn’t have been. Generally, this was where the outer group-by query was truncating or casting columns used by the inner group-by. This is now fixed and it’s covered by patch number 21826068. Please use MOS to check availability for your platform and database version.\n","title":"Group by and Aggregation Elimination"},{"location":"https://codenow.me/algorithm/leetcode_62._unique_paths_by_jarvys/","text":"62. Unique Paths 是一道基础动规题，递推公式：f(x,y) = f(x+1,y) + f(x, y+1)。我用递归 + memo 的方式完成的，代码如下：\nclass Solution(object): def fn(self, i, j, rows, cols, memo): if j \u0026gt;= cols or i \u0026gt;= rows: return 0 if j == cols - 1 or i == rows - 1: return 1 if memo[i][j] is None: memo[i][j] = self.fn(i+1,j,rows,cols,memo) + self.fn(i,j+1,rows,cols,memo) return memo[i][j] def uniquePaths(self, m, n): \u0026#34;\u0026#34;\u0026#34; :type m: int :type n: int :rtype: int \u0026#34;\u0026#34;\u0026#34; if m == 1 and n == 1: return 1 memo = [] for i in range(n): r = [] for j in range(m): r.append(None) memo.append(r) return self.fn(0, 0, n, m, memo) ","title":"Leetcode: 62. Unique Paths by jarvys"},{"location":"https://codenow.me/joinus/","text":" 如何加入 email 联系: h1x2y3awalm@gmail.com\n发送你的 github 邮箱，到邮件正文，邀请到 github 组织中。\n成员列表    Github Username Email 完成次数 加入时间     zhegnxiaowai h1x2y3awalm@gmail.com 0 2019年3月18日   jarvys leguroky@gmail.com 0 2019年3月18日   tubby tubby.xue@gmail.com 0 2019年3月18日   WokoLiu banbooliu@gmail.com 0 2019年3月18日    ","title":"加入我们"},{"location":"https://codenow.me/rules/","text":" 食用指南 这是一个自发的学习小组，主要以学习为目的，提升自己的能力。\n 学习是泯灭人性的行为，所以我们要有一些强制手段来逼迫自己。\n 我们的学习计划以每周为一个周期，分成四个任务：\n 每周一个算法题：算法题可以来自任何 OJ 网站（比如 Leetcode 等），难度根据每人情况不同自己选择。 每周一篇文章翻译：翻译文章可长可短，只要和计算机相关的知识就可以。 每周一个 Tips：Tip 就是一个很小的记录，比如这周学会了什么小技巧。 每周一个分享：需要整理出一篇文章，内容不限，但是要和计算机相关，篇幅在 500 - 800 字最好。分享能成一整个系列最好。  任务时间为，周一 0 点整至周日 24 点整。统计方式以 commit 时间为准，合并到分支后会触发 CI 自动部署到该网站。\n惩罚措施 这是一个 强制 的学习计划，所以我们会有惩罚措施。\u0008惩罚措施 的 Base 为 100 元，每周的 4 个任务未完成一个需要惩罚 25 元。 惩罚的金额将会进入 惩罚基金，基金中所有的钱将会用于购买 学习资料。\n最后就是退出机制，任何人可以在 任何时候 退出或者在你未完成任务时候又不愿意缴纳惩罚时候，将会被强制清理出学习小组。\n惩罚基金在 未使用 时候会一并退回，同时删除 repo 中有关退出者的内容；如果惩罚基金已经被使用，将不会退回，需要注意。\n提交方法 首先要克隆本仓库到本地：\ngit clone --recurse-submodules https://github.com/atts-group/atts.git 任何时间都可以直接 push master 提交。\n 开始使用前你需要先安装 hugo，下载对应平台的版本即可。\n 目前有 4 个分类（type）：\n algorithm articles tips translation  新建文章需要使用 hugo new {type}/{title}.md 的方式新建，不能直接创建文件。\n 这里的 {type} 就是上面 4 个分类的其中一个，{title} 就是你写的标题\n 当写完后用使用 hugo server 来观察一下你新添加的是否正确，然后用 git push 推送的 master 分支，即可完成。\n 禁止修改非你创建的文件，特别的隐藏文件，除非你知道为什么要这么修改\n ","title":"玩法"},{"location":"https://codenow.me/","text":"","title":"ATTS Group"},{"location":"https://codenow.me/categories/","text":"","title":"Categories"},{"location":"https://codenow.me/tags/","text":"","title":"Tags"}]